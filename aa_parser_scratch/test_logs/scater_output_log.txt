
###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_6 scater_release_3_7

{
    "package": "scater",
    "release_versions": "scater_release_3_6 scater_release_3_7",
    "old_release_number": 1,
    "new_release_number": 2,
    "function_removals": 7,
    "function_additions": 5,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 12,
    "parameter_default_changes": 0
}

##########
Functions Removed
##########

exprs
fromCellDataSet
newSCESet
plotExpressionDefault
plotMetadata
plotReducedDimDefault
toCellDataSet


##########
Functions Added
##########

centreSizeFactors
librarySizeFactors
plotHeatmap
runMDS
uniquifyFeatureNames


##########
Removed Parameters
##########



##########
Added Parameters
##########



##########
Renamed Parameters
##########

0.
{
  "old_function": {
    "name": "calcIsExprs",
    "representation": "calcIsExprs",
    "parameters": "function ( object , lowerDetectionLimit = 0 , exprs_values = \"counts\" )",
    "body": "{    assay (  object , i =  exprs_values ) undefined  lowerDetectionLimit } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calcIsExprs",
    "representation": "calcIsExprs",
    "parameters": "function ( object , detection_limit = 0 , exprs_values = \"counts\" )",
    "body": "{    assay (  object , i =  exprs_values ) undefined  detection_limit } ",
    "filename": "calculate-expression.txt"
  }
}

1.
{
  "old_function": {
    "name": "nexprs",
    "representation": "nexprs",
    "parameters": "function ( object , lowerDetectionLimit = 0 , exprs_values = \"counts\" , byrow = FALSE , subset_row = NULL , subset_col = NULL )",
    "body": "{   exprs_mat -   assay (  object , i =  exprs_values )   subset_row -   .subset2index (  subset_row , target =  exprs_mat , byrow =  TRUE )   subset_col -   .subset2index (  subset_col , target =  exprs_mat , byrow =  FALSE )  if (  !  byrow )  {   margin.stats -   .Call (  cxx_margin_summary ,  exprs_mat ,  lowerDetectionLimit ,   subset_row -  1L ,  FALSE )   return (    margin.stats [[  2 ] ] [  subset_col ] ) } else  {   margin.stats -   .Call (  cxx_margin_summary ,  exprs_mat ,  lowerDetectionLimit ,   subset_col -  1L ,  TRUE )   return (    margin.stats [[  2 ] ] [  subset_row ] ) } } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "nexprs",
    "representation": "nexprs",
    "parameters": "function ( object , detection_limit = 0 , exprs_values = \"counts\" , byrow = FALSE , subset_row = NULL , subset_col = NULL )",
    "body": "{  if (   methods :: is (  object ,  \"SingleCellExperiment\" ) )  {   exprs_mat -   assay (  object , i =  exprs_values ) } else  {   exprs_mat -  object }   subset_row -   .subset2index (  subset_row , target =  exprs_mat , byrow =  TRUE )   subset_col -   .subset2index (  subset_col , target =  exprs_mat , byrow =  FALSE )  if (  !  byrow )  {   return (   .colAbove (  exprs_mat , rows =  subset_row , cols =  subset_col , value =  detection_limit ) ) } else  {   return (   .rowAbove (  exprs_mat , rows =  subset_row , cols =  subset_col , value =  detection_limit ) ) } } ",
    "filename": "calculate-expression.txt"
  }
}

2.
{
  "old_function": {
    "name": "calculateCPM",
    "representation": "calculateCPM",
    "parameters": "function ( object , use.size.factors = TRUE )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"object must be an SingleCellExperiment\" )   counts_mat -   counts (  object )   subset_row -   .subset2index (  NULL , target =  counts_mat , byrow =  TRUE )   margin.stats -   .Call (  cxx_margin_summary ,  counts_mat ,  0 ,   subset_row -  1L ,  FALSE )  if (  use.size.factors )  {   sf.list -   .get_all_sf_sets (  object )  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {   warning (  \"size factors requested but not specified, \r\n                    using library sizes instead\" )     sf.list $ size.factors [[  1 ] ] -   margin.stats [[  1 ] ] } } else  {   sf.list -   list ( size.factors =   list (   margin.stats [[  1 ] ] ) , index =   rep (  1 ,   nrow (  object ) ) ) } # Scaling the size factors to the library size.   cpm_mat -  counts_mat   mean.lib.size -   mean (   margin.stats [[  1 ] ] )   by.type -   split (   seq_along (   sf.list $ index ) ,   sf.list $ index )  for  ( g in   seq_along (  by.type ) )  {   chosen -   by.type [[  g ] ]   sf -    sf.list $ size.factors [[  g ] ]   scaled.sf -    sf /   mean (  sf ) *  mean.lib.size    cpm_mat [  chosen , ] -   .compute_exprs (   counts_mat [  chosen , , drop =  FALSE ] ,  sf , sf_to_use =  NULL , log =  FALSE , sum =  FALSE , subset_row =  NULL , logExprsOffset =  0 ) } # Restoring attributes.    rownames (  cpm_mat ) -   rownames (  object )    colnames (  cpm_mat ) -   colnames (  object )   return (  cpm_mat ) } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calculateCPM",
    "representation": "calculateCPM",
    "parameters": "function ( object , exprs_values = \"counts\" , use_size_factors = TRUE , size_factor_grouping = NULL , subset_row = NULL )",
    "body": "{  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   assays -   list (  object )    names (  assays ) -  exprs_values   object -   SingleCellExperiment (  assays ) } # Setting up the size factors.   object -   .replace_size_factors (  object ,  use_size_factors )  if (   is.null (   sizeFactors (  object ) ) )  {    sizeFactors (  object ) -   librarySizeFactors (  object ) }   object -   centreSizeFactors (  object , grouping =  size_factor_grouping )   sf_list -   .get_all_sf_sets (  object ) # Computes the average count, adjusting for size factors or library size.   extracted -   assay (  object ,  exprs_values )   normed -   .compute_exprs (  extracted , size_factor_val =   sf_list $ size.factors , size_factor_idx =   sf_list $ index , log =  FALSE , sum =  FALSE , logExprsOffset =  0 , subset_row =  subset_row )   lib_sizes -   colSums2 (   DelayedArray (  extracted ) )   cpm_mat -   normed /  (    mean (  lib_sizes ) /  1e6 )   return (  cpm_mat ) } ",
    "filename": "calculateCPM.txt"
  }
}

3.
{
  "old_function": {
    "name": "calcAverage",
    "representation": "calcAverage",
    "parameters": "function ( object , size.factors = NULL )",
    "body": "{  if (   methods :: is (  object ,  \"SingleCellExperiment\" ) )  {   sf.list -   .get_all_sf_sets (  object )   mat -   counts (  object ) } else  { # Using the lone set of size factors, if provided.   sf.list -   list ( index =   rep (  1L ,   nrow (  object ) ) , size.factors =   list (  size.factors ) )   mat -  object }   subset_row -   .subset2index (  NULL , target =  mat , byrow =  TRUE )   margin.stats -   .Call (  cxx_margin_summary ,  mat ,  0 ,   subset_row -  1L ,  FALSE ) # Set size factors to library sizes if not available.  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {     sf.list $ size.factors [[  1 ] ] -   margin.stats [[  1 ] ] } # Computes the average count, adjusting for size factors or library size.   all.ave -   .compute_exprs (  mat ,   sf.list $ size.factors , sf_to_use =   sf.list $ index , log =  FALSE , sum =  TRUE , logExprsOffset =  0 , subset_row =  NULL )    names (  all.ave ) -   rownames (  mat )   return (   all.ave /   ncol (  mat ) ) } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calcAverage",
    "representation": "calcAverage",
    "parameters": "function ( object , exprs_values = \"counts\" , use_size_factors = TRUE , size_factor_grouping = NULL , subset_row = NULL )",
    "body": "{  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   assays -   list (  object )    names (  assays ) -  exprs_values   object -   SingleCellExperiment (  assays ) } # Setting up the size factors.   object -   .replace_size_factors (  object ,  use_size_factors )  if (   is.null (   sizeFactors (  object ) ) )  {    sizeFactors (  object ) -   librarySizeFactors (  object ) }   object -   centreSizeFactors (  object , grouping =  size_factor_grouping )   sf_list -   .get_all_sf_sets (  object ) # Computes the average count, adjusting for size factors or library size.   all.ave -   .compute_exprs (   assay (  object ,  exprs_values ) , size_factor_val =   sf_list $ size.factors , size_factor_idx =   sf_list $ index , log =  FALSE , sum =  TRUE , logExprsOffset =  0 , subset_row =  subset_row )   return (   all.ave /   ncol (  object ) ) } ",
    "filename": "calcAverage.txt"
  }
}

4.
{
  "old_function": {
    "name": "readKallistoResults",
    "representation": "readKallistoResults",
    "parameters": "function ( kallisto_log = NULL , samples = NULL , directories = NULL , read_h5 = FALSE , kallisto_version = \"current\" , logExprsOffset = 1 , verbose = TRUE )",
    "body": "{   kallisto_fail -   rep (  FALSE ,   length (  samples ) ) ## Checks on arguments  if (  !   is.null (  kallisto_log ) )  {   cat (  \"Using kallisto_log argument to define samples and results directories.\" )  if (  !   is.list (  kallisto_log ) )   stop (  \"The kallisto_log argument should be a list returned by runKallisto()\" )   samples -   names (  kallisto_log )   directories -   sapply (  kallisto_log ,  function ( x )  {   x $ output_dir } )   logs -   lapply (  kallisto_log ,  function ( x )  {   x $ kallisto_log } ) ## Can only check kallisto fail if log provided   kallisto_fail -   sapply (  logs ,  function ( x )  {   any (   grepl (  \"[wW]arning|[eE]rror\" ,  x ) ) } )  if (   any (  kallisto_fail ) )  {   warning (   paste0 (  \"The kallisto job failed for the following samples:\\n \" ,   paste0 (    names (  logs ) [  kallisto_fail ] , collapse =  \"\\n\" ) ,  \"\\n It is recommended that you inspect kallisto_log for these samples.\" ) ) } } else  {   cat (  \"Kallisto log not provided - assuming all runs successful\" )  if (    is.null (  samples ) |   is.null (  directories ) )   stop (  \"If kallisto_log argument is not used, then both samples and directories must be provided.\" )  if (    length (  samples ) !=   length (  directories ) )   stop (  \"samples and directories arguments must be the same length\" ) }   samples -   samples [  !  kallisto_fail ]   directories -   directories [  !  kallisto_fail ]  if (  !   all (   dir.exists (  directories ) ) )   stop (  \"Some of the desired directories to import do not exist!\" ) ## Read first file to get size of feature set   s1 -   readKallistoResultsOneSample (   directories [  1 ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version )   nsamples -   length (  samples )   nfeatures -   nrow (   s1 $ abundance )   nbootstraps -    s1 $ run_info $ n_bootstraps   navec_samples -   rep (  NA ,  nsamples ) ## Set up results objects   pdata -   data.frame ( n_targets =  navec_samples , n_bootstraps =  navec_samples , kallisto_version =  navec_samples , index_version =  navec_samples , start_time =  navec_samples , call =  navec_samples )    rownames (  pdata ) -  samples   fdata -   data.frame ( feature_id =    s1 $ abundance $ target_id , feature_length =    s1 $ abundance $ length , feature_eff_length =    s1 $ abundance $ eff_length )    rownames (  fdata ) -    s1 $ abundance $ target_id   est_counts -   tpm -   feat_eff_len -   matrix (  NA , nrow =  nfeatures , ncol =  nsamples )    colnames (  est_counts ) -    colnames (  tpm ) -    colnames (  feat_eff_len ) -  samples    rownames (  est_counts ) -    rownames (  tpm ) -    rownames (  feat_eff_len ) -    s1 $ abundance $ target_id  if (  read_h5 )  {   bootstraps -   array (  NA , dim =   c (  nfeatures ,  nsamples ,  nbootstraps ) )    rownames (  bootstraps ) -    s1 $ abundance $ target_id    colnames (  bootstraps ) -  samples } ## Read kallisto results into results objects  if (  verbose )   cat (   paste (  \"\\nReading results for\" ,  nsamples ,  \"samples:\\n\" ) )  for  ( i in   seq_len (  nsamples ) )  {   tmp_samp -   readKallistoResultsOneSample (   directories [  i ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version ) ## counts  if (    length (    tmp_samp $ abundance $ est_counts ) !=  nfeatures )   warning (   paste (  \"Results for directory\" ,   directories [  i ] ,  \"do not match dimensions of other samples.\" ) ) else    est_counts [ ,  i ] -    tmp_samp $ abundance $ est_counts ## tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm ## feature effective length  if (    length (    tmp_samp $ abundance $ eff_length ) ==  nfeatures )    feat_eff_len [ ,  i ] -    tmp_samp $ abundance $ eff_length ## run info     pdata $ n_targets [  i ] -    tmp_samp $ run_info $ n_targets     pdata $ n_processed [  i ] -    tmp_samp $ run_info $ n_processed     pdata $ n_bootstraps [  i ] -    tmp_samp $ run_info $ n_bootstraps     pdata $ kallisto_version [  i ] -    tmp_samp $ run_info $ kallisto_version     pdata $ index_version [  i ] -    tmp_samp $ run_info $ index_version     pdata $ start_time [  i ] -    tmp_samp $ run_info $ start_time     pdata $ call [  i ] -    tmp_samp $ run_info $ call ## bootstraps  if (  read_h5 )    bootstraps [ ,  i , ] -   as.matrix (    tmp_samp $ abundance [ ,  -   c (   1 :  5 ) ] )  if (  verbose )  {   cat (  \".\" )  if (    i %%  80 ==  0 )   cat (  \"\\n\" ) } } ## Add median feature effective length to fData    fdata $ median_effective_length -   matrixStats :: rowMedians (  feat_eff_len )  if (  verbose )   cat (  \"\\n\" ) ## Produce SingleCellExperiment object   sce_out -   SingleCellExperiment (   list ( exprs =   log2 (   tpm +  logExprsOffset ) , counts =  est_counts , tpm =  tpm , feature_effective_length =  feat_eff_len ) , colData =  pdata , rowData =  fdata )  if (  verbose )   cat (  \"Using log2(TPM + 1) as 'exprs' values in output.\" )  if (  read_h5 )    bootstraps (  sce_out ) -  bootstraps ## Return SCESet object  sce_out } ",
    "filename": "kallisto-wrapper.txt"
  },
  "new_function": {
    "name": "readKallistoResults",
    "representation": "readKallistoResults",
    "parameters": "function ( kallisto_log = NULL , samples = NULL , directories = NULL , read_h5 = FALSE , kallisto_version = \"current\" , verbose = TRUE )",
    "body": "{   kallisto_fail -   rep (  FALSE ,   length (  samples ) ) ## Checks on arguments  if (  !   is.null (  kallisto_log ) )  {   cat (  \"Using kallisto_log argument to define samples and results directories.\" )  if (  !   is.list (  kallisto_log ) )   stop (  \"The kallisto_log argument should be a list returned by runKallisto()\" )   samples -   names (  kallisto_log )   directories -   sapply (  kallisto_log ,  function ( x )  {   x $ output_dir } )   logs -   lapply (  kallisto_log ,  function ( x )  {   x $ kallisto_log } ) ## Can only check kallisto fail if log provided   kallisto_fail -   sapply (  logs ,  function ( x )  {   any (   grepl (  \"[wW]arning|[eE]rror\" ,  x ) ) } )  if (   any (  kallisto_fail ) )  {   warning (   paste0 (  \"The kallisto job failed for the following samples:\\n \" ,   paste0 (    names (  logs ) [  kallisto_fail ] , collapse =  \"\\n\" ) ,  \"\\n It is recommended that you inspect kallisto_log for these samples.\" ) ) } } else  {   cat (  \"Kallisto log not provided - assuming all runs successful\" )  if (    is.null (  samples ) |   is.null (  directories ) )   stop (  \"If kallisto_log argument is not used, then both samples and directories must be provided.\" )  if (    length (  samples ) !=   length (  directories ) )   stop (  \"samples and directories arguments must be the same length\" ) }   samples -   samples [  !  kallisto_fail ]   directories -   directories [  !  kallisto_fail ]  if (  !   all (   dir.exists (  directories ) ) )   stop (  \"Some of the desired directories to import do not exist!\" ) ## Read first file to get size of feature set   s1 -   readKallistoResultsOneSample (   directories [  1 ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version )   nsamples -   length (  samples )   nfeatures -   nrow (   s1 $ abundance )   nbootstraps -    s1 $ run_info $ n_bootstraps   navec_samples -   rep (  NA ,  nsamples ) ## Set up results objects   pdata -   data.frame ( n_targets =  navec_samples , n_bootstraps =  navec_samples , kallisto_version =  navec_samples , index_version =  navec_samples , start_time =  navec_samples , call =  navec_samples )    rownames (  pdata ) -  samples   fdata -   data.frame ( feature_id =    s1 $ abundance $ target_id , feature_length =    s1 $ abundance $ length , feature_eff_length =    s1 $ abundance $ eff_length )    rownames (  fdata ) -    s1 $ abundance $ target_id   est_counts -   tpm -   feat_eff_len -   matrix (  NA , nrow =  nfeatures , ncol =  nsamples )    colnames (  est_counts ) -    colnames (  tpm ) -    colnames (  feat_eff_len ) -  samples    rownames (  est_counts ) -    rownames (  tpm ) -    rownames (  feat_eff_len ) -    s1 $ abundance $ target_id  if (  read_h5 )  {   bootstraps -   array (  NA , dim =   c (  nfeatures ,  nsamples ,  nbootstraps ) )    rownames (  bootstraps ) -    s1 $ abundance $ target_id    colnames (  bootstraps ) -  samples } ## Read kallisto results into results objects  if (  verbose )   cat (   paste (  \"\\nReading results for\" ,  nsamples ,  \"samples:\\n\" ) )  for  ( i in   seq_len (  nsamples ) )  {   tmp_samp -   readKallistoResultsOneSample (   directories [  i ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version ) ## counts  if (    length (    tmp_samp $ abundance $ est_counts ) !=  nfeatures )   warning (   paste (  \"Results for directory\" ,   directories [  i ] ,  \"do not match dimensions of other samples.\" ) ) else    est_counts [ ,  i ] -    tmp_samp $ abundance $ est_counts ## tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm ## feature effective length  if (    length (    tmp_samp $ abundance $ eff_length ) ==  nfeatures )    feat_eff_len [ ,  i ] -    tmp_samp $ abundance $ eff_length ## run info     pdata $ n_targets [  i ] -    tmp_samp $ run_info $ n_targets     pdata $ n_processed [  i ] -    tmp_samp $ run_info $ n_processed     pdata $ n_bootstraps [  i ] -    tmp_samp $ run_info $ n_bootstraps     pdata $ kallisto_version [  i ] -    tmp_samp $ run_info $ kallisto_version     pdata $ index_version [  i ] -    tmp_samp $ run_info $ index_version     pdata $ start_time [  i ] -    tmp_samp $ run_info $ start_time     pdata $ call [  i ] -    tmp_samp $ run_info $ call ## bootstraps  if (  read_h5 )    bootstraps [ ,  i , ] -   as.matrix (    tmp_samp $ abundance [ ,  -   c (   1 :  5 ) ] )  if (  verbose )  {   cat (  \".\" )  if (    i %%  80 ==  0 )   cat (  \"\\n\" ) } } ## Add median feature effective length to fData    fdata $ median_effective_length -   DelayedMatrixStats :: rowMedians (   DelayedArray (  feat_eff_len ) )  if (  verbose )   cat (  \"\\n\" ) ## Produce SingleCellExperiment object   sce_out -   SingleCellExperiment (   list ( counts =  est_counts , tpm =  tpm , feature_effective_length =  feat_eff_len ) , colData =  pdata , rowData =  fdata )  if (  read_h5 )    bootstraps (  sce_out ) -  bootstraps ## Return SCESet object  sce_out } ",
    "filename": "kallisto-wrapper.txt"
  }
}

5.
{
  "old_function": {
    "name": "normalizeSCE",
    "representation": "normalizeSCE",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (   exprs_values ==  \"exprs\" )  {   exprs_values -  \"logcounts\" }   exprs_mat -   assay (  object , i =  exprs_values )  if (   exprs_values ==  \"counts\" )  {   sf.list -   .get_all_sf_sets (  object )  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {   warning (  \"using library sizes as size factors\" )     sf.list $ size.factors [[  1 ] ] -   .general_colSums (  exprs_mat ) } ## figuring out how many controls have their own size factors   spike.names -   spikeNames (  object )   no.spike.sf -  !   spike.names %in%   sf.list $ available  if (   any (  no.spike.sf ) )  {   warning (   sprintf (  \"spike-in transcripts in '%s' should have their own size factors\" ,    spike.names [  no.spike.sf ] [  1 ] ) ) } } else  { # ignoring size factors for non-count data.   sf.list -   list ( size.factors =   rep (  1 ,   ncol (  object ) ) , index =  NULL ) } ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (  exprs_mat ,   sf.list $ size.factors , sf_to_use =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {  if (  return_norm_as_exprs )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"norm_exprs\" ) -  norm_exprs } } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## centering all existing size factors if requested  if (    exprs_values ==  \"counts\" undefined  centre_size_factors )  {   sf -   sizeFactors (  object )  if (  !   is.null (  sf ) )  {   sf -   sf /   mean (  sf )    sizeFactors (  object ) -  sf } # ... and for all controls.  for  ( type in   sf.list $ available )  {   sf -   sizeFactors (  object , type =  type )   sf -   sf /   mean (  sf )    sizeFactors (  object , type =  type ) -  sf } } ## return object   return (  object ) } ",
    "filename": "normalisation.txt"
  },
  "new_function": {
    "name": "normalizeSCE",
    "representation": "normalizeSCE",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , size_factor_grouping = NULL )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) }  if (  centre_size_factors )  {   object -   centreSizeFactors (  object , grouping =  size_factor_grouping ) }   sf.list -   .get_all_sf_sets (  object ) ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (   assay (  object , i =  exprs_values ) , size_factor_val =   sf.list $ size.factors , size_factor_idx =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "filename": "normalizeSCE.txt"
  }
}

6.
{
  "old_function": {
    "name": "plotScater",
    "representation": "plotScater",
    "parameters": "function ( x , block1 = NULL , block2 = NULL , colour_by = NULL , nfeatures = 500 , exprs_values = \"counts\" , ncol = 3 , linewidth = 1.5 , theme_size = 10 )",
    "body": "{  if (  !   is (  x ,  \"SingleCellExperiment\" ) )   stop (  \"x must be of class SingleCellExperiment\" )  if (  !   is.null (  block1 ) )  {  if (  !  (   block1 %in%   colnames (   colData (  x ) ) ) )   stop (  \"The block1 argument must either be NULL or a column of colData(x).\" ) }  if (  !   is.null (  block2 ) )  {  if (  !  (   block2 %in%   colnames (   colData (  x ) ) ) )   stop (  \"The block2 argument must either be NULL or a column of colData(x).\" ) } ## Setting values to colour by.   colour_by_out -   .choose_vis_values (  x ,  colour_by )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## Define an expression matrix depending on which values we're using   exprs_mat -   assay (  x , i =  exprs_values ) ## Use plyr to get the sequencing real estate accounted for by features   nfeatures_total -   nrow (  exprs_mat )   seq_real_estate -   t (   plyr :: aaply (  exprs_mat ,  2 , .fun =  function ( x )  {   cumsum (   sort (  x , decreasing =  TRUE ) ) } ) )    rownames (  seq_real_estate ) -   seq_len (  nfeatures_total )   nfeatures_to_plot -  nfeatures   to_plot -   seq_len (  nfeatures_to_plot )   seq_real_estate_long -   reshape2 :: melt (   seq_real_estate [  to_plot , ] , value.name =  exprs_values ) ## Get the proportion of the library accounted for by the top features   prop_library -   reshape2 :: melt (   t (    t (   seq_real_estate [  to_plot , ] ) /   .general_colSums (  exprs_mat ) ) , value.name =  \"prop_library\" )    colnames (  seq_real_estate_long ) -   c (  \"Feature\" ,  \"Cell\" ,  exprs_values )    seq_real_estate_long $ Proportion_Library -   prop_library $ prop_library ## Add block and colour_by information if provided  if (  !   is.null (  block1 ) )   seq_real_estate_long -   dplyr :: mutate (  seq_real_estate_long , block1 =   as.factor (   rep (   x [[  block1 ] ] , each =  nfeatures_to_plot ) ) )  if (  !   is.null (  block2 ) )   seq_real_estate_long -   dplyr :: mutate (  seq_real_estate_long , block2 =   as.factor (   rep (   x [[  block2 ] ] , each =  nfeatures_to_plot ) ) )  if (  !   is.null (  colour_by ) )   seq_real_estate_long -   dplyr :: mutate (  seq_real_estate_long , colour_by =   rep (  colour_by_vals , each =  nfeatures_to_plot ) ) ## Set up plot  if (   is.null (  colour_by ) )  {   plot_out -    ggplot (  seq_real_estate_long ,   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" ) ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  linewidth ) } else  {   plot_out -    ggplot (  seq_real_estate_long ,   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" , colour =  \"colour_by\" ) ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  linewidth ) } ## Deal with blocks for grid  if (  !  (    is.null (  block1 ) |   is.null (  block2 ) ) )   plot_out -   plot_out +   facet_grid (   block2 ~  block1 ) else  {  if (   !   is.null (  block1 ) undefined   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block1 , ncol =  ncol ) }  if (    is.null (  block1 ) undefined  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block2 , ncol =  ncol ) } } ## Add extra plot theme and details  if (  !   is.null (   seq_real_estate_long $ colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   seq_real_estate_long $ colour_by ,  colour_by ) }   plot_out -    plot_out +   xlab (  \"Number of features\" ) +   ylab (  \"Cumulative proportion of library\" )  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size ) ## Return plot  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotScater",
    "representation": "plotScater",
    "parameters": "function ( x , nfeatures = 500 , exprs_values = \"counts\" , colour_by = NULL , by_exprs_values = exprs_values , by_show_single = FALSE , block1 = NULL , block2 = NULL , ncol = 3 , line_width = 1.5 , theme_size = 10 )",
    "body": "{  if (  !   is (  x ,  \"SingleCellExperiment\" ) )  {   stop (  \"x must be of class SingleCellExperiment\" ) }   block1_out -   .choose_vis_values (  x ,  block1 , mode =  \"column\" , search =  \"metadata\" )   block1 -   block1_out $ name   block1_vals -   block1_out $ val   block2_out -   .choose_vis_values (  x ,  block2 , mode =  \"column\" , search =  \"metadata\" )   block2 -   block2_out $ name   block2_vals -   block2_out $ val ## Setting values to colour by.   colour_by_out -   .choose_vis_values (  x ,  colour_by , mode =  \"column\" , search =  \"any\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## Define an expression matrix depending on which values we're using   exprs_mat -   assay (  x , i =  exprs_values )   nfeatures -   min (  nfeatures ,   nrow (  exprs_mat ) ) ## Use C++ to get the sequencing real estate accounted for by features   to_plot -   seq_len (  nfeatures )   ncells -   ncol (  exprs_mat )   seq_real_estate -   .Call (  cxx_calc_top_features ,  exprs_mat ,  to_plot ,  NULL )   seq_real_estate_long -   data.frame ( Feature =   rep (  to_plot , each =  ncells ) , Cell =   rep (   seq_len (  ncells ) ,  nfeatures ) )    seq_real_estate_long $ Proportion_Library -    unlist (  seq_real_estate ) /  100 ## Add block and colour_by information if provided    seq_real_estate_long $ block1 -   rep (  block1_vals ,  nfeatures )    seq_real_estate_long $ block2 -   rep (  block2_vals ,  nfeatures )    seq_real_estate_long $ colour_by -   rep (  colour_by_vals ,  nfeatures ) ## Set up plot   aes -   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" )  if (  !   is.null (  colour_by ) )  {    aes $ colour -   as.symbol (  \"colour_by\" ) }   plot_out -    ggplot (  seq_real_estate_long ,  aes ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  line_width ) ## Deal with blocks for grid  if (   !   is.null (  block1 ) undefined  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_grid (   block2 ~  block1 ) } else  {  if (  !   is.null (  block1 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block1 , ncol =  ncol ) } else  if (  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block2 , ncol =  ncol ) } } ## Add extra plot theme and details  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   seq_real_estate_long $ colour_by ,  colour_by ) }   plot_out -    plot_out +   xlab (  \"Number of features\" ) +   ylab (  \"Cumulative proportion of library\" )  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )  {   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) } else  {   plot_out -   plot_out +   theme_bw (  theme_size ) }  plot_out } ",
    "filename": "plotScater.txt"
  }
}

7.
{
  "old_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , x_position = NULL , y_position = NULL , exprs_values = \"logcounts\" , theme_size = 24 , legend = \"auto\" )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"Object must be of class SingleCellExperiment\" ) ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ## Checking colour validity   colour_by_out -   .choose_vis_values (  object ,  colour_by , cell_control_default =  TRUE , check_features =  TRUE , exprs_values =  exprs_values )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## obtain well positions  if (  !   is.null (  plate_position ) )  {  if (    length (  plate_position ) !=   ncol (  object ) )   stop (  \"Supplied plate_position argument must have same length as number of columns of SingleCellExperiment object.\" )   plate_position_char -  plate_position } else   plate_position_char -   object $ plate_position  if (   is.null (  plate_position_char ) )  {  if (    is.null (  x_position ) ||   is.null (  y_position ) )   stop (  \"If plate_position is NULL then both x_position and y_position must be supplied.\" )   plate_position_x -  x_position   plate_position_y -  y_position } else  {   plate_position_y -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position_char )   plate_position_y -   factor (  plate_position_y ,   rev (   sort (   unique (  plate_position_y ) ) ) )   plate_position_x -   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position_char )   plate_position_x -   ordered (   as.integer (  plate_position_x ) ) } ## Define data.frame for plotting   df_to_plot -   data.frame (  plate_position_x ,  plate_position_y )  if (  !   is.null (  plate_position_char ) )    df_to_plot [[  \"plate_position_char\" ] ] -  plate_position_char    df_to_plot $ colour_by -  colour_by_vals ## make the plot   aesth -   aes ( x =  plate_position_x , y =  plate_position_y , fill =  colour_by )  if (  !   is.null (  plate_position_char ) )    aesth $ label -   as.symbol (  \"plate_position_char\" )   plot_out -    ggplot (  df_to_plot ,  aesth ) +   geom_point ( shape =  21 , size =  theme_size , colour =  \"gray50\" )  if (  !   is.null (  plate_position_char ) )   plot_out -   plot_out +   geom_text ( colour =  \"gray90\" ) ## make sure colours are nice   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =  TRUE ) ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## return plot  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , size_by = NULL , shape_by = NULL , by_exprs_values = \"logcounts\" , by_show_single = FALSE , legend = TRUE , theme_size = 24 , alpha = 0.6 , size = 24 )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"Object must be of class SingleCellExperiment\" ) } ## obtain well positions  if (  !   is.list (  plate_position ) )  {  if (   is.null (  plate_position ) )  {   plate_position -   object $ plate_position }  if (   any (  !   grepl (  \"^[A-Z][0-9]+$\" ,  plate_position ) ) )  {   stop (  \"invalid format specified in 'plate_position'\" ) }   y_position -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position )   x_position -   as.integer (   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position ) ) } else  {   x_position -   as.integer (   plate_position $ column )   y_position -   as.character (   plate_position $ row )   plate_position -  NULL }   x_position -   as.factor (  x_position )   y_position -   factor (  y_position , levels =   rev (  LETTERS ) ) # Up-   df_to_plot -   data.frame ( X =  x_position , Y =  y_position ) ## checking visualization arguments   vis_out -   .incorporate_common_vis (  df_to_plot , se =  object , mode =  \"column\" , colour_by =  colour_by , shape_by =  shape_by , size_by =  size_by , by_exprs_values =  by_exprs_values , by_show_single =  by_show_single )   df_to_plot -   vis_out $ df   colour_by -   vis_out $ colour_by   shape_by -   vis_out $ shape_by   size_by -   vis_out $ size_by ## make the plot with appropriate colours.   plot_out -   ggplot (  df_to_plot ,   aes_string ( x =  \"X\" , y =  \"Y\" ) )   point_out -   .get_point_args (  colour_by ,  shape_by ,  size_by , alpha =  alpha , size =  size )   plot_out -   plot_out +   do.call (  geom_point ,   point_out $ args )  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =   point_out $ fill ) } ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired   plot_out -   .add_extra_guide (  plot_out ,  shape_by ,  size_by )  if (  !  legend )  {   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) }  plot_out } ",
    "filename": "plotPlatePosition.txt"
  }
}

8.
{
  "old_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , exprs_values = \"counts\" , feature_controls = NULL , cell_controls = NULL , nmads = 5 , pct_feature_controls_threshold = 80 )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"object must be a SingleCellExperiment\" )   exprs_mat -   assay (  object , i =  exprs_values )  if (    exprs_values ==  \"counts\" ||   exprs_values ==  \"cpm\" )  {   linear -  TRUE } else  {   linear -  FALSE } ## Adding general metrics for each cell.   cd -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  NULL , subset_type =  NULL , linear =  TRUE )   rd -   DataFrame ( is_feature_control =   logical (   nrow (  exprs_mat ) ) , row.names =   rownames (  exprs_mat ) ) ## Adding metrics for the technical controls.   n_feature_sets -   length (  feature_controls )  if (  n_feature_sets )  {  if (   is.null (   names (  feature_controls ) ) )  {   stop (  \"feature_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  feature_controls , FUN =  .subset2index , target =  exprs_mat )   is_fcon -   Reduce (  union ,  reindexed )     rd $ is_feature_control [  is_fcon ] -  TRUE # Adding feature controls.  for  ( f in   seq_len (  n_feature_sets ) )  {   cur.index -   logical (   nrow (  exprs_mat ) )    cur.index [   reindexed [[  f ] ] ] -  TRUE    rd [[   paste0 (  \"is_feature_control_\" ,    names (  reindexed ) [  f ] ) ] ] -  cur.index } # Running through all endogenous genes.   is_endog -   which (  !   rd $ is_feature_control )   cd_endog -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  is_endog , subset_type =  \"endogenous\" , linear =  linear ) # Running through all feature controls.   cd_fcon -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  is_fcon , subset_type =  \"feature_control\" , linear =  linear ) # Running through each of the feature controls.   cd_per_fcon -   vector (  \"list\" ,  n_feature_sets )  for  ( f in   seq_len (  n_feature_sets ) )  {    cd_per_fcon [[  f ] ] -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =   reindexed [[  f ] ] , subset_type =    names (  reindexed ) [  f ] , linear =  linear ) }   cd -   do.call (  cbind ,   c (   list (  cd ,  cd_endog ,  cd_fcon ) ,  cd_per_fcon ) ) } ## Define cell controls ### Determine if vector or list   rd_all -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  NULL , subset_type =  NULL , linear =  linear )   rd -   cbind (  rd ,  rd_all )    cd $ is_cell_control -   logical (   ncol (  exprs_mat ) )   n_cell_sets -   length (  cell_controls )  if (  n_cell_sets )  { # Converting indices to integer.   reindexed -   lapply (  cell_controls , FUN =  .subset2index , target =  exprs_mat , byrow =  FALSE )   is_ccon -   Reduce (  union ,  reindexed )     cd $ is_cell_control [  is_ccon ] -  TRUE # Adding sets to the colData.  for  ( cx in   seq_len (  n_cell_sets ) )  {   current_control -   logical (   ncol (  exprs_mat ) )    current_control [   reindexed [[  cx ] ] ] -  TRUE    cd [[   paste0 (  \"is_cell_control_\" ,    names (  reindexed ) [  cx ] ) ] ] -  current_control } # Adding statistics for non-control cells.   is_noncon -   which (  !   cd $ is_cell_control )   rd_noncon -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  is_noncon , subset_type =  \"non_control\" , linear =  linear ) # Adding statistics for all control cells.   rd_con -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  is_ccon , subset_type =  \"cell_control\" , linear =  linear ) # Adding statistics for each set of control cells.   rd_collected -   vector (  \"list\" ,  n_cell_sets )  for  ( cx in   seq_len (  n_cell_sets ) )  {   rd_current -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =   reindexed [[  cx ] ] , subset_type =    names (  reindexed ) [  cx ] , linear =  linear )    rd_collected [[  cx ] ] -  rd_current }   rd -   do.call (  cbind ,   c (   list (  rd ,  rd_con ,  rd_noncon ) ,  rd_collected ) ) } ### Remove columns to be replaced   old_rd -   rowData (  object )   old_rd -   old_rd [ ,  !  (    colnames (  old_rd ) %in%   colnames (  rd ) ) , drop =  FALSE ]   rd -   cbind (  old_rd ,  rd )    rowData (  object ) -  rd   old_cd -   colData (  object )   old_cd -   old_cd [ ,  !  (    colnames (  old_cd ) %in%   colnames (  cd ) ) , drop =  FALSE ]   cd -   cbind (  old_cd ,  cd )    colData (  object ) -  cd   return (  object ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , exprs_values = \"counts\" , feature_controls = NULL , cell_controls = NULL , percent_top = c ( 50 , 100 , 200 , 500 ) , detection_limit = 0 , use_spikes = TRUE , compact = FALSE )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"object must be a SingleCellExperiment\" ) }   exprs_mat -   assay (  object , i =  exprs_values )   percent_top -   as.integer (  percent_top ) ### Adding general metrics for each cell. ### # We first assemble the list of all metrics (all MUST be first). # We also add any existing spike-ins to this set unless otherwise specified.   all_feature_sets -   list ( all =  NULL )   existing_spikes -   spikeNames (  object )  if (   use_spikes undefined   length (  existing_spikes ) )  {   existing -   vector (  \"list\" ,   length (  existing_spikes ) )    names (  existing ) -  existing_spikes  for  ( spset in  existing_spikes )  {    existing [[  spset ] ] -   isSpike (  object , type =  spset ) }   already_there -    names (  existing ) %in%   names (  feature_controls )  if (   any (  already_there ) )  {   warning (   sprintf (  \"spike-in set '%s' overwritten by feature_controls set of the same name\" ,     names (  existing ) [  already_there ] [  1 ] ) ) }   feature_controls -   c (  feature_controls ,   existing [  !  already_there ] ) }   feature_set_rdata -   list ( )  if (   length (  feature_controls ) )  {  if (   is.null (   names (  feature_controls ) ) )  {   stop (  \"feature_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  feature_controls , FUN =  .subset2index , target =  exprs_mat )    names (  reindexed ) -   sprintf (  \"feature_control_%s\" ,   names (  reindexed ) )   is_fcon -   Reduce (  union ,  reindexed )   is_endog -    seq_len (   nrow (  exprs_mat ) ) [  -  is_fcon ]   all_feature_sets -   c (  all_feature_sets ,   list ( endogenous =  is_endog , feature_control =  is_fcon ) ,  reindexed ) # But storing logical vectors in the metadata.   feature_set_rdata -   vector (  \"list\" ,    length (  reindexed ) +  1 )    names (  feature_set_rdata ) -   c (  \"feature_control\" ,   names (  reindexed ) )  for  ( set in   names (  feature_set_rdata ) )  {   new_set -   logical (   nrow (  exprs_mat ) )    new_set [   all_feature_sets [[  set ] ] ] -  TRUE    feature_set_rdata [[  set ] ] -  new_set } } else  {   feature_set_rdata -   list ( feature_control =   logical (   nrow (  object ) ) ) } # Computing the cell-level metrics for each set.   cell_stats_by_feature_set -  all_feature_sets   total_exprs -  NULL  for  ( set in   names (  all_feature_sets ) )  {    cell_stats_by_feature_set [[  set ] ] -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =   all_feature_sets [[  set ] ] , percent_top =  percent_top , detection_limit =  detection_limit , total_exprs =  total_exprs , legacy =  !  compact )  if (   set ==  \"all\" )  {   total_exprs -    cell_stats_by_feature_set [[  set ] ] [[   paste0 (  \"total_\" ,  exprs_values ) ] ] } } ### Adding general metrics for each feature. ### # We first assemble thie list of all metrics (all MUST be first).   all_cell_sets -   list ( all =  NULL )   cell_set_cdata -   list ( )  if (   length (  cell_controls ) )  {  if (   is.null (   names (  cell_controls ) ) )  {   stop (  \"cell_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  cell_controls , FUN =  .subset2index , target =  exprs_mat , byrow =  FALSE )    names (  reindexed ) -   sprintf (  \"cell_control_%s\" ,   names (  reindexed ) )   is_ccon -   Reduce (  union ,  reindexed )   is_ncon -    seq_len (   ncol (  exprs_mat ) ) [  -  is_ccon ]   all_cell_sets -   c (  all_cell_sets ,   list ( non_control =  is_ncon , cell_control =  is_ccon ) ,  reindexed ) # But storing logical vectors in the metadata.   cell_set_cdata -   vector (  \"list\" ,    length (  reindexed ) +  1 )    names (  cell_set_cdata ) -   c (  \"cell_control\" ,   names (  reindexed ) )  for  ( set in   names (  cell_set_cdata ) )  {   new_set -   logical (   ncol (  exprs_mat ) )    new_set [   all_cell_sets [[  set ] ] ] -  TRUE    cell_set_cdata [[  set ] ] -  new_set } } else  {   cell_set_cdata -   list ( cell_control =   logical (   ncol (  object ) ) ) } # Computing the feature-level metrics for each set.   feature_stats_by_cell_set -  all_cell_sets   total_exprs -  NULL  for  ( set in   names (  all_cell_sets ) )  {    feature_stats_by_cell_set [[  set ] ] -   .get_qc_metrics_per_feature (  exprs_mat , exprs_type =  exprs_values , subset_col =   all_cell_sets [[  set ] ] , detection_limit =  detection_limit , total_exprs =  total_exprs , legacy =  !  compact )  if (   set ==  \"all\" )  {   total_exprs -    feature_stats_by_cell_set [[  set ] ] [[   paste0 (  \"total_\" ,  exprs_values ) ] ] } } ### Formatting output depending on whether we're compacting or not. ###  if (  compact )  {   scater_cd -   .convert_to_nested_DataFrame (    colData (  object ) $ scater_qc ,  cell_set_cdata ,  cell_stats_by_feature_set )   scater_rd -   .convert_to_nested_DataFrame (    rowData (  object ) $ scater_qc ,  feature_set_rdata ,  feature_stats_by_cell_set )     colData (  object ) $ scater_qc -  scater_cd     rowData (  object ) $ scater_qc -  scater_rd } else  {   message (  \"Note that the names of some metrics have changed, see 'Renamed metrics' in ?calculateQCMetrics.\r\nOld names are currently maintained for back-compatibility, but may be removed in future releases.\" )   scater_cd -   .convert_to_full_DataFrame (   colData (  object ) ,  cell_set_cdata ,  cell_stats_by_feature_set , trim.fun =  function ( x )   sub (  \"^feature_control_\" ,  \"\" ,  x ) )   scater_rd -   .convert_to_full_DataFrame (   rowData (  object ) ,  feature_set_rdata ,  feature_stats_by_cell_set , trim.fun =  function ( x )   sub (  \"^cell_control_\" ,  \"\" ,  x ) )    colData (  object ) -  scater_cd    rowData (  object ) -  scater_rd }   return (  object ) } ",
    "filename": "calculateQCMetrics.txt"
  }
}

9.
{
  "old_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL , min.diff = NA )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }  if (   any (   is.na (  metric ) ) )  {   warning (  \"missing values ignored during outlier detection\" ) }  if (  !   is.null (  batch ) )  {   N -   length (  metric )  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) } # Coercing non-NULL subset into a logical vector.  if (  !   is.null (  subset ) )  {   new.subset -   logical (  N )    names (  new.subset ) -   names (  metric )    new.subset [  subset ] -  TRUE   subset -  new.subset } # Computing QC metrics for each batch.   by.batch -   split (   seq_len (  N ) ,  batch )   collected -   logical (  N )  for  ( b in  by.batch )  {    collected [  b ] -   Recall (   metric [  b ] , nmads =  nmads , type =  type , log =  FALSE , subset =   subset [  b ] , batch =  NULL , min.diff =  min.diff ) }   return (  collected ) } # Computing median/MAD (possibly based on subset of the data).  if (  !   is.null (  subset ) )  {   submetric -   metric [  subset ]  if (    length (  submetric ) ==  0L )  {   warning (  \"no observations remaining after subsetting\" ) } } else  {   submetric -  metric }   cur.med -   median (  submetric , na.rm =  TRUE )   cur.mad -   mad (  submetric , center =  cur.med , na.rm =  TRUE )   diff.val -   max (  min.diff ,   nmads *  cur.mad , na.rm =  TRUE )   upper.limit -   cur.med +  diff.val   lower.limit -   cur.med -  diff.val   type -   match.arg (  type )  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   return (    metric undefined  lower.limit |   upper.limit undefined  metric ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL , min_diff = NA )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }  if (   any (   is.na (  metric ) ) )  {   warning (  \"missing values ignored during outlier detection\" ) }  if (  !   is.null (  batch ) )  {   N -   length (  metric )  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) } # Coercing non-NULL subset into a logical vector.  if (  !   is.null (  subset ) )  {   new.subset -   logical (  N )    names (  new.subset ) -   names (  metric )    new.subset [  subset ] -  TRUE   subset -  new.subset } # Computing QC metrics for each batch.   by.batch -   split (   seq_len (  N ) ,  batch )   collected -   logical (  N )  for  ( b in  by.batch )  {    collected [  b ] -   Recall (   metric [  b ] , nmads =  nmads , type =  type , log =  FALSE , subset =   subset [  b ] , batch =  NULL , min_diff =  min_diff ) }   return (  collected ) } # Computing median/MAD (possibly based on subset of the data).  if (  !   is.null (  subset ) )  {   submetric -   metric [  subset ]  if (    length (  submetric ) ==  0L )  {   warning (  \"no observations remaining after subsetting\" ) } } else  {   submetric -  metric }   cur.med -   median (  submetric , na.rm =  TRUE )   cur.mad -   mad (  submetric , center =  cur.med , na.rm =  TRUE )   diff.val -   max (  min_diff ,   nmads *  cur.mad , na.rm =  TRUE )   upper.limit -   cur.med +  diff.val   lower.limit -   cur.med -  diff.val   type -   match.arg (  type )  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   return (    metric undefined  lower.limit |   upper.limit undefined  metric ) } ",
    "filename": "isOutlier.txt"
  }
}

10.
{
  "old_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , col_by_variable = \"total_features\" , n = 50 , drop_features = NULL , exprs_values = \"counts\" , feature_names_to_plot = NULL )",
    "body": "{ ## Check that variable to colour points exists  if (  !  (   col_by_variable %in%   colnames (   colData (  object ) ) ) )  {   stop (  \"col_by_variable not found in colData(object).\r\n             Please make sure colData(object)[, variable] exists.\" )   plot_cols -  FALSE } else   plot_cols -  TRUE   x -    colData (  object ) [ ,  col_by_variable ] #     x_na #     x ## Determine type of variable   typeof_x -   .getTypeOfVariable (  object ,  col_by_variable ) ## Figure out which features to drop  if (  !  (    is.null (  drop_features ) |    length (  drop_features ) ==  0 ) )  {  if (   is.character (  drop_features ) )   drop_features -   which (    rownames (  object ) %in%  drop_features )  if (   is.logical (  drop_features ) )   object -   object [  !  drop_features , ] else   object -   object [  -  drop_features , ] } ## Compute QC metrics on the (possibly) modified SingleCellExperiment object to make sure ## we have the relevant values for this set of features  if (  !   is.null (    rowData (  object ) $ is_feature_control ) )   object -   calculateQCMetrics (  object , feature_controls =   list ( all =    rowData (  object ) $ is_feature_control ) ) else   object -   calculateQCMetrics (  object ) ## Define expression values to be used   exprs_values -   match.arg (  exprs_values ,   c (  \"logcounts\" ,  \"tpm\" ,  \"cpm\" ,  \"fpkm\" ,  \"counts\" ) )   exprs_mat -   assay (  object ,  exprs_values )  if (    is.null (  exprs_mat ) undefined  !   is.null (   counts (  object ) ) )  {   exprs_mat -   counts (  object )   message (  \"Using counts as expression values.\" )   exprs_values -  \"counts\" } else  if (   is.null (  exprs_mat ) )  {   exprs_mat -   exprs (  object )   message (  \"Using exprs(object) values as expression values.\" )   exprs_values -  \"logcounts\" }  if (   exprs_values ==  \"logcounts\" )   exprs_mat -    2 ^  exprs_mat -   object @ logExprsOffset ## Find the most highly expressed features in this dataset ### Order by total feature counts across whole dataset   rdata -   rowData (  object )  if (    paste0 (  \"rank_\" ,  exprs_values ) %in%   colnames (  rdata ) )   oo -   order (   rdata [[   paste0 (  \"rank_\" ,  exprs_values ) ] ] , decreasing =  TRUE ) else  {  if (   \"rank_counts\" %in%   colnames (  rdata ) )  {   oo -   order (   rdata [[  \"rank_counts\" ] ] , decreasing =  TRUE )   exprs_values -  \"counts\"   message (  \"Using counts to order total expression of features.\" ) } else  {   exprs_values -  \"logcounts\"   oo -   order (   rdata [[  \"rank_exprs\" ] ] , decreasing =  TRUE )   message (  \"Using 'exprs' to order total expression of features.\" ) } } ## define feature names for plot  if (    is.null (  feature_names_to_plot ) ||   is.null (    rowData (  object ) [[  feature_names_to_plot ] ] ) )    rdata $ feature -   factor (   rownames (  object ) , levels =    rownames (  object ) [   rev (  oo ) ] ) else    rdata $ feature -   factor (    rowData (  object ) [[  feature_names_to_plot ] ] , levels =     rowData (  object ) [[  feature_names_to_plot ] ] [   rev (  oo ) ] )    rdata $ Feature -   rdata $ feature ## Check if is_feature_control is defined  if (   is.null (   rdata $ is_feature_control ) )    rdata $ is_feature_control -   rep (  FALSE ,   nrow (  rdata ) ) ## Determine percentage expression accounted for by top features across all ## cells   total_exprs -   sum (  exprs_mat )   top50_pctage -    100 *   sum (    .general_rowSums (  exprs_mat ) [   oo [   1 :  n ] ] ) /  total_exprs ## Determine percentage of counts for top features by cell   df_pct_exprs_by_cell -  (    100 *   t (   exprs_mat [   oo [   1 :  n ] , ] ) /   .general_colSums (  exprs_mat ) )   pct_total -    100 *   .general_rowSums (  exprs_mat ) /  total_exprs    rdata [[  \"pct_total\" ] ] -  pct_total ## Melt dataframe so it is conducive to ggplot  if (   is.null (   rownames (  rdata ) ) )    rownames (  rdata ) -   as.character (   rdata $ feature )   df_pct_exprs_by_cell -   as.matrix (  df_pct_exprs_by_cell ) # coercing to a normal matrix.   df_pct_exprs_by_cell_long -   reshape2 :: melt (  df_pct_exprs_by_cell )    colnames (  df_pct_exprs_by_cell_long ) -   c (  \"Cell\" ,  \"Tags\" ,  \"value\" )    df_pct_exprs_by_cell_long $ Feature -   rdata [   as.character (   df_pct_exprs_by_cell_long $ Tags ) ,  \"feature\" ]    df_pct_exprs_by_cell_long $ Tags -   factor (   df_pct_exprs_by_cell_long $ Tags , levels =    rownames (  object ) [   rev (   oo [   1 :  n ] ) ] )    df_pct_exprs_by_cell_long $ Feature -   factor (   df_pct_exprs_by_cell_long $ Feature , levels =    rdata $ feature [   rev (   oo [   1 :  n ] ) ] ) ## Add colour variable information  if (   typeof_x ==  \"discrete\" )    df_pct_exprs_by_cell_long $ colour_by -   factor (  x ) else    df_pct_exprs_by_cell_long $ colour_by -  x ## Make plot   plot_most_expressed -         ggplot (  df_pct_exprs_by_cell_long ,   aes_string ( y =  \"Feature\" , x =  \"value\" , colour =  \"colour_by\" ) ) +   geom_point ( alpha =  0.6 , shape =  124 ) +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top50_pctage , digits =  3 ) ,  \"% of total\" ) ) +   ylab (  \"Feature\" ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (   typeof_x ==  \"discrete\" )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_pct_exprs_by_cell_long $ colour_by ,  col_by_variable ) #         plot_most_expressed #             ggthemes::scale_colour_tableau(name = col_by_variable) } else  {   plot_most_expressed -   plot_most_expressed +   scale_colour_gradient ( name =  col_by_variable , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) }     plot_most_expressed +   geom_point (   aes_string ( x =  \"as.numeric(pct_total)\" , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =   as.data.frame (   rdata [   oo [   1 :  n ] , ] ) , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , controls , colour_cells_by , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , by_show_single = TRUE , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{  if (   is.null (   rownames (  object ) ) )  {    rownames (  object ) -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) }  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   object -   object [  -  to_discard , ] } ## Define expression values to be used ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   .rowSums (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )   chosen -   head (  oo ,  n ) ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object ) } else  {   feature_names -    .choose_vis_values (  object ,  feature_names_to_plot , search =  \"metadata\" , mode =  \"row\" ) $ val }    rownames (  exprs_mat ) -  feature_names ## Compute expression values and reshape them for ggplot.   df_exprs_by_cell -   t (   exprs_mat [  chosen , ] )   df_exprs_by_cell -   as.matrix (  df_exprs_by_cell )  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top50_pctage -    100 *   sum (   ave_exprs [  chosen ] ) /  total_exprs   df_exprs_by_cell -    100 *  df_exprs_by_cell /   .colSums (  exprs_mat ) }   df_exprs_by_cell_long -   reshape2 :: melt (  df_exprs_by_cell )    colnames (  df_exprs_by_cell_long ) -   c (  \"Cell\" ,  \"Tag\" ,  \"value\" )    df_exprs_by_cell_long $ Tag -   factor (   df_exprs_by_cell_long $ Tag ,   rev (   feature_names [  chosen ] ) ) ## Colouring the individual dashes for the cells.  if (   missing (  colour_cells_by ) )  {   colour_cells_by -   .qc_hunter (  object ,   paste0 (  \"total_features_by_\" ,  exprs_values ) , mode =  \"column\" ) }  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   .choose_vis_values (  object ,  colour_cells_by , mode =  \"column\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )  if (  as_percentage )  {   plot_most_expressed -    plot_most_expressed +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top50_pctage , digits =  3 ) ,  \"% of total\" ) ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) } else  {   plot_most_expressed -   plot_most_expressed +   xlab (  exprs_values ) }   plot_most_expressed -     plot_most_expressed +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (  !   is.null (  colour_cells_by ) )  {  if (  !   is.numeric (   df_exprs_by_cell_long $ colour_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } else  {   plot_most_expressed -   plot_most_expressed +   scale_colour_gradient ( name =  colour_cells_by , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) } } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =   factor (  feature_names , levels =   rev (  feature_names ) ) )  if (  as_percentage )  {   pct_total -    100 *  ave_exprs /  total_exprs    df_to_plot $ pct_total -  pct_total   legend_val -  \"as.numeric(pct_total)\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  ave_exprs   legend_val -   sprintf (  \"as.numeric(ave_%s)\" ,  exprs_values ) } ## Check if is_feature_control is defined, and using it for colouring of the points.  if (   missing (  controls ) )  {   controls -   .qc_hunter (  object ,  \"is_feature_control\" , mode =  \"row\" ) }  if (  !   is.null (  controls ) )  {   cont_out -   .choose_vis_values (  object ,  controls , mode =  \"row\" , search =  \"metadata\" )    df_to_plot $ is_feature_control -   cont_out $ val   plot_most_expressed -     plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =   df_to_plot [  chosen , ] , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } else  {   plot_most_expressed -   plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" ) , data =   df_to_plot [  chosen , ] , fill =  \"grey80\" , colour =  \"grey30\" , shape =  21 ) }  plot_most_expressed } ",
    "filename": "plotHighestExprs.txt"
  }
}

11.
{
  "old_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "SingleCellExperiment",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (   exprs_values ==  \"exprs\" )  {   exprs_values -  \"logcounts\" }   exprs_mat -   assay (  object , i =  exprs_values )  if (   exprs_values ==  \"counts\" )  {   sf.list -   .get_all_sf_sets (  object )  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {   warning (  \"using library sizes as size factors\" )     sf.list $ size.factors [[  1 ] ] -   .general_colSums (  exprs_mat ) } ## figuring out how many controls have their own size factors   spike.names -   spikeNames (  object )   no.spike.sf -  !   spike.names %in%   sf.list $ available  if (   any (  no.spike.sf ) )  {   warning (   sprintf (  \"spike-in transcripts in '%s' should have their own size factors\" ,    spike.names [  no.spike.sf ] [  1 ] ) ) } } else  { # ignoring size factors for non-count data.   sf.list -   list ( size.factors =   rep (  1 ,   ncol (  object ) ) , index =  NULL ) } ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (  exprs_mat ,   sf.list $ size.factors , sf_to_use =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {  if (  return_norm_as_exprs )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"norm_exprs\" ) -  norm_exprs } } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## centering all existing size factors if requested  if (    exprs_values ==  \"counts\" undefined  centre_size_factors )  {   sf -   sizeFactors (  object )  if (  !   is.null (  sf ) )  {   sf -   sf /   mean (  sf )    sizeFactors (  object ) -  sf } # ... and for all controls.  for  ( type in   sf.list $ available )  {   sf -   sizeFactors (  object , type =  type )   sf -   sf /   mean (  sf )    sizeFactors (  object , type =  type ) -  sf } } ## return object   return (  object ) } ",
    "replacementFunction": "normalizeSCE",
    "filename": "normalisation.txt"
  },
  "new_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "SingleCellExperiment",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , size_factor_grouping = NULL )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) }  if (  centre_size_factors )  {   object -   centreSizeFactors (  object , grouping =  size_factor_grouping ) }   sf.list -   .get_all_sf_sets (  object ) ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (   assay (  object , i =  exprs_values ) , size_factor_val =   sf.list $ size.factors , size_factor_idx =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "replacementFunction": "normalizeSCE",
    "filename": "normalizeSCE.txt"
  }
}



##########
Parameter Default Value Changes
##########

