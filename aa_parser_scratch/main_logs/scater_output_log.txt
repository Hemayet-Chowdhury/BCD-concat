
###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_3 scater_release_3_4

{
    "package": "scater",
    "release_versions": "scater_release_3_3 scater_release_3_4",
    "desc_release_old": "1.0.4",
    "desc_release_new": "1.2.0",
    "old_release_number": 0,
    "new_release_number": 1,
    "function_removals": 0,
    "function_additions": 8,
    "parameter_removals": 1,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 4,
    "total_count": 4
}

##########
Functions Removed
##########



##########
Functions Added
##########

cellNames<-
mergeSCESet
nexprs
plotExprsVsTxLength
plotPlatePosition
runSalmon
updateSCESet
writeSCESet


##########
Removed Non Default Parameters
##########

0.
{
  "old_function": {
    "name": "plotExpressionSCESet",
    "representation": "plotExpressionSCESet",
    "parameters": "function ( object , features , x , exprs_values = \"exprs\" , colour_by = NULL , shape_by = NULL , size_by = NULL , ncol = 2 , xlab = NULL , show_median = FALSE , show_violin = TRUE , show_smooth = FALSE , theme_size = 10 , log2_values = FALSE )",
    "body": "{ ## Check object is an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet\" ) ## Define number of features to plot  if (   is.logical (  features ) )   nfeatures -   sum (  features ) else   nfeatures -   length (  features ) ## Check arguments are valid  if (   !  (   x %in%   varLabels (  object ) ) undefined  !  (   x %in%   featureNames (  object ) ) )   stop (  \"the argument 'x' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)\" )  if (   x %in%   featureNames (  object ) )  {   x_is_feature -  TRUE   show_violin -  FALSE   show_median -  FALSE } else   x_is_feature -  FALSE  if (  !   is.null (  colour_by ) )  {  if (  !  (   colour_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (  !   is.null (  shape_by ) )  {  if (  !  (   shape_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'shape_by' should specify a column of pData(object) [see varLabels(object)]\" )  if (    nlevels (   as.factor (    pData (  object ) [[  shape_by ] ] ) ) undefined  10 )   stop (  \"when coerced to a factor, 'shape_by' should have fewer than 10 levels\" ) }  if (  !   is.null (  size_by ) )  {  if (  !  (   size_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'size_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (    typeof (  features ) ==  \"character\" )  {  if (  !  (   all (   features %in%   featureNames (  object ) ) ) )   stop (  \"when the argument 'features' is of type character, all features must be in featureNames(object)\" ) }   exprs_mat -   get_exprs (  object ,  exprs_values )  if (  log2_values )  {   exprs_mat -   log2 (   exprs_mat +  1 )   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \"; log2-scale)\" ) } else   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \")\" )   to_melt -   as.matrix (   exprs_mat [  features , , drop =  FALSE ] ) ## Melt the expression data and metadata into a convenient form   evals_long -   reshape2 :: melt (  to_melt , value.name =  \"evals\" )    colnames (  evals_long ) -   c (  \"Feature\" ,  \"Cell\" ,  \"evals\" ) ## Extend the samples information   samps -   pData (  object )  if (  x_is_feature )    samps [[  x ] ] -   exprs_mat [  x , ]   samples_long -   samps [   rep (   seq_len (   ncol (  object ) ) , each =  nfeatures ) , ] ## Construct a ggplot2 aesthetic for the plot   aesth -   aes ( )    aesth $ x -   as.symbol (  x )    aesth $ y -   as.symbol (  \"evals\" )  if (  !   is.null (  colour_by ) )    aesth $ colour -   as.symbol (  colour_by ) else  {  if (  !   is.null (   is_exprs (  object ) ) )  { ## Colour by is_exprs if we can (i.e. is_exprs is not NULL)   isexpr_long -   reshape2 :: melt (    is_exprs (  object ) [  features , ] , value.name =  \"is_exprs\" )   evals_long -   dplyr :: mutate (  evals_long , Is_Expressed =   as.vector (   isexpr_long $ is_exprs ) )    aesth $ colour -   as.symbol (  \"Is_Expressed\" ) } }  if (  !   is.null (  shape_by ) )    aesth $ shape -   as.symbol (  shape_by ) ## Define sensible x-axis label if NULL  if (   is.null (  xlab ) )   xlab -  x ## Combine the expression values and sample information   object -   cbind (  evals_long ,  samples_long ) ## Make the plot   plot_out -   plotExpressionDefault (  object ,  aesth ,  ncol ,  xlab ,  ylab ,  show_median ,  show_violin ,  show_smooth ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size )  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotExpressionSCESet",
    "representation": "plotExpressionSCESet",
    "parameters": "function ( object , features , x = NULL , exprs_values = \"exprs\" , colour_by = NULL , shape_by = NULL , size_by = NULL , ncol = 2 , xlab = NULL , show_median = FALSE , show_violin = TRUE , show_smooth = FALSE , alpha = 0.6 , theme_size = 10 , log2_values = FALSE , size = NULL , scales = \"fixed\" , se = TRUE , jitter = \"swarm\" )",
    "body": "{ ## Check object is an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet\" ) ## Define number of features to plot  if (   is.logical (  features ) )   nfeatures -   sum (  features ) else   nfeatures -   length (  features ) ## Check arguments are valid  if (   is.null (  x ) )  {   x_is_feature -  FALSE } else  {  if (   !  (   x %in%   varLabels (  object ) ) undefined  !  (   x %in%   featureNames (  object ) ) )   stop (  \"the argument 'x' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)\" )  if (   x %in%   featureNames (  object ) )  {   x_is_feature -  TRUE   show_violin -  FALSE   show_median -  FALSE } else   x_is_feature -  FALSE }  if (  !   is.null (  colour_by ) )  {  if (  !  (   colour_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (  !   is.null (  shape_by ) )  {  if (  !  (   shape_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'shape_by' should specify a column of pData(object) [see varLabels(object)]\" )  if (    nlevels (   as.factor (    pData (  object ) [[  shape_by ] ] ) ) undefined  10 )   stop (  \"when coerced to a factor, 'shape_by' should have fewer than 10 levels\" ) }  if (  !   is.null (  size_by ) )  {  if (  !  (   size_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'size_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (    typeof (  features ) ==  \"character\" )  {  if (  !  (   all (   features %in%   featureNames (  object ) ) ) )   stop (  \"when the argument 'features' is of type character, all features must be in featureNames(object)\" ) }   exprs_mat -   get_exprs (  object ,  exprs_values )  if (  log2_values )  {   exprs_mat -   log2 (   exprs_mat +  1 )   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \"; log2-scale)\" ) } else   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \")\" )   to_melt -   as.matrix (   exprs_mat [  features , , drop =  FALSE ] ) ## Melt the expression data and metadata into a convenient form   evals_long -   reshape2 :: melt (  to_melt , value.name =  \"evals\" )    colnames (  evals_long ) -   c (  \"Feature\" ,  \"Cell\" ,  \"evals\" ) ## Extend the samples information   samps -   pData (  object )  if (  x_is_feature )    samps [[  x ] ] -   exprs_mat [  x , ]   samples_long -   samps [   rep (   seq_len (   ncol (  object ) ) , each =  nfeatures ) , ] ## Construct a ggplot2 aesthetic for the plot   aesth -   aes ( )  if (   is.null (  x ) )  {    aesth $ x -   as.symbol (  \"Feature\" )   one_facet -  TRUE } else  {    aesth $ x -   as.symbol (  x )   one_facet -  FALSE }    aesth $ y -   as.symbol (  \"evals\" )  if (  !   is.null (  colour_by ) )    aesth $ colour -   as.symbol (  colour_by ) else  {  if (  !   is.null (   is_exprs (  object ) ) )  { ## Colour by is_exprs if we can (i.e. is_exprs is not NULL)   isexpr_long -   reshape2 :: melt (    is_exprs (  object ) [  features , ] , value.name =  \"is_exprs\" )   evals_long -   dplyr :: mutate (  evals_long , Is_Expressed =   as.vector (   isexpr_long $ is_exprs ) )    aesth $ colour -   as.symbol (  \"Is_Expressed\" ) } }  if (  !   is.null (  shape_by ) )    aesth $ shape -   as.symbol (  shape_by ) ## Define sensible x-axis label if NULL  if (   is.null (  xlab ) )   xlab -  x ## Combine the expression values and sample information   object -   cbind (  evals_long ,  samples_long ) ## Make the plot   plot_out -   plotExpressionDefault (  object ,  aesth ,  ncol ,  xlab ,  ylab ,  show_median ,  show_violin ,  show_smooth ,  alpha ,  size ,  scales ,  one_facet ,  se ,  jitter ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size )  if (   is.null (  x ) )  { ## in this case, do not show x-axis ticks or labels   plot_out -   plot_out +   theme ( axis.text.x =   element_text ( angle =  60 , vjust =  1 , hjust =  1 ) , axis.ticks.x =   element_blank ( ) , plot.margin =   unit (   c (  .03 ,  .02 ,  .05 ,  .02 ) ,  \"npc\" ) )  if (   is.null (  colour_by ) )   plot_out -   plot_out +   guides ( fill =  \"none\" , colour =  \"none\" ) }  plot_out } ",
    "filename": "plotting.txt"
  }
}



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "plotReducedDim",
    "representation": "plotReducedDim",
    "signature": "signature (  SCESet )",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , legend = TRUE )",
    "body": "{   plotReducedDim.SCESet (  object ,  ncomponents ,  colour_by ,  shape_by ,  size_by ,  legend ) } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotReducedDim",
    "representation": "plotReducedDim",
    "signature": "signature (  SCESet )",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , theme_size = 10 , legend = \"auto\" )",
    "body": "{   plotReducedDim.SCESet (  object ,  ncomponents ,  colour_by ,  shape_by ,  size_by ,  theme_size ,  legend ) } ",
    "filename": "plotting.txt"
  }
}

1.
{
  "old_function": {
    "name": "plotExpressionSCESet",
    "representation": "plotExpressionSCESet",
    "parameters": "function ( object , features , x , exprs_values = \"exprs\" , colour_by = NULL , shape_by = NULL , size_by = NULL , ncol = 2 , xlab = NULL , show_median = FALSE , show_violin = TRUE , show_smooth = FALSE , theme_size = 10 , log2_values = FALSE )",
    "body": "{ ## Check object is an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet\" ) ## Define number of features to plot  if (   is.logical (  features ) )   nfeatures -   sum (  features ) else   nfeatures -   length (  features ) ## Check arguments are valid  if (   !  (   x %in%   varLabels (  object ) ) undefined  !  (   x %in%   featureNames (  object ) ) )   stop (  \"the argument 'x' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)\" )  if (   x %in%   featureNames (  object ) )  {   x_is_feature -  TRUE   show_violin -  FALSE   show_median -  FALSE } else   x_is_feature -  FALSE  if (  !   is.null (  colour_by ) )  {  if (  !  (   colour_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (  !   is.null (  shape_by ) )  {  if (  !  (   shape_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'shape_by' should specify a column of pData(object) [see varLabels(object)]\" )  if (    nlevels (   as.factor (    pData (  object ) [[  shape_by ] ] ) ) undefined  10 )   stop (  \"when coerced to a factor, 'shape_by' should have fewer than 10 levels\" ) }  if (  !   is.null (  size_by ) )  {  if (  !  (   size_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'size_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (    typeof (  features ) ==  \"character\" )  {  if (  !  (   all (   features %in%   featureNames (  object ) ) ) )   stop (  \"when the argument 'features' is of type character, all features must be in featureNames(object)\" ) }   exprs_mat -   get_exprs (  object ,  exprs_values )  if (  log2_values )  {   exprs_mat -   log2 (   exprs_mat +  1 )   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \"; log2-scale)\" ) } else   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \")\" )   to_melt -   as.matrix (   exprs_mat [  features , , drop =  FALSE ] ) ## Melt the expression data and metadata into a convenient form   evals_long -   reshape2 :: melt (  to_melt , value.name =  \"evals\" )    colnames (  evals_long ) -   c (  \"Feature\" ,  \"Cell\" ,  \"evals\" ) ## Extend the samples information   samps -   pData (  object )  if (  x_is_feature )    samps [[  x ] ] -   exprs_mat [  x , ]   samples_long -   samps [   rep (   seq_len (   ncol (  object ) ) , each =  nfeatures ) , ] ## Construct a ggplot2 aesthetic for the plot   aesth -   aes ( )    aesth $ x -   as.symbol (  x )    aesth $ y -   as.symbol (  \"evals\" )  if (  !   is.null (  colour_by ) )    aesth $ colour -   as.symbol (  colour_by ) else  {  if (  !   is.null (   is_exprs (  object ) ) )  { ## Colour by is_exprs if we can (i.e. is_exprs is not NULL)   isexpr_long -   reshape2 :: melt (    is_exprs (  object ) [  features , ] , value.name =  \"is_exprs\" )   evals_long -   dplyr :: mutate (  evals_long , Is_Expressed =   as.vector (   isexpr_long $ is_exprs ) )    aesth $ colour -   as.symbol (  \"Is_Expressed\" ) } }  if (  !   is.null (  shape_by ) )    aesth $ shape -   as.symbol (  shape_by ) ## Define sensible x-axis label if NULL  if (   is.null (  xlab ) )   xlab -  x ## Combine the expression values and sample information   object -   cbind (  evals_long ,  samples_long ) ## Make the plot   plot_out -   plotExpressionDefault (  object ,  aesth ,  ncol ,  xlab ,  ylab ,  show_median ,  show_violin ,  show_smooth ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size )  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotExpressionSCESet",
    "representation": "plotExpressionSCESet",
    "parameters": "function ( object , features , x = NULL , exprs_values = \"exprs\" , colour_by = NULL , shape_by = NULL , size_by = NULL , ncol = 2 , xlab = NULL , show_median = FALSE , show_violin = TRUE , show_smooth = FALSE , alpha = 0.6 , theme_size = 10 , log2_values = FALSE , size = NULL , scales = \"fixed\" , se = TRUE , jitter = \"swarm\" )",
    "body": "{ ## Check object is an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet\" ) ## Define number of features to plot  if (   is.logical (  features ) )   nfeatures -   sum (  features ) else   nfeatures -   length (  features ) ## Check arguments are valid  if (   is.null (  x ) )  {   x_is_feature -  FALSE } else  {  if (   !  (   x %in%   varLabels (  object ) ) undefined  !  (   x %in%   featureNames (  object ) ) )   stop (  \"the argument 'x' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)\" )  if (   x %in%   featureNames (  object ) )  {   x_is_feature -  TRUE   show_violin -  FALSE   show_median -  FALSE } else   x_is_feature -  FALSE }  if (  !   is.null (  colour_by ) )  {  if (  !  (   colour_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (  !   is.null (  shape_by ) )  {  if (  !  (   shape_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'shape_by' should specify a column of pData(object) [see varLabels(object)]\" )  if (    nlevels (   as.factor (    pData (  object ) [[  shape_by ] ] ) ) undefined  10 )   stop (  \"when coerced to a factor, 'shape_by' should have fewer than 10 levels\" ) }  if (  !   is.null (  size_by ) )  {  if (  !  (   size_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'size_by' should specify a column of pData(object) [see varLabels(object)]\" ) }  if (    typeof (  features ) ==  \"character\" )  {  if (  !  (   all (   features %in%   featureNames (  object ) ) ) )   stop (  \"when the argument 'features' is of type character, all features must be in featureNames(object)\" ) }   exprs_mat -   get_exprs (  object ,  exprs_values )  if (  log2_values )  {   exprs_mat -   log2 (   exprs_mat +  1 )   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \"; log2-scale)\" ) } else   ylab -   paste0 (  \"Expression (\" ,  exprs_values ,  \")\" )   to_melt -   as.matrix (   exprs_mat [  features , , drop =  FALSE ] ) ## Melt the expression data and metadata into a convenient form   evals_long -   reshape2 :: melt (  to_melt , value.name =  \"evals\" )    colnames (  evals_long ) -   c (  \"Feature\" ,  \"Cell\" ,  \"evals\" ) ## Extend the samples information   samps -   pData (  object )  if (  x_is_feature )    samps [[  x ] ] -   exprs_mat [  x , ]   samples_long -   samps [   rep (   seq_len (   ncol (  object ) ) , each =  nfeatures ) , ] ## Construct a ggplot2 aesthetic for the plot   aesth -   aes ( )  if (   is.null (  x ) )  {    aesth $ x -   as.symbol (  \"Feature\" )   one_facet -  TRUE } else  {    aesth $ x -   as.symbol (  x )   one_facet -  FALSE }    aesth $ y -   as.symbol (  \"evals\" )  if (  !   is.null (  colour_by ) )    aesth $ colour -   as.symbol (  colour_by ) else  {  if (  !   is.null (   is_exprs (  object ) ) )  { ## Colour by is_exprs if we can (i.e. is_exprs is not NULL)   isexpr_long -   reshape2 :: melt (    is_exprs (  object ) [  features , ] , value.name =  \"is_exprs\" )   evals_long -   dplyr :: mutate (  evals_long , Is_Expressed =   as.vector (   isexpr_long $ is_exprs ) )    aesth $ colour -   as.symbol (  \"Is_Expressed\" ) } }  if (  !   is.null (  shape_by ) )    aesth $ shape -   as.symbol (  shape_by ) ## Define sensible x-axis label if NULL  if (   is.null (  xlab ) )   xlab -  x ## Combine the expression values and sample information   object -   cbind (  evals_long ,  samples_long ) ## Make the plot   plot_out -   plotExpressionDefault (  object ,  aesth ,  ncol ,  xlab ,  ylab ,  show_median ,  show_violin ,  show_smooth ,  alpha ,  size ,  scales ,  one_facet ,  se ,  jitter ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size )  if (   is.null (  x ) )  { ## in this case, do not show x-axis ticks or labels   plot_out -   plot_out +   theme ( axis.text.x =   element_text ( angle =  60 , vjust =  1 , hjust =  1 ) , axis.ticks.x =   element_blank ( ) , plot.margin =   unit (   c (  .03 ,  .02 ,  .05 ,  .02 ) ,  \"npc\" ) )  if (   is.null (  colour_by ) )   plot_out -   plot_out +   guides ( fill =  \"none\" , colour =  \"none\" ) }  plot_out } ",
    "filename": "plotting.txt"
  }
}

2.
{
  "old_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , feature_controls = NULL , technical_feature_controls = NULL , biological_feature_controls = NULL , cell_controls = NULL , nmads = 5 , pct_feature_controls_threshold = 80 )",
    "body": "{ ## We must have an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet object.\" ) ## the object must have some samples  if (    ncol (  object ) undefined  1 )   stop (  \"object must have at least one sample (column)\" )  if (    nrow (  object ) undefined  1 )   stop (  \"object must have at least one feature (row)\" ) ## Compute cell-level metrics  if (   is.null (   is_exprs (  object ) ) )  {  if (   is.null (   counts (  object ) ) )  {   stop (  \"Please define is_exprs(object).\r\n                 E.g. use is_exprs(object) ) } else  {   warning (  \"is_exprs(object) is null.\r\nDefining 'is_exprs' using count data and\r\na lower count threshold of 0.\" )   isexprs -   calcIsExprs (  object , lowerDetectionLimit =  0 )    rownames (  isexprs ) -   rownames (   counts (  object ) )    colnames (  isexprs ) -   colnames (   counts (  object ) )    is_exprs (  object ) -  isexprs } } ## See what versions of the expression data are available in the object   exprs_mat -   exprs (  object )  if (   object @ logged )   exprs_mat -    2 ^  exprs_mat -   object @ logExprsOffset   counts_mat -   counts (  object )   tpm_mat -   tpm (  object )   fpkm_mat -   fpkm (  object )  if (  !   is.null (  technical_feature_controls ) )  {  if (  !   is.null (  feature_controls ) )   warning (  \"Both technical_feature_controls and feature_controls arguments provided, so ignoring feature_controls\" )   feature_controls -  technical_feature_controls } ## get number of sets of technical feature controls  if (   is.list (  feature_controls ) )   n_sets_feature_controls -   length (  feature_controls ) else   n_sets_feature_controls -  1 ## Contributions from technical control features   tech_features -   .process_feature_controls (  object ,  feature_controls ,  pct_feature_controls_threshold ,  exprs_mat ,  counts_mat ,  tpm_mat ,  fpkm_mat )   feature_controls_pdata -   tech_features $ pData   feature_controls_fdata -   tech_features $ fData ## Fix column names and define feature controls across all control sets  if (   n_sets_feature_controls ==  1 )  {    colnames (  feature_controls_pdata ) -   gsub (  \"_$\" ,  \"\" ,   colnames (  feature_controls_pdata ) )    colnames (  feature_controls_fdata ) -   gsub (  \"_$\" ,  \"\" ,   colnames (  feature_controls_fdata ) )   is_feature_control -   apply (  feature_controls_fdata ,  1 ,  any ) } else  { ## Combine all feature controls   is_feature_control -   apply (  feature_controls_fdata ,  1 ,  any )   feature_controls_fdata -   cbind (  feature_controls_fdata ,  is_feature_control ) ## Compute metrics using all feature controls   df_pdata_this -   .get_qc_metrics_exprs_mat (  exprs_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"exprs\" )  if (  !   is.null (  counts_mat ) )  {   df_pdata_counts -   .get_qc_metrics_exprs_mat (  counts_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"counts\" )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_counts ) }  if (  !   is.null (  tpm_mat ) )  {   df_pdata_tpm -   .get_qc_metrics_exprs_mat (  tpm_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"tpm\" )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_tpm ) }  if (  !   is.null (  fpkm_mat ) )  {   df_pdata_fpkm -   .get_qc_metrics_exprs_mat (  fpkm_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"fpkm\" )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_fpkm ) }   n_detected_feature_controls -   colSums (    is_exprs (  object ) [  is_feature_control , ] )    df_pdata_this $ n_detected_feature_controls -  n_detected_feature_controls   feature_controls_pdata -   cbind (  feature_controls_pdata ,  df_pdata_this ) } ## get metrics for biological feature controls if present  if (  !   is.null (  biological_feature_controls ) )  {  if (   is.list (  biological_feature_controls ) )   n_sets_biological_feature_controls -   length (  biological_feature_controls ) else   n_sets_biological_feature_controls -  1 ## compute metrics   biol_features -   .process_feature_controls (  object ,  biological_feature_controls ,  pct_feature_controls_threshold ,  exprs_mat ,  counts_mat ,  tpm_mat ,  fpkm_mat , biological =  TRUE )   biol_feature_controls_pdata -   biol_features $ pData   biol_feature_controls_fdata -   biol_features $ fData   feature_controls_pdata -   cbind (  feature_controls_pdata ,  biol_feature_controls_pdata )   feature_controls_fdata -   cbind (  feature_controls_fdata ,  biol_feature_controls_fdata ) } ## Compute total_features and find outliers   total_features -   colSums (    is_exprs (  object ) [  !  is_feature_control , ] )   filter_on_total_features -   isOutlier (  total_features ,  nmads , type =  \"lower\" ) ## Compute total_counts if counts are present  if (  !   is.null (  counts_mat ) )   total_counts -   colSums (  counts_mat ) else   total_counts -   colSums (  exprs_mat )   filter_on_total_counts -   isOutlier (  total_counts ,  nmads , log =  TRUE ) ## Define counts from endogenous features   qc_pdata -  feature_controls_pdata    qc_pdata $ exprs_endogenous_features -    colSums (  exprs_mat ) -   feature_controls_pdata $ exprs_feature_controls  if (  !   is.null (  counts_mat ) )  {    qc_pdata $ counts_endogenous_features -   total_counts -   feature_controls_pdata $ counts_feature_controls }  if (  !   is.null (  tpm_mat ) )  {    qc_pdata $ tpm_endogenous_features -    colSums (  tpm_mat ) -   feature_controls_pdata $ tpm_feature_controls }  if (  !   is.null (  fpkm_mat ) )  {    qc_pdata $ fpkm_endogenous_features -    colSums (  fpkm_mat ) -   feature_controls_pdata $ fpkm_feature_controls } ## Define log10 read counts from feature controls   cols_to_log -   grep (  \"^counts_|^exprs_|^tpm_|^fpkm_\" ,   colnames (  qc_pdata ) )   log10_cols -   log10 (    qc_pdata [ ,  cols_to_log , drop =  FALSE ] +  1 )    colnames (  log10_cols ) -   paste0 (  \"log10_\" ,    colnames (  qc_pdata ) [  cols_to_log ] ) ## Combine into a big pdata object   qc_pdata -   cbind (  qc_pdata ,  log10_cols ) ## Define cell controls ### Determine if vector or list  if (    is.null (  cell_controls ) |    length (  cell_controls ) ==  0 )  {   is_cell_control -   rep (  FALSE ,   ncol (  object ) )   cell_controls_pdata -   data.frame (  is_cell_control )   n_sets_cell_controls -  1 } else  {  if (   is.list (  cell_controls ) )  {   cell_controls_list -  cell_controls   n_sets_cell_controls -   length (  cell_controls ) } else  {   cell_controls_list -   list (  cell_controls )   n_sets_cell_controls -  1 }  for  ( i in   seq_len (  n_sets_cell_controls ) )  {   cc_set -   cell_controls_list [[  i ] ]   set_name -    names (  cell_controls_list ) [  i ]  if (   is.logical (  cc_set ) )  {   is_cell_control -  cc_set   cc_set -   which (  cc_set ) } else  {   is_cell_control -   rep (  FALSE ,   ncol (  object ) ) }  if (   is.character (  cc_set ) )   cc_set -   which (    cellNames (  object ) %in%  cc_set )    is_cell_control [  cc_set ] -  TRUE ## Construct data.frame for pData from this feature control set   is_cell_control -   as.data.frame (  is_cell_control )    colnames (  is_cell_control ) -   paste0 (  \"is_cell_control_\" ,  set_name )  if (   exists (  \"cell_controls_pdata\" ) )  {   cell_controls_pdata -   data.frame (  cell_controls_pdata ,  is_cell_control ) } else   cell_controls_pdata -  is_cell_control } } ## Check column names and get cell controls across all sets  if (   n_sets_cell_controls ==  1 )  {    colnames (  cell_controls_pdata ) -  \"is_cell_control\" } else  { ## Combine all cell controls   is_cell_control -   apply (  cell_controls_pdata ,  1 ,  any )   cell_controls_pdata -   cbind (  cell_controls_pdata ,  is_cell_control ) } ## Add cell-level QC metrics to pData   new_pdata -   as.data.frame (   pData (  object ) ) ### Remove columns to be replaced   to_replace -    colnames (  new_pdata ) %in%   c (   colnames (  qc_pdata ) ,   colnames (  cell_controls_pdata ) )   new_pdata -   new_pdata [ ,  !  to_replace , drop =  FALSE ] ### Add new QC metrics    new_pdata $ total_counts -  total_counts    new_pdata $ log10_total_counts -   log10 (  total_counts )    new_pdata $ filter_on_total_counts -  filter_on_total_counts    new_pdata $ total_features -  total_features    new_pdata $ log10_total_features -   log10 (  total_features )    new_pdata $ filter_on_total_features -  filter_on_total_features    new_pdata $ pct_dropout -    100 *   colSums (  !   is_exprs (  object ) ) /   nrow (  object )   new_pdata -   cbind (  new_pdata ,  qc_pdata ,  cell_controls_pdata )    pData (  object ) -   new (  \"AnnotatedDataFrame\" ,  new_pdata ) ## indicate if feature is feature control across any set ## here use technical feature controls  if (   is.list (  feature_controls ) )  {   feat_controls_cols -   grep (  \"^is_feature_control\" ,   colnames (  feature_controls_fdata ) )    feature_controls_fdata $ is_feature_control -  (    rowSums (   feature_controls_fdata [ ,  feat_controls_cols , drop =  FALSE ] ) undefined  0 ) } ## Add feature-level QC metrics to fData   new_fdata -   as.data.frame (   fData (  object ) ) ### Remove columns that are to be replaced   to_replace -    colnames (  new_fdata ) %in%   colnames (  feature_controls_fdata )   new_fdata -   new_fdata [ ,  !  to_replace , drop =  FALSE ] ### Add new QC information    new_fdata $ mean_exprs -   rowMeans (   exprs (  object ) )    new_fdata $ exprs_rank -   rank (   rowMeans (   exprs (  object ) ) )    new_fdata $ n_cells_exprs -   rowSums (   is_exprs (  object ) )   total_exprs -   sum (  exprs_mat )    new_fdata $ total_feature_exprs -   rowSums (  exprs_mat )    new_fdata $ log10_total_feature_exprs -   log10 (    new_fdata $ total_feature_exprs +  1 )    new_fdata $ pct_total_exprs -    100 *   rowSums (  exprs_mat ) /  total_exprs    new_fdata $ pct_dropout -    100 *   rowSums (  !   is_exprs (  object ) ) /   ncol (  object )  if (  !   is.null (  counts_mat ) )  {   total_counts -   sum (  counts_mat )    new_fdata $ total_feature_counts -   rowSums (  counts_mat )    new_fdata $ log10_total_feature_counts -   log10 (    new_fdata $ total_feature_counts +  1 )    new_fdata $ pct_total_counts -    100 *   rowSums (  counts_mat ) /  total_counts }  if (  !   is.null (  tpm_mat ) )  {   total_tpm -   sum (  tpm_mat )    new_fdata $ total_feature_tpm -   rowSums (  tpm_mat )    new_fdata $ log10_total_feature_tpm -   log10 (    new_fdata $ total_feature_tpm +  1 )    new_fdata $ pct_total_tpm -    100 *   rowSums (  tpm_mat ) /  total_tpm }  if (  !   is.null (  fpkm_mat ) )  {   total_fpkm -   sum (  fpkm_mat )    new_fdata $ total_feature_fpkm -   rowSums (  fpkm_mat )    new_fdata $ log10_total_feature_fpkm -   log10 (    new_fdata $ total_feature_fpkm +  1 )    new_fdata $ pct_total_fpkm -    100 *   rowSums (  fpkm_mat ) /  total_fpkm } ## Add new fdata to object   new_fdata -   cbind (  new_fdata ,  feature_controls_fdata )    fData (  object ) -   new (  \"AnnotatedDataFrame\" ,  new_fdata ) ## Ensure sample names are correct and return object    sampleNames (  object ) -   colnames (   exprs (  object ) )  object } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , feature_controls = NULL , cell_controls = NULL , nmads = 5 , pct_feature_controls_threshold = 80 )",
    "body": "{ ## We must have an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet object.\" ) ## the object must have some samples  if (    ncol (  object ) undefined  1 )   stop (  \"object must have at least one sample (column)\" )  if (    nrow (  object ) undefined  1 )   stop (  \"object must have at least one feature (row)\" ) ## Compute cell-level metrics  if (   is.null (   is_exprs (  object ) ) )  {  if (   is.null (   counts (  object ) ) )  {   stop (  \"need either is_exprs(object) or counts(object) to be defined,\r\n  e.g., use `is_exprs(object) ) } } ## See what versions of the expression data are available in the object   exprs_mat -   exprs (  object )   counts_mat -   counts (  object )   tpm_mat -   tpm (  object )   fpkm_mat -   fpkm (  object ) ## get number of sets of feature controls, and name them  if (   is.null (  feature_controls ) )  {   feature_controls -   list ( ) } else  if (  !   is.list (  feature_controls ) )  {   feature_controls -   list (  feature_controls ) }   n_sets_feature_controls -   length (  feature_controls )   counter -  1L  for  ( i in   seq_len (  n_sets_feature_controls ) )  {   curname -    names (  feature_controls ) [  i ]  if (    is.null (  curname ) ||   curname ==  \"\" )  {     names (  feature_controls ) [  i ] -   paste0 (  \"unnamed\" ,  counter )   counter -   counter +  1L } }    object @ featureControlInfo -   AnnotatedDataFrame (   data.frame ( name =   names (  feature_controls ) , stringsAsFactors =  FALSE ) )  if (  n_sets_feature_controls )  { ## Contributions from technical control features   tech_features -   .process_feature_controls (  object ,  feature_controls ,  pct_feature_controls_threshold ,  exprs_mat ,  counts_mat ,  tpm_mat ,  fpkm_mat )   feature_controls_pdata -   tech_features $ pData   feature_controls_fdata -   tech_features $ fData ## Combine all feature controls   is_feature_control -   apply (  feature_controls_fdata ,  1 ,  any )   feature_controls_fdata -   cbind (  feature_controls_fdata ,  is_feature_control ) } else  {   is_feature_control -   logical (   nrow (  object ) )   feature_controls_fdata -   data.frame (  is_feature_control )   feature_controls_pdata -   data.frame (   matrix (  0 , nrow =   ncol (  object ) , ncol =  0 ) ) } ## Compute metrics using all feature controls   df_pdata_this -   .get_qc_metrics_exprs_mat (  exprs_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"exprs\" , compute_endog =  FALSE )  if (  !   is.null (  counts_mat ) )  {   df_pdata_counts -   .get_qc_metrics_exprs_mat (  counts_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"counts\" , compute_endog =  TRUE )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_counts ) }  if (  !   is.null (  tpm_mat ) )  {   df_pdata_tpm -   .get_qc_metrics_exprs_mat (  tpm_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"tpm\" , compute_endog =  TRUE )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_tpm ) }  if (  !   is.null (  fpkm_mat ) )  {   df_pdata_fpkm -   .get_qc_metrics_exprs_mat (  fpkm_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  \"fpkm\" , compute_endog =  TRUE )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_fpkm ) }   n_detected_feature_controls -   nexprs (  object , subset.row =  is_feature_control )    df_pdata_this $ n_detected_feature_controls -  n_detected_feature_controls   feature_controls_pdata -   cbind (  feature_controls_pdata ,  df_pdata_this ) ## Compute total_features and find outliers   total_features -   nexprs (  object , subset.row =  !  is_feature_control )   filter_on_total_features -   isOutlier (  total_features ,  nmads , type =  \"lower\" ) ## Compute total_counts if counts are present  if (  !   is.null (  counts_mat ) )   total_counts -   colSums (  counts_mat ) else   total_counts -   colSums (  exprs_mat )   filter_on_total_counts -   isOutlier (  total_counts ,  nmads , log =  TRUE ) ## Define counts from endogenous features   qc_pdata -  feature_controls_pdata    qc_pdata $ exprs_endogenous_features -    colSums (  exprs_mat ) -   feature_controls_pdata $ exprs_feature_controls  if (  !   is.null (  counts_mat ) )  {    qc_pdata $ counts_endogenous_features -   total_counts -   feature_controls_pdata $ counts_feature_controls }  if (  !   is.null (  tpm_mat ) )  {    qc_pdata $ tpm_endogenous_features -    colSums (  tpm_mat ) -   feature_controls_pdata $ tpm_feature_controls }  if (  !   is.null (  fpkm_mat ) )  {    qc_pdata $ fpkm_endogenous_features -    colSums (  fpkm_mat ) -   feature_controls_pdata $ fpkm_feature_controls } ## Define log10 read counts from feature controls   cols_to_log -   grep (  \"^counts_|^tpm_|^fpkm_\" ,   colnames (  qc_pdata ) )  if (  !   object @ logged )  {   cols_to_log -   c (  cols_to_log ,   grep (  \"^exprs_\" ,   colnames (  qc_pdata ) ) ) }   log10_cols -   log10 (    qc_pdata [ ,  cols_to_log , drop =  FALSE ] +  1 )    colnames (  log10_cols ) -   paste0 (  \"log10_\" ,    colnames (  qc_pdata ) [  cols_to_log ] ) ## Combine into a big pdata object   qc_pdata -   cbind (  qc_pdata ,  log10_cols ) ## Define cell controls ### Determine if vector or list  if (    is.null (  cell_controls ) |    length (  cell_controls ) ==  0 )  {   is_cell_control -   rep (  FALSE ,   ncol (  object ) )   cell_controls_pdata -   data.frame (  is_cell_control )   n_sets_cell_controls -  1 } else  {  if (   is.list (  cell_controls ) )  {   cell_controls_list -  cell_controls   n_sets_cell_controls -   length (  cell_controls ) } else  {   cell_controls_list -   list (  cell_controls )   n_sets_cell_controls -  1 }  for  ( i in   seq_len (  n_sets_cell_controls ) )  {   cc_set -   cell_controls_list [[  i ] ]   set_name -    names (  cell_controls_list ) [  i ]  if (   is.logical (  cc_set ) )  {   is_cell_control -  cc_set   cc_set -   which (  cc_set ) } else  {   is_cell_control -   rep (  FALSE ,   ncol (  object ) ) }  if (   is.character (  cc_set ) )   cc_set -   which (    cellNames (  object ) %in%  cc_set )    is_cell_control [  cc_set ] -  TRUE ## Construct data.frame for pData from this feature control set   is_cell_control -   as.data.frame (  is_cell_control )    colnames (  is_cell_control ) -   paste0 (  \"is_cell_control_\" ,  set_name )  if (   i undefined  1L )  {   cell_controls_pdata -   data.frame (  cell_controls_pdata ,  is_cell_control ) } else   cell_controls_pdata -  is_cell_control } } ## Check column names and get cell controls across all sets  if (   n_sets_cell_controls ==  1 )  {    colnames (  cell_controls_pdata ) -  \"is_cell_control\" } else  { ## Combine all cell controls   is_cell_control -   apply (  cell_controls_pdata ,  1 ,  any )   cell_controls_pdata -   cbind (  cell_controls_pdata ,  is_cell_control ) } ## Add cell-level QC metrics to pData   new_pdata -   as.data.frame (   pData (  object ) ) ### Remove columns to be replaced   to_replace -    colnames (  new_pdata ) %in%   c (   colnames (  qc_pdata ) ,   colnames (  cell_controls_pdata ) )   new_pdata -   new_pdata [ ,  !  to_replace , drop =  FALSE ] ### Add new QC metrics    new_pdata $ total_counts -  total_counts    new_pdata $ log10_total_counts -   log10 (  total_counts )    new_pdata $ filter_on_total_counts -  filter_on_total_counts    new_pdata $ total_features -  total_features    new_pdata $ log10_total_features -   log10 (  total_features )    new_pdata $ filter_on_total_features -  filter_on_total_features    new_pdata $ pct_dropout -   100 *  (   1 -    nexprs (  object , subset.row =  NULL ) /   nrow (  object ) )   new_pdata -   cbind (  new_pdata ,  qc_pdata ,  cell_controls_pdata )    pData (  object ) -   new (  \"AnnotatedDataFrame\" ,  new_pdata ) ## indicate if feature is feature control across any set ## here use technical feature controls  if (   is.list (  feature_controls ) )  {   feat_controls_cols -   grep (  \"^is_feature_control\" ,   colnames (  feature_controls_fdata ) )    feature_controls_fdata $ is_feature_control -  (    rowSums (   feature_controls_fdata [ ,  feat_controls_cols , drop =  FALSE ] ) undefined  0 ) } ## Add feature-level QC metrics to fData   new_fdata -   as.data.frame (   fData (  object ) ) ### Remove columns that are to be replaced   to_replace -    colnames (  new_fdata ) %in%   colnames (  feature_controls_fdata )   new_fdata -   new_fdata [ ,  !  to_replace , drop =  FALSE ] ### Add new QC information    new_fdata $ mean_exprs -   rowMeans (   exprs (  object ) )    new_fdata $ exprs_rank -   rank (   rowMeans (   exprs (  object ) ) )    new_fdata $ n_cells_exprs -   nexprs (  object , byrow =  TRUE )   total_exprs -   sum (  exprs_mat )    new_fdata $ total_feature_exprs -   rowSums (  exprs_mat )  if (  !   object @ logged )  {    new_fdata $ log10_total_feature_exprs -   log10 (    new_fdata $ total_feature_exprs +  1 ) }    new_fdata $ pct_total_exprs -    100 *   rowSums (  exprs_mat ) /  total_exprs    new_fdata $ pct_dropout -   100 *  (   1 -    new_fdata $ n_cells_exprs /   ncol (  object ) )  if (  !   is.null (  counts_mat ) )  {   total_counts -   sum (   as.double (   colSums (  counts_mat ) ) ) # avoid integer overflow    new_fdata $ total_feature_counts -   rowSums (  counts_mat )    new_fdata $ log10_total_feature_counts -   log10 (    new_fdata $ total_feature_counts +  1 )    new_fdata $ pct_total_counts -    100 *   rowSums (  counts_mat ) /  total_counts }  if (  !   is.null (  tpm_mat ) )  {   total_tpm -   sum (  tpm_mat )    new_fdata $ total_feature_tpm -   rowSums (  tpm_mat )    new_fdata $ log10_total_feature_tpm -   log10 (    new_fdata $ total_feature_tpm +  1 )    new_fdata $ pct_total_tpm -    100 *   rowSums (  tpm_mat ) /  total_tpm }  if (  !   is.null (  fpkm_mat ) )  {   total_fpkm -   sum (  fpkm_mat )    new_fdata $ total_feature_fpkm -   rowSums (  fpkm_mat )    new_fdata $ log10_total_feature_fpkm -   log10 (    new_fdata $ total_feature_fpkm +  1 )    new_fdata $ pct_total_fpkm -    100 *   rowSums (  fpkm_mat ) /  total_fpkm } ## Add new fdata to object   new_fdata -   cbind (  new_fdata ,  feature_controls_fdata )    fData (  object ) -   new (  \"AnnotatedDataFrame\" ,  new_fdata ) ## Ensure sample names are correct and return object    sampleNames (  object ) -   colnames (   exprs (  object ) )  object } ",
    "filename": "qc.txt"
  }
}

3.
{
  "old_function": {
    "name": "newSCESet",
    "representation": "newSCESet",
    "parameters": "function ( exprsData = NULL , countData = NULL , tpmData = NULL , fpkmData = NULL , cpmData = NULL , phenoData = NULL , featureData = NULL , experimentData = NULL , is_exprsData = NULL , lowerDetectionLimit = 0 , logExprsOffset = 1 , logged = FALSE , useForExprs = \"exprs\" )",
    "body": "{ ## Check that we have some expression data  if (       is.null (  exprsData ) undefined   is.null (  countData ) undefined   is.null (  tpmData ) undefined   is.null (  fpkmData ) undefined   is.null (  cpmData ) )   stop (  \"Require at least one of exprsData, tpmData, fpkmData or countData arguments.\" ) ## Check dimensions of data matrices ## Check counts are a matrix; renames is_exprsData if not null  if (  !   is.null (  countData ) )   countData -   as.matrix (  countData )  if (  !   is.null (  is_exprsData ) )   isexprs -  is_exprsData ## If no exprsData provided define is_exprs from tpmData, fpkmData or countData  if (   is.null (  exprsData ) )  { ## Define exprs data if null  if (  !   is.null (  tpmData ) )  {   exprsData -   log2 (   tpmData +  logExprsOffset )   logged -  TRUE } else  {  if (  !   is.null (  fpkmData ) )  {   exprsData -   log2 (   fpkmData +  logExprsOffset )   logged -  TRUE } else  {  if (  !   is.null (  cpmData ) )  {   exprsData -   log2 (   cpmData +  logExprsOffset )   logged -  TRUE } else  {   cpmData -   edgeR :: cpm.default (  countData , prior.count =  logExprsOffset , log =  FALSE )   exprsData -   log2 (   cpmData +  logExprsOffset )   logged -  TRUE } } } ## Define isexprs if null  if (   is.null (  is_exprsData ) )  {  if (  !   is.null (  tpmData ) )  {   isexprs -   tpmData undefined  lowerDetectionLimit    rownames (  isexprs ) -   rownames (  tpmData )    colnames (  isexprs ) -   colnames (  tpmData ) # message(paste0(\"Defining 'is_exprs' using TPM data and a lower TPM threshold of \", lowerDetectionLimit)) } else  {  if (  !   is.null (  fpkmData ) )  {   isexprs -   fpkmData undefined  lowerDetectionLimit    rownames (  isexprs ) -   rownames (  fpkmData )    colnames (  isexprs ) -   colnames (  fpkmData ) # message(paste0(\"Defining 'is_exprs' using FPKM data and a lower FPKM threshold of \", lowerDetectionLimit)) } else  {   isexprs -   countData undefined  lowerDetectionLimit    rownames (  isexprs ) -   rownames (  countData )    colnames (  isexprs ) -   colnames (  countData ) # message(paste0(\"Defining 'is_exprs' using count data and a lower count threshold of \", lowerDetectionLimit)) } } } } else  {   exprsData -   as.matrix (  exprsData )  if (   is.null (  is_exprsData ) )  {   isexprs -   exprsData undefined  lowerDetectionLimit    rownames (  isexprs ) -   rownames (  exprsData )    colnames (  isexprs ) -   colnames (  exprsData ) # message(paste0(\"Defining 'is_exprs' using exprsData and a lower exprs threshold of \", lowerDetectionLimit)) } } ## Generate valid phenoData and featureData if not provided  if (   is.null (  phenoData ) )   phenoData -   annotatedDataFrameFrom (  exprsData , byrow =  FALSE )  if (   is.null (  featureData ) )   featureData -   annotatedDataFrameFrom (  exprsData , byrow =  TRUE ) ## Check experimentData   expData_null -   new (  \"MIAME\" , name =  \" , lab =  \" , contact =  \" , title =  \" , abstract =  \"An SCESet\" , url =  \" , other =   list ( notes =  \"This dataset created from ...\" , coauthors =   c (  \"\" ) ) )  if (  !   is.null (  experimentData ) )  {  if (   is (  experimentData ,  \"MIAME\" ) )   expData -  experimentData else  {   expData -  expData_null   warning (  \"experimentData supplied is not an 'MIAME' object. Thus, experimentData is being set to an empty MIAME object.\\n Please supply a valid 'MIAME' class object containing experiment data to experimentData(object).\" ) } } else  {   expData -  expData_null } ## Check valid useForExprs   useForExprs -   match.arg (  useForExprs ,   c (  \"exprs\" ,  \"tpm\" ,  \"counts\" ,  \"fpkm\" ) ) ## Generate new SCESet object   assaydata -   assayDataNew (  \"environment\" , exprs =  exprsData , is_exprs =  isexprs )   sceset -   new (  \"SCESet\" , assayData =  assaydata , phenoData =  phenoData , featureData =  featureData , experimentData =  expData , lowerDetectionLimit =  lowerDetectionLimit , logExprsOffset =  logExprsOffset , logged =  logged , useForExprs =  useForExprs ) ## Add non-null slots to assayData for SCESet object, omitting null slots  if (  !   is.null (  tpmData ) )    tpm (  sceset ) -  tpmData  if (  !   is.null (  fpkmData ) )    fpkm (  sceset ) -  fpkmData  if (  !   is.null (  countData ) )    counts (  sceset ) -  countData  if (  !   is.null (  cpmData ) )    cpm (  sceset ) -  cpmData ## Check validity of object   validObject (  sceset )  sceset } ",
    "filename": "SCESet-methods.txt"
  },
  "new_function": {
    "name": "newSCESet",
    "representation": "newSCESet",
    "parameters": "function ( exprsData = NULL , countData = NULL , tpmData = NULL , fpkmData = NULL , cpmData = NULL , phenoData = NULL , featureData = NULL , experimentData = NULL , is_exprsData = NULL , cellPairwiseDistances = dist ( vector ( ) ) , featurePairwiseDistances = dist ( vector ( ) ) , lowerDetectionLimit = 0 , logExprsOffset = 1 , logged = FALSE , useForExprs = \"exprs\" )",
    "body": "{ ## Check that we have some expression data  if (       is.null (  exprsData ) undefined   is.null (  countData ) undefined   is.null (  tpmData ) undefined   is.null (  fpkmData ) undefined   is.null (  cpmData ) )   stop (  \"Require at least one of exprsData, tpmData, fpkmData or countData arguments.\" ) ## Check dimensions of data matrices ## Check counts are a matrix; renames is_exprsData if not null  if (  !   is.null (  countData ) )   countData -   as.matrix (  countData )  if (  !   is.null (  is_exprsData ) )   isexprs -  is_exprsData ## If no exprsData provided define is_exprs from tpmData, fpkmData or countData  if (   is.null (  exprsData ) )  { ## Define exprs data if null  if (  !   is.null (  tpmData ) )  {   exprsData -   log2 (   tpmData +  logExprsOffset )   logged -  TRUE } else  {  if (  !   is.null (  fpkmData ) )  {   exprsData -   log2 (   fpkmData +  logExprsOffset )   logged -  TRUE } else  {  if (  !   is.null (  cpmData ) )  {   exprsData -   log2 (   cpmData +  logExprsOffset )   logged -  TRUE } else  {   exprsData -   edgeR :: cpm.default (  countData , prior.count =  logExprsOffset , log =  TRUE )   logged -  TRUE } } } } else  {   exprsData -   as.matrix (  exprsData ) } ## Generate valid phenoData and featureData if not provided  if (   is.null (  phenoData ) )   phenoData -   annotatedDataFrameFrom (  exprsData , byrow =  FALSE )  if (   is.null (  featureData ) )   featureData -   annotatedDataFrameFrom (  exprsData , byrow =  TRUE ) ## Check experimentData   expData_null -   new (  \"MIAME\" , name =  \" , lab =  \" , contact =  \" , title =  \" , abstract =  \"An SCESet\" , url =  \" , other =   list ( notes =  \"This dataset created from ...\" , coauthors =   c (  \"\" ) ) )  if (  !   is.null (  experimentData ) )  {  if (   is (  experimentData ,  \"MIAME\" ) )   expData -  experimentData else  {   expData -  expData_null   warning (  \"experimentData supplied is not an 'MIAME' object. Thus, experimentData is being set to an empty MIAME object.\\n Please supply a valid 'MIAME' class object containing experiment data to experimentData(object).\" ) } } else  {   expData -  expData_null } ## Check valid useForExprs   useForExprs -   match.arg (  useForExprs ,   c (  \"exprs\" ,  \"tpm\" ,  \"counts\" ,  \"fpkm\" ) ) ## Generate new SCESet object  if (  !   is.null (  is_exprsData ) )   assaydata -   assayDataNew (  \"lockedEnvironment\" , exprs =  exprsData , is_exprs =  isexprs ) else   assaydata -   assayDataNew (  \"lockedEnvironment\" , exprs =  exprsData )   sceset -   new (  \"SCESet\" , assayData =  assaydata , phenoData =  phenoData , featureData =  featureData , experimentData =  expData , cellPairwiseDistances =  cellPairwiseDistances , featurePairwiseDistances =  featurePairwiseDistances , lowerDetectionLimit =  lowerDetectionLimit , logExprsOffset =  logExprsOffset , logged =  logged , featureControlInfo =   AnnotatedDataFrame ( ) , useForExprs =  useForExprs ) ## Add non-null slots to assayData for SCESet object, omitting null slots  if (  !   is.null (  tpmData ) )    tpm (  sceset ) -  tpmData  if (  !   is.null (  fpkmData ) )    fpkm (  sceset ) -  fpkmData  if (  !   is.null (  countData ) )    counts (  sceset ) -  countData  if (  !   is.null (  cpmData ) )    cpm (  sceset ) -  cpmData ## Check validity of object   validObject (  sceset )  sceset } ",
    "filename": "SCESet-methods.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_4 scater_release_3_5

{
    "package": "scater",
    "release_versions": "scater_release_3_4 scater_release_3_5",
    "desc_release_old": "1.2.0",
    "desc_release_new": "1.4.0",
    "old_release_number": 1,
    "new_release_number": 2,
    "function_removals": 1,
    "function_additions": 12,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 12,
    "total_count": 13
}

##########
Functions Removed
##########

getExprs


##########
Functions Added
##########

featureControlInfo<-
setSpike<-
areSizeFactorsCentred
calcAverage
calculateCPM
featureControlInfo
featureControlInfo.SCESet
isSpike
plotRLE
read10XResults
spikes
whichSpike


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "calcIsExprs",
    "representation": "calcIsExprs",
    "parameters": "function ( object , lowerDetectionLimit = NULL , exprs_data = \"counts\" )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"Object must be an SCESet.\" ) ## Check that args are appropriate   exprs_data -   match.arg (  exprs_data ,   c (  \"counts\" ,  \"exprs\" ,  \"tpm\" ,  \"cpm\" ,  \"fpkm\" ) )   dat_matrix -   switch (  exprs_data , counts =   counts (  object ) , exprs =   exprs (  object ) , tpm =   tpm (  object ) , cpm =   cpm (  object ) , fpkm =   fpkm (  object ) )  if (   is.null (  dat_matrix ) )   stop (   paste0 (  \"Tried to use \" ,  exprs_data ,  \" as expression data, but \" ,  exprs_data ,  \"(object) is null.\" ) ) # #     if ( exprs_data == \"counts\" ) { #         dat_matrix #     } #     else { #         if ( exprs_data == \"exprs\" ) #             dat_matrix #         else # #     } ## Extract lowerDetectionLimit if not provided  if (   is.null (  lowerDetectionLimit ) )   lowerDetectionLimit -   object @ lowerDetectionLimit ## Decide which observations are above detection limit and return matrix   isexprs -   dat_matrix undefined  lowerDetectionLimit    rownames (  isexprs ) -   rownames (  dat_matrix )    colnames (  isexprs ) -   colnames (  dat_matrix )  isexprs } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calcIsExprs",
    "representation": "calcIsExprs",
    "parameters": "function ( object , lowerDetectionLimit = NULL , exprs_values = NULL )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"Object must be an SCESet.\" ) ## Check that args are appropriate   exprs_values -   .exprs_hunter (  object ,  exprs_values )   dat_matrix -   get_exprs (  object ,  exprs_values , warning =  FALSE ) ## Extract lowerDetectionLimit if not provided  if (   is.null (  lowerDetectionLimit ) )   lowerDetectionLimit -   object @ lowerDetectionLimit ## Decide which observations are above detection limit and return matrix   isexprs -   dat_matrix undefined  lowerDetectionLimit    rownames (  isexprs ) -   rownames (  dat_matrix )    colnames (  isexprs ) -   colnames (  dat_matrix )  isexprs } ",
    "filename": "calculate-expression.txt"
  }
}

1.
{
  "old_function": {
    "name": "normalize.SCESet",
    "representation": "normalize.SCESet",
    "parameters": "function ( object , exprs_values = \"counts\" , logExprsOffset = NULL , recompute_cpm = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet.\" ) ## Define expression values to be used   exprs_values -   match.arg (  exprs_values ,   c (  \"tpm\" ,  \"fpkm\" ,  \"counts\" ) )   isCount -   exprs_values ==  \"counts\"   exprs_mat -   get_exprs (  object ,  exprs_values ) ## extract existing size factors   size_factors -   sizeFactors (  object )  if (   is.null (  size_factors ) )  {   message (  \"No size factors defined in object$size_factor so returning\r\n                original object\" )   return (  object ) } ## figuring out how many controls have their own size factors   control_list -   list ( )  for  ( fc in   .get_feature_control_names (  object ) )  {   specific_sf -   suppressWarnings (   sizeFactors (  object , type =  fc ) )  if (  !   is.null (  specific_sf ) )  {   which.current -    fData (  object ) [[   paste0 (  \"is_feature_control_\" ,  fc ) ] ]    control_list [[  fc ] ] -   list ( SF =  specific_sf , ID =  which.current ) } } ## extract logExprsOffset if argument is NULL  if (   is.null (  logExprsOffset ) )   logExprsOffset -   object @ logExprsOffset ## recompute cpm if desired  if (    !   is.null (   cpm (  object ) ) undefined  recompute_cpm undefined  isCount )  {   lib_size -   colSums (  exprs_mat )   new_cpm -   .recompute_cpm_fun ( exprs_mat =  exprs_mat , size_factors =  size_factors , lib_size =  lib_size , logExprsOffset =  logExprsOffset )  for  ( alt in  control_list )  {    new_cpm [   alt $ ID , ] -   .recompute_cpm_fun ( exprs_mat =   exprs_mat [   alt $ ID , , drop =  FALSE ] , size_factors =   alt $ SF , lib_size =  lib_size , logExprsOffset =  logExprsOffset ) }    cpm (  object ) -  new_cpm } ## compute normalised expression values   norm_exprs_mat -   .recompute_expr_fun ( exprs_mat =  exprs_mat , size_factors =  size_factors , logExprsOffset =  logExprsOffset , isCount =  isCount )  for  ( alt in  control_list )  {    norm_exprs_mat [   alt $ ID , ] -   .recompute_expr_fun (   exprs_mat [   alt $ ID , , drop =  FALSE ] , size_factors =   alt $ SF , logExprsOffset =  logExprsOffset , isCount =  isCount ) } ## add normalised values to object    norm_exprs (  object ) -  norm_exprs_mat  if (  return_norm_as_exprs )    exprs (  object ) -  norm_exprs_mat ## return object   return (  object ) } ",
    "filename": "normalisation.txt"
  },
  "new_function": {
    "name": "normalize.SCESet",
    "representation": "normalize.SCESet",
    "parameters": "function ( object , exprs_values = NULL , logExprsOffset = NULL , centre_size_factors = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"'object' must be an SCESet\" ) ## Define expression values to be used   exprs_values -   .exprs_hunter (  object ,  exprs_values )  if (   exprs_values ==  \"exprs\" )  {   stop (  \"cannot compute normalized values from 'exprs'\" ) }   exprs_mat -   get_exprs (  object ,  exprs_values , warning =  FALSE )  if (   exprs_values ==  \"counts\" )  { ## extract existing size factors   size_factors -   suppressWarnings (   sizeFactors (  object ) )  if (   is.null (  size_factors ) )  {   warning (  \"skipping normalization of counts as size factors were not defined\" )   return (  object ) } ## figuring out how many controls have their own size factors   control_list -   .find_control_SF (  object )   spike.names -   .spike_fcontrol_names (  object )   no.spike.sf -  !   spike.names %in%   names (  control_list )  if (   any (  no.spike.sf ) )  {   warning (   sprintf (  \"spike-in transcripts in '%s' should have their own size factors\" ,    spike.names [  no.spike.sf ] [  1 ] ) ) } } else  {   size_factors -   rep (  1 ,   ncol (  object ) ) # ignoring size factors for non-count data.   control_list -   list ( ) } ## extract logExprsOffset if argument is NULL  if (   is.null (  logExprsOffset ) )   logExprsOffset -   object @ logExprsOffset ## compute normalised expression values   norm_exprs_mat -   .recompute_expr_fun ( exprs_mat =  exprs_mat , size_factors =  size_factors , logExprsOffset =  logExprsOffset )  for  ( alt in  control_list )  {    norm_exprs_mat [   alt $ ID , ] -   .recompute_expr_fun (  exprs_mat , size_factors =   alt $ SF , logExprsOffset =  logExprsOffset , subset_row =   alt $ ID ) } ## add normalised values to object    norm_exprs (  object ) -  norm_exprs_mat  if (  return_norm_as_exprs )    exprs (  object ) -  norm_exprs_mat ## centering all existing size factors if requested  if (    exprs_values ==  \"counts\" undefined  centre_size_factors )  {   all.sf.fields -   c (  \"size_factor\" ,   sprintf (  \"size_factor_%s\" ,   names (  control_list ) ) )  for  ( sf in  all.sf.fields )  {   cur.sf -    pData (  object ) [[  sf ] ]   cur.sf -   cur.sf /   mean (  cur.sf )     pData (  object ) [[  sf ] ] -  cur.sf } } ## return object   return (  object ) } ",
    "filename": "normalisation.txt"
  }
}

2.
{
  "old_function": {
    "name": "plotMDSSCESet",
    "representation": "plotMDSSCESet",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , return_SCESet = FALSE , draw_plot = TRUE , theme_size = 10 , legend = \"auto\" )",
    "body": "{ ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ##   cell_dist -   cellDist (  object )   ncells -   ncol (  object )  if (  !  (      dim (   cellDist (  object ) ) [  1 ] ==  ncells undefined     dim (   cellDist (  object ) ) [  2 ] ==  ncells ) )   stop (  \"cellDist(object) is not of the correct dimensions. Please define cell pairwise distances and try again:\r\n             e.g. cellDist(object) ) ## Set up indicator for whether to use pData or features ## for size_by and ## colour_by   colour_by_use_pdata -  TRUE   size_by_use_pdata -  TRUE ## Check arguments are valid  if (  !   is.null (  colour_by ) )  {  if (   !  (   colour_by %in%   varLabels (  object ) ) undefined  !  (   colour_by %in%   featureNames (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)]\" )  if (   !  (   colour_by %in%   varLabels (  object ) ) undefined  (   colour_by %in%   featureNames (  object ) ) )   colour_by_use_pdata -  FALSE } else  {  if (   \"is_cell_control\" %in%   varLabels (  object ) )   colour_by -  \"is_cell_control\" }  if (  !   is.null (  shape_by ) )  {  if (  !  (   shape_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'shape_by' should specify a column of pData(object) [see varLabels(object)]\" )  if (    nlevels (   as.factor (    pData (  object ) [[  shape_by ] ] ) ) undefined  10 )   stop (  \"when coerced to a factor, 'shape_by' should have fewer than 10 levels\" ) } else  {  if (   \"is_cell_control\" %in%   varLabels (  object ) )   shape_by -  \"is_cell_control\" }  if (  !   is.null (  size_by ) )  {  if (   !  (   size_by %in%   varLabels (  object ) ) undefined  !  (   size_by %in%   featureNames (  object ) ) )   stop (  \"the argument 'size_by' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)]\" )  if (   !  (   size_by %in%   varLabels (  object ) ) undefined  (   size_by %in%   featureNames (  object ) ) )   size_by_use_pdata -  FALSE } ## Compute multidimentional scaling   mds_out -   cmdscale (  cell_dist , k =  ncomponents ) ## Define data.frame for plotting   df_to_plot -   data.frame (   mds_out [ ,   1 :  ncomponents ] , row.names =   sampleNames (  object ) )    colnames (  df_to_plot ) -   paste0 (  \"Component_\" ,   1 :  ncomponents )  if (  !   is.null (  colour_by ) )  {  if (  colour_by_use_pdata )   colour_by_vals -    pData (  object ) [[  colour_by ] ] else   colour_by_vals -    exprs (  object ) [  colour_by , ]  if (    nlevels (   as.factor (  colour_by_vals ) ) undefined  10 )   df_to_plot -   data.frame (  df_to_plot , colour_by =  colour_by_vals ) else   df_to_plot -   data.frame (  df_to_plot , colour_by =   as.factor (  colour_by_vals ) ) }  if (  !   is.null (  shape_by ) )   df_to_plot -   data.frame (  df_to_plot , shape_by =   as.factor (    pData (  object ) [[  shape_by ] ] ) )  if (  !   is.null (  size_by ) )  {  if (  size_by_use_pdata )   size_by_vals -    pData (  object ) [[  size_by ] ] else   size_by_vals -    exprs (  object ) [  size_by , ]   df_to_plot -   data.frame (  df_to_plot , size_by =  size_by_vals ) } ## Make reduced-dimension plot   plot_out -   plotReducedDim.default (  df_to_plot ,  ncomponents ,  colour_by ,  shape_by ,  size_by , legend =  legend ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## Plot PCA and return appropriate object  if (  return_SCESet )  {   df_out -   mds_out [ ,   1 :  ncomponents ]    rownames (  df_out ) -   sampleNames (  object )    reducedDimension (  object ) -  df_out  if (  draw_plot )   print (  plot_out )   return (  object ) } else  { ## Return PCA plot   return (  plot_out ) } } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotMDSSCESet",
    "representation": "plotMDSSCESet",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , return_SCESet = FALSE , draw_plot = TRUE , exprs_values = \"exprs\" , theme_size = 10 , legend = \"auto\" )",
    "body": "{ ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ##   cell_dist -   cellDist (  object )   ncells -   ncol (  object )  if (  !  (      dim (   cellDist (  object ) ) [  1 ] ==  ncells undefined     dim (   cellDist (  object ) ) [  2 ] ==  ncells ) )   stop (  \"cellDist(object) is not of the correct dimensions. Please define cell pairwise distances and try again:\r\n             e.g. cellDist(object) ) ## Check arguments are valid   colour_by_out -   .choose_vis_values (  object ,  colour_by , cell_control_default =  TRUE , check_features =  TRUE , exprs_values =  exprs_values )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val   shape_by_out -   .choose_vis_values (  object ,  shape_by , cell_control_default =  TRUE , coerce_factor =  TRUE , level_limit =  10 )   shape_by -   shape_by_out $ name   shape_by_vals -   shape_by_out $ val   size_by_out -   .choose_vis_values (  object ,  size_by , check_features =  TRUE , exprs_values =  exprs_values )   size_by -   size_by_out $ name   size_by_vals -   size_by_out $ val ## Compute multidimentional scaling   mds_out -   cmdscale (  cell_dist , k =  ncomponents ) ## Define data.frame for plotting   df_to_plot -   data.frame (   mds_out [ ,   1 :  ncomponents ] , row.names =   sampleNames (  object ) )    colnames (  df_to_plot ) -   paste0 (  \"Component_\" ,   1 :  ncomponents )    df_to_plot $ colour_by -  colour_by_vals    df_to_plot $ shape_by -  shape_by_vals    df_to_plot $ size_by -  size_by_vals ## Make reduced-dimension plot   plot_out -   plotReducedDim.default (  df_to_plot ,  ncomponents ,  colour_by ,  shape_by ,  size_by , legend =  legend ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## Plot PCA and return appropriate object  if (  return_SCESet )  {   df_out -   mds_out [ ,   1 :  ncomponents ]    rownames (  df_out ) -   sampleNames (  object )    reducedDimension (  object ) -  df_out  if (  draw_plot )   print (  plot_out )   return (  object ) } else  { ## Return PCA plot   return (  plot_out ) } } ",
    "filename": "plotting.txt"
  }
}

3.
{
  "old_function": {
    "name": "plotMDS",
    "representation": "plotMDS",
    "signature": "signature (  SCESet )",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , return_SCESet = FALSE , draw_plot = TRUE , theme_size = 10 , legend = \"auto\" )",
    "body": "{   plotMDSSCESet (  object ,  ncomponents ,  colour_by ,  shape_by ,  size_by ,  return_SCESet ,  draw_plot ,  theme_size ,  legend ) } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotMDS",
    "representation": "plotMDS",
    "signature": "signature (  SCESet )",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , return_SCESet = FALSE , draw_plot = TRUE , exprs_values = \"exprs\" , theme_size = 10 , legend = \"auto\" )",
    "body": "{   plotMDSSCESet (  object ,  ncomponents ,  colour_by ,  shape_by ,  size_by ,  return_SCESet ,  draw_plot ,  exprs_values ,  theme_size ,  legend ) } ",
    "filename": "plotting.txt"
  }
}

4.
{
  "old_function": {
    "name": "plotReducedDim.SCESet",
    "representation": "plotReducedDim.SCESet",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , theme_size = 10 , legend = \"auto\" )",
    "body": "{ ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ## Set up indicator for whether to use pData or features for size_by and ## colour_by   colour_by_use_pdata -  TRUE   size_by_use_pdata -  TRUE ## Check arguments are valid  if (  !   is.null (  colour_by ) )  {  if (   !  (   colour_by %in%   varLabels (  object ) ) undefined  !  (   colour_by %in%   featureNames (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)]\" )  if (   !  (   colour_by %in%   varLabels (  object ) ) undefined  (   colour_by %in%   featureNames (  object ) ) )   colour_by_use_pdata -  FALSE } else  {  if (   \"is_cell_control\" %in%   varLabels (  object ) )   colour_by -  \"is_cell_control\" }  if (  !   is.null (  shape_by ) )  {  if (  !  (   shape_by %in%   varLabels (  object ) ) )   stop (  \"the argument 'shape_by' should specify a column of pData(object) [see varLabels(object)]\" )  if (    nlevels (   as.factor (    pData (  object ) [[  shape_by ] ] ) ) undefined  10 )   stop (  \"when coerced to a factor, 'shape_by' should have fewer than 10 levels\" ) } else  {  if (   \"is_cell_control\" %in%   varLabels (  object ) )   shape_by -  \"is_cell_control\" }  if (  !   is.null (  size_by ) )  {  if (   !  (   size_by %in%   varLabels (  object ) ) undefined  !  (   size_by %in%   featureNames (  object ) ) )   stop (  \"the argument 'size_by' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)]\" )  if (   !  (   size_by %in%   varLabels (  object ) ) undefined  (   size_by %in%   featureNames (  object ) ) )   size_by_use_pdata -  FALSE } ## Extract reduced dimension representation of cells  if (   is.null (   reducedDimension (  object ) ) )   stop (  \"reducedDimension slot of object is NULL. Need non null reducedDimension to plot.\" )   red_dim -   redDim (  object )  if (   ncomponents undefined   ncol (  red_dim ) )   stop (  \"ncomponents to plot is larger than number of columns of reducedDimension(object)\" ) ## Define data.frame for plotting   df_to_plot -   data.frame (   red_dim [ ,   1 :  ncomponents ] )  if (  !   is.null (  colour_by ) )  {  if (  colour_by_use_pdata )   colour_by_vals -    pData (  object ) [[  colour_by ] ] else   colour_by_vals -    exprs (  object ) [  colour_by , ]  if (    nlevels (   as.factor (  colour_by_vals ) ) undefined  10 )   df_to_plot -   data.frame (  df_to_plot , colour_by =  colour_by_vals ) else   df_to_plot -   data.frame (  df_to_plot , colour_by =   as.factor (  colour_by_vals ) ) }  if (  !   is.null (  shape_by ) )   df_to_plot -   data.frame (  df_to_plot , shape_by =   as.factor (    pData (  object ) [[  shape_by ] ] ) )  if (  !   is.null (  size_by ) )  {  if (  size_by_use_pdata )   size_by_vals -    pData (  object ) [[  size_by ] ] else   size_by_vals -    exprs (  object ) [  size_by , ]   df_to_plot -   data.frame (  df_to_plot , size_by =  size_by_vals ) } ## Call default method to make the plot   plot_out -   plotReducedDim.default (  df_to_plot ,  ncomponents ,  colour_by ,  shape_by ,  size_by , percentVar =  NULL , legend =  legend ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## return plot  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotReducedDim.SCESet",
    "representation": "plotReducedDim.SCESet",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , exprs_values = \"exprs\" , theme_size = 10 , legend = \"auto\" )",
    "body": "{ ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ## Check arguments are valid   colour_by_out -   .choose_vis_values (  object ,  colour_by , cell_control_default =  TRUE , check_features =  TRUE , exprs_values =  exprs_values )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val   shape_by_out -   .choose_vis_values (  object ,  shape_by , cell_control_default =  TRUE , coerce_factor =  TRUE , level_limit =  10 )   shape_by -   shape_by_out $ name   shape_by_vals -   shape_by_out $ val   size_by_out -   .choose_vis_values (  object ,  size_by , check_features =  TRUE , exprs_values =  exprs_values )   size_by -   size_by_out $ name   size_by_vals -   size_by_out $ val ## Extract reduced dimension representation of cells  if (   is.null (   reducedDimension (  object ) ) )   stop (  \"reducedDimension slot of object is NULL. Need non null reducedDimension to plot.\" )   red_dim -   redDim (  object )  if (   ncomponents undefined   ncol (  red_dim ) )   stop (  \"ncomponents to plot is larger than number of columns of reducedDimension(object)\" ) ## Define data.frame for plotting   df_to_plot -   data.frame (   red_dim [ ,   1 :  ncomponents ] )    df_to_plot $ colour_by -  colour_by_vals    df_to_plot $ shape_by -  shape_by_vals    df_to_plot $ size_by -  size_by_vals ## Call default method to make the plot   plot_out -   plotReducedDim.default (  df_to_plot ,  ncomponents ,  colour_by ,  shape_by ,  size_by , percentVar =  NULL , legend =  legend ) ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## return plot  plot_out } ",
    "filename": "plotting.txt"
  }
}

5.
{
  "old_function": {
    "name": "plotReducedDim",
    "representation": "plotReducedDim",
    "signature": "signature (  SCESet )",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , theme_size = 10 , legend = \"auto\" )",
    "body": "{   plotReducedDim.SCESet (  object ,  ncomponents ,  colour_by ,  shape_by ,  size_by ,  theme_size ,  legend ) } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotReducedDim",
    "representation": "plotReducedDim",
    "signature": "signature (  SCESet )",
    "parameters": "function ( object , ncomponents = 2 , colour_by = NULL , shape_by = NULL , size_by = NULL , exprs_values = \"exprs\" , theme_size = 10 , legend = \"auto\" )",
    "body": "{   plotReducedDim.SCESet (  object ,  ncomponents ,  colour_by ,  shape_by ,  size_by ,  exprs_values ,  theme_size ,  legend ) } ",
    "filename": "plotting.txt"
  }
}

6.
{
  "old_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , x_position = NULL , y_position = NULL , theme_size = 24 , legend = \"auto\" )",
    "body": "{ ## check object is SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"Object must be of class SCESet.\" ) ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ## Set up indicator for whether to use pData or features ## for colour_by   colour_by_use_pdata -  TRUE ## Check arguments are valid  if (  !   is.null (  colour_by ) )  {  if (   !  (   colour_by %in%   varLabels (  object ) ) undefined  !  (   colour_by %in%   featureNames (  object ) ) )   stop (  \"the argument 'colour_by' should specify a column of pData(object) [see varLabels(object)] or a feature [see featureNames(object)]\" )  if (   !  (   colour_by %in%   varLabels (  object ) ) undefined  (   colour_by %in%   featureNames (  object ) ) )   colour_by_use_pdata -  FALSE } else  {  if (   \"is_cell_control\" %in%   varLabels (  object ) )   colour_by -  \"is_cell_control\" } ## obtain well positions  if (  !   is.null (  plate_position ) )  {  if (    length (  plate_position ) !=   ncol (  object ) )   stop (  \"Supplied plate_position argument must have same length as number of columns of SCESet object.\" )   plate_position_char -  plate_position } else   plate_position_char -   object $ plate_position  if (   is.null (  plate_position_char ) )  {  if (    is.null (  x_position ) ||   is.null (  y_position ) )   stop (  \"If plate_position is NULL then both x_position and y_position must be supplied.\" )   plate_position_x -  x_position   plate_position_y -  y_position } else  {   plate_position_y -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position_char )   plate_position_y -   factor (  plate_position_y ,   rev (   sort (   unique (  plate_position_y ) ) ) )   plate_position_x -   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position_char )   plate_position_x -   ordered (   as.integer (  plate_position_x ) ) } ## Define data.frame for plotting   df_to_plot -   data.frame (  plate_position_x ,  plate_position_y )  if (  !   is.null (  plate_position_char ) )    df_to_plot [[  \"plate_position_char\" ] ] -  plate_position_char  if (  !   is.null (  colour_by ) )  {  if (  colour_by_use_pdata )   colour_by_vals -    pData (  object ) [[  colour_by ] ] else   colour_by_vals -    exprs (  object ) [  colour_by , ]   df_to_plot -   data.frame (  df_to_plot , colour_by =  colour_by_vals ) } ## make the plot   aesth -   aes ( x =  plate_position_x , y =  plate_position_y , fill =  colour_by )  if (  !   is.null (  plate_position_char ) )    aesth $ label -   as.symbol (  \"plate_position_char\" )   plot_out -    ggplot (  df_to_plot ,  aesth ) +   geom_point ( shape =  21 , size =  theme_size , colour =  \"gray50\" )  if (  !   is.null (  plate_position_char ) )   plot_out -   plot_out +   geom_text ( colour =  \"gray90\" ) ## make sure colours are nice   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =  TRUE ) ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## return plot  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , x_position = NULL , y_position = NULL , exprs_values = \"exprs\" , theme_size = 24 , legend = \"auto\" )",
    "body": "{ ## check object is SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"Object must be of class SCESet.\" ) ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ## Checking colour validity   colour_by_out -   .choose_vis_values (  object ,  colour_by , cell_control_default =  TRUE , check_features =  TRUE , exprs_values =  exprs_values )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## obtain well positions  if (  !   is.null (  plate_position ) )  {  if (    length (  plate_position ) !=   ncol (  object ) )   stop (  \"Supplied plate_position argument must have same length as number of columns of SCESet object.\" )   plate_position_char -  plate_position } else   plate_position_char -   object $ plate_position  if (   is.null (  plate_position_char ) )  {  if (    is.null (  x_position ) ||   is.null (  y_position ) )   stop (  \"If plate_position is NULL then both x_position and y_position must be supplied.\" )   plate_position_x -  x_position   plate_position_y -  y_position } else  {   plate_position_y -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position_char )   plate_position_y -   factor (  plate_position_y ,   rev (   sort (   unique (  plate_position_y ) ) ) )   plate_position_x -   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position_char )   plate_position_x -   ordered (   as.integer (  plate_position_x ) ) } ## Define data.frame for plotting   df_to_plot -   data.frame (  plate_position_x ,  plate_position_y )  if (  !   is.null (  plate_position_char ) )    df_to_plot [[  \"plate_position_char\" ] ] -  plate_position_char    df_to_plot $ colour_by -  colour_by_vals ## make the plot   aesth -   aes ( x =  plate_position_x , y =  plate_position_y , fill =  colour_by )  if (  !   is.null (  plate_position_char ) )    aesth $ label -   as.symbol (  \"plate_position_char\" )   plot_out -    ggplot (  df_to_plot ,  aesth ) +   geom_point ( shape =  21 , size =  theme_size , colour =  \"gray50\" )  if (  !   is.null (  plate_position_char ) )   plot_out -   plot_out +   geom_text ( colour =  \"gray90\" ) ## make sure colours are nice   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =  TRUE ) ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## return plot  plot_out } ",
    "filename": "plotting.txt"
  }
}

7.
{
  "old_function": {
    "name": "nexprs",
    "representation": "nexprs",
    "parameters": "function ( object , threshold = NULL , subset.row = NULL , byrow = FALSE )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )  {   stop (  \"'object' must be a SCESet\" ) }   is_exprs_mat -   is_exprs (  object )   counts_mat -   counts (  object )  if (    is.null (  is_exprs_mat ) undefined   is.null (  counts_mat ) )  {   stop (  \"either 'is_exprs(object)' or 'counts(object)' must be non-NULL\" ) } # Setting the detection threshold properly.  if (   is.null (  threshold ) )  {   threshold -   object @ lowerDetectionLimit }  if (  !   is.null (  counts_mat ) )  {    storage.mode (  threshold ) -   storage.mode (  counts_mat ) }  if (  !  byrow )  {  if (  !   is.null (  is_exprs_mat ) )  { # Counting expressing genes per cell, using predefined 'is_exprs(object)'.  if (   is.null (  subset.row ) )  {   return (   colSums (  is_exprs_mat ) ) } else  {   subset.row -   .subset2index (  subset.row , names =   rownames (  counts_mat ) )   return (   .checkedCall (  cxx_colsum_subset ,  is_exprs_mat ,  subset.row ) ) } } else  { # Counting expressing genes per cell, using the counts to define 'expressing'.  if (   is.null (  subset.row ) )  {   subset.row -   seq_len (   nrow (  counts_mat ) ) } else  {   subset.row -   .subset2index (  subset.row , names =   rownames (  counts_mat ) ) }   return (   .checkedCall (  cxx_colsum_exprs_subset ,  counts_mat ,  threshold ,  subset.row ) ) } } else  { # Counting expressing cells per gene.  if (  !   is.null (  is_exprs_mat ) )  {   return (   rowSums (  is_exprs_mat ) ) } else  {   return (   .checkedCall (  cxx_rowsum_exprs ,  counts_mat ,  threshold ) ) } } } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "nexprs",
    "representation": "nexprs",
    "parameters": "function ( object , lowerDetectionLimit = NULL , exprs_values = NULL , byrow = FALSE , subset_row = NULL , subset_col = NULL )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )  {   stop (  \"'object' must be a SCESet\" ) }   is_exprs_mat -   is_exprs (  object )   exprs_values -   .exprs_hunter (  object ,  exprs_values )   exprs_mat -   suppressWarnings (   get_exprs (  object ,  exprs_values ) )  if (    is.null (  is_exprs_mat ) undefined   is.null (  exprs_mat ) )  {   stop (   sprintf (  \"either 'is_exprs(object)' or '%s(object)' must be non-NULL\" ,  exprs_values ) ) } # Setting the detection lowerDetectionLimit properly.  if (   is.null (  lowerDetectionLimit ) )  {   lowerDetectionLimit -   object @ lowerDetectionLimit }  if (  !   is.null (  exprs_mat ) )  {    storage.mode (  lowerDetectionLimit ) -   storage.mode (  exprs_mat ) }  if (  !  byrow )  {  if (  !   is.null (  is_exprs_mat ) )  { # Counting expressing genes per cell, using predefined 'is_exprs(object)'.  if (   is.null (  subset_row ) )  {   out -   colSums (  is_exprs_mat ) } else  {   subset_row -   .subset2index (  subset_row ,  is_exprs_mat )   out -   .checkedCall (  cxx_colsum_subset ,  is_exprs_mat ,   subset_row -  1L )    names (  out ) -   colnames (  is_exprs_mat ) } } else  { # Counting expressing genes per cell, using the counts to define 'expressing'.  if (   is.null (  subset_row ) )  {   subset_row -   seq_len (   nrow (  exprs_mat ) ) } else  {   subset_row -   .subset2index (  subset_row ,  exprs_mat ) }   out -   .checkedCall (  cxx_colsum_exprs_subset ,  exprs_mat ,  lowerDetectionLimit ,   subset_row -  1L )    names (  out ) -   colnames (  exprs_mat ) } } else  {  if (  !   is.null (  is_exprs_mat ) )  { # Counting expressing cells per gene, using predefined 'is_exprs(object)'.  if (   is.null (  subset_col ) )  {   out -   rowSums (  is_exprs_mat ) } else  {   subset_col -   .subset2index (  subset_col ,  is_exprs_mat , byrow =  FALSE )   out -   .checkedCall (  cxx_rowsum_subset ,  is_exprs_mat ,   subset_col -  1L )    names (  out ) -   rownames (  is_exprs_mat ) } } else  { # Counting expressing cells per gene, using the counts to define 'expressing'.  if (   is.null (  subset_col ) )  {   subset_col -   seq_len (   ncol (  exprs_mat ) ) } else  {   subset_col -   .subset2index (  subset_col ,  exprs_mat , byrow =  FALSE ) }   out -   .checkedCall (  cxx_rowsum_exprs_subset ,  exprs_mat ,  lowerDetectionLimit ,   subset_col -  1L )    names (  out ) -   rownames (  exprs_mat ) } }   return (  out ) } ",
    "filename": "calculate-expression.txt"
  }
}

8.
{
  "old_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , na.rm = FALSE )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }   cur.med -   median (  metric , na.rm =  na.rm )   cur.mad -   mad (  metric , center =  cur.med , na.rm =  na.rm )   type -   match.arg (  type )   upper.limit -   cur.med +   nmads *  cur.mad   lower.limit -   cur.med -   nmads *  cur.mad  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   return (    metric undefined  lower.limit |   upper.limit undefined  metric ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }  if (   any (   is.na (  metric ) ) )  {   warning (  \"missing values ignored during outlier detection\" ) }  if (  !   is.null (  batch ) )  {   N -   length (  metric )  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) } # Coercing non-NULL subset into a logical vector.  if (  !   is.null (  subset ) )  {   new.subset -   logical (  N )    names (  new.subset ) -   names (  metric )    new.subset [  subset ] -  TRUE   subset -  new.subset } # Computing QC metrics for each batch.   by.batch -   split (   seq_len (  N ) ,  batch )   collected -   logical (  N )  for  ( b in  by.batch )  {    collected [  b ] -   Recall (   metric [  b ] , nmads =  nmads , type =  type , log =  FALSE , subset =   subset [  b ] , batch =  NULL ) }   return (  collected ) } # Computing median/MAD based on subsets.  if (  !   is.null (  subset ) )  {   submetric -   metric [  subset ]  if (    length (  submetric ) ==  0L )  {   warning (  \"no observations remaining after subsetting\" ) } } else  {   submetric -  metric }   cur.med -   median (  submetric , na.rm =  TRUE )   cur.mad -   mad (  submetric , center =  cur.med , na.rm =  TRUE )   type -   match.arg (  type )   upper.limit -   cur.med +   nmads *  cur.mad   lower.limit -   cur.med -   nmads *  cur.mad  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   return (    metric undefined  lower.limit |   upper.limit undefined  metric ) } ",
    "filename": "qc.txt"
  }
}

9.
{
  "old_function": {
    "name": "newSCESet",
    "representation": "newSCESet",
    "parameters": "function ( exprsData = NULL , countData = NULL , tpmData = NULL , fpkmData = NULL , cpmData = NULL , phenoData = NULL , featureData = NULL , experimentData = NULL , is_exprsData = NULL , cellPairwiseDistances = dist ( vector ( ) ) , featurePairwiseDistances = dist ( vector ( ) ) , lowerDetectionLimit = 0 , logExprsOffset = 1 , logged = FALSE , useForExprs = \"exprs\" )",
    "body": "{ ## Check that we have some expression data  if (       is.null (  exprsData ) undefined   is.null (  countData ) undefined   is.null (  tpmData ) undefined   is.null (  fpkmData ) undefined   is.null (  cpmData ) )   stop (  \"Require at least one of exprsData, tpmData, fpkmData or countData arguments.\" ) ## Check dimensions of data matrices ## Check counts are a matrix; renames is_exprsData if not null  if (  !   is.null (  countData ) )   countData -   as.matrix (  countData )  if (  !   is.null (  is_exprsData ) )   isexprs -  is_exprsData ## If no exprsData provided define is_exprs from tpmData, fpkmData or countData  if (   is.null (  exprsData ) )  { ## Define exprs data if null  if (  !   is.null (  tpmData ) )  {   exprsData -   log2 (   tpmData +  logExprsOffset )   logged -  TRUE } else  {  if (  !   is.null (  fpkmData ) )  {   exprsData -   log2 (   fpkmData +  logExprsOffset )   logged -  TRUE } else  {  if (  !   is.null (  cpmData ) )  {   exprsData -   log2 (   cpmData +  logExprsOffset )   logged -  TRUE } else  {   exprsData -   edgeR :: cpm.default (  countData , prior.count =  logExprsOffset , log =  TRUE )   logged -  TRUE } } } } else  {   exprsData -   as.matrix (  exprsData ) } ## Generate valid phenoData and featureData if not provided  if (   is.null (  phenoData ) )   phenoData -   annotatedDataFrameFrom (  exprsData , byrow =  FALSE )  if (   is.null (  featureData ) )   featureData -   annotatedDataFrameFrom (  exprsData , byrow =  TRUE ) ## Check experimentData   expData_null -   new (  \"MIAME\" , name =  \" , lab =  \" , contact =  \" , title =  \" , abstract =  \"An SCESet\" , url =  \" , other =   list ( notes =  \"This dataset created from ...\" , coauthors =   c (  \"\" ) ) )  if (  !   is.null (  experimentData ) )  {  if (   is (  experimentData ,  \"MIAME\" ) )   expData -  experimentData else  {   expData -  expData_null   warning (  \"experimentData supplied is not an 'MIAME' object. Thus, experimentData is being set to an empty MIAME object.\\n Please supply a valid 'MIAME' class object containing experiment data to experimentData(object).\" ) } } else  {   expData -  expData_null } ## Check valid useForExprs   useForExprs -   match.arg (  useForExprs ,   c (  \"exprs\" ,  \"tpm\" ,  \"counts\" ,  \"fpkm\" ) ) ## Generate new SCESet object  if (  !   is.null (  is_exprsData ) )   assaydata -   assayDataNew (  \"lockedEnvironment\" , exprs =  exprsData , is_exprs =  isexprs ) else   assaydata -   assayDataNew (  \"lockedEnvironment\" , exprs =  exprsData )   sceset -   new (  \"SCESet\" , assayData =  assaydata , phenoData =  phenoData , featureData =  featureData , experimentData =  expData , cellPairwiseDistances =  cellPairwiseDistances , featurePairwiseDistances =  featurePairwiseDistances , lowerDetectionLimit =  lowerDetectionLimit , logExprsOffset =  logExprsOffset , logged =  logged , featureControlInfo =   AnnotatedDataFrame ( ) , useForExprs =  useForExprs ) ## Add non-null slots to assayData for SCESet object, omitting null slots  if (  !   is.null (  tpmData ) )    tpm (  sceset ) -  tpmData  if (  !   is.null (  fpkmData ) )    fpkm (  sceset ) -  fpkmData  if (  !   is.null (  countData ) )    counts (  sceset ) -  countData  if (  !   is.null (  cpmData ) )    cpm (  sceset ) -  cpmData ## Check validity of object   validObject (  sceset )  sceset } ",
    "filename": "SCESet-methods.txt"
  },
  "new_function": {
    "name": "newSCESet",
    "representation": "newSCESet",
    "parameters": "function ( exprsData = NULL , countData = NULL , tpmData = NULL , fpkmData = NULL , cpmData = NULL , phenoData = NULL , featureData = NULL , experimentData = NULL , is_exprsData = NULL , cellPairwiseDistances = dist ( vector ( ) ) , featurePairwiseDistances = dist ( vector ( ) ) , lowerDetectionLimit = NULL , logExprsOffset = NULL )",
    "body": "{ ## Checking which value to use, in the hierarchy specified.   have.data -  NULL  for  ( dataname in   c (  \"countData\" ,  \"tpmData\" ,  \"cpmData\" ,  \"fpkmData\" ,  \"exprsData\" ) )  {   eData -   get (  dataname )  if (  !   is.null (  eData ) )  {  if (  !   is.null (  have.data ) )  {   warning (   sprintf (  \"'%s' provided, '%s' will be ignored\" ,  have.data ,  dataname ) )   assign (  dataname ,  NULL ) } else  {   assign (  dataname ,   as.matrix (  eData ) )   have.data -  dataname } } }  if (   is.null (  have.data ) )  {   stop (  \"one set of expression values should be supplied\" ) }  if (  !   is.null (  is_exprsData ) )  {  if (   have.data !=  \"exprsData\" )  {   warning (   sprintf (  \"'%s' provided, 'is_exprsData' will be ignored\" ,  have.data ) )   is_exprsData -  NULL } else  {   is_exprsData -   as.matrix (  is_exprsData ) } } ## Setting logExprsOffset and lowerDetectionLimit.  if (   is.null (  logExprsOffset ) )  {   logExprsOffset -  1  if (   have.data !=  \"countData\" )  {   warning (  \"'logExprsOffset' should be set manually for non-count data\" ) } }  if (   is.null (  lowerDetectionLimit ) )  {   lowerDetectionLimit -  0  if (   have.data ==  \"exprsData\" )  {   warning (  \"'lowerDetectionLimit' should be set manually for log-expression values\" ) } } ## If no exprsData provided, define it from counts or T/C/FPKMs  if (   have.data ==  \"countData\" )  {   exprsData -   .compute_exprs (  countData , size_factors =   colSums (  countData ) , log =  TRUE , sum =  FALSE , logExprsOffset =  logExprsOffset )    dimnames (  exprsData ) -   dimnames (  countData ) } else  if (   have.data !=  \"exprsData\" )  {   exprsData -   log2 (    get (  have.data ) +  logExprsOffset ) } ## Generate valid phenoData and featureData if not provided  if (   is.null (  phenoData ) )   phenoData -   annotatedDataFrameFrom (  exprsData , byrow =  FALSE )  if (   is.null (  featureData ) )   featureData -   annotatedDataFrameFrom (  exprsData , byrow =  TRUE ) ## Check experimentData   expData_null -   new (  \"MIAME\" , name =  \" , lab =  \" , contact =  \" , title =  \" , abstract =  \"An SCESet\" , url =  \" , other =   list ( notes =  \"This dataset created from ...\" , coauthors =   c (  \"\" ) ) )  if (  !   is.null (  experimentData ) )  {  if (   is (  experimentData ,  \"MIAME\" ) )   expData -  experimentData else  {   expData -  expData_null   warning (  \"'experimentData' is not an 'MIAME' object, setting to an empty object\" ) } } else  {   expData -  expData_null } ## Generate new SCESet object   assaydata -   assayDataNew (  \"lockedEnvironment\" , exprs =  exprsData )   sceset -   new (  \"SCESet\" , assayData =  assaydata , phenoData =  phenoData , featureData =  featureData , experimentData =  expData , cellPairwiseDistances =  cellPairwiseDistances , featurePairwiseDistances =  featurePairwiseDistances , lowerDetectionLimit =  lowerDetectionLimit , logExprsOffset =  logExprsOffset , featureControlInfo =   AnnotatedDataFrame ( ) ) ## Add non-null slots to assayData for SCESet object, omitting null slots  if (  !   is.null (  is_exprsData ) )    is_exprs (  sceset ) -  is_exprsData  if (  !   is.null (  tpmData ) )    tpm (  sceset ) -  tpmData  if (  !   is.null (  fpkmData ) )    fpkm (  sceset ) -  fpkmData  if (  !   is.null (  countData ) )    counts (  sceset ) -  countData  if (  !   is.null (  cpmData ) )    cpm (  sceset ) -  cpmData ## Check validity of object   validObject (  sceset )  sceset } ",
    "filename": "SCESet-methods.txt"
  }
}

10.
{
  "old_function": {
    "name": "mergeSCESet",
    "representation": "mergeSCESet",
    "parameters": "function ( x , y , fdata_cols_x = 1 : ncol ( fData ( x ) ) , fdata_cols_y = fdata_cols_x , pdata_cols_x = NULL , pdata_cols_y = NULL )",
    "body": "{  if (  !   is (  x ,  'SCESet' ) )   stop (  'x must be of type SCESet' )  if (  !   is (  y ,  'SCESet' ) )   stop (  'y must be of type SCESet' )  if (  !   identical (   featureNames (  x ) ,   featureNames (  y ) ) )   stop (  \"feature names of x and y must be identical\" )  if (    x @ logged !=   y @ logged )   stop (  \"x and y do not have the same value for the 'logged' slot.\" )  if (    x @ lowerDetectionLimit !=   y @ lowerDetectionLimit )   stop (  \"x and y do not have the same lowerDetectionLimit.\" )  if (    x @ logExprsOffset !=   y @ logExprsOffset )   stop (  \"x and y do not have the same logExprsOffset.\" ) ## combine fData  if (    ncol (   fData (  x ) ) ==  0 )  {  if (  !   identical (   fData (  x ) ,   fData (  y ) ) )   stop (  \"featureData do not match for x and y.\" )   new_fdata -   as (   fData (  x ) ,  \"AnnotatedDataFrame\" ) } else  {   fdata1 -    fData (  x ) [ ,  fdata_cols_x , drop =  FALSE ]   fdata2 -    fData (  y ) [ ,  fdata_cols_y , drop =  FALSE ]  if (  !   identical (  fdata1 ,  fdata2 ) )   stop (  \"featureData columns specified are not identical for x and y.\" )   new_fdata -   as (  fdata1 ,  \"AnnotatedDataFrame\" ) } ## combine pData  if (     ncol (   pData (  x ) ) ==  0 ||    pData (  y ) ==  0 )   stop (  \"phenoData slot is empty for x or y.\" )   pdata_x -   pData (  x )   pdata_y -   pData (  y )  if (   is.null (  pdata_cols_x ) )  {  if (   is.null (  pdata_cols_y ) )  {   pdata_cols_x -   which (    colnames (  pdata_x ) %in%   colnames (  pdata_y ) )   pdata_cols_y -   which (    colnames (  pdata_y ) %in%   colnames (  pdata_x ) ) } else   pdata_cols_x -   which (    colnames (  pdata_x ) %in%    colnames (  pdata_y ) [  pdata_cols_y ] ) } else  {  if (   is.null (  pdata_cols_y ) )   pdata_cols_y -   which (    colnames (  pdata_y ) %in%    colnames (  pdata_x ) [  pdata_cols_x ] ) }  if (     length (  pdata_cols_x ) ==  0 |    length (  pdata_cols_y ) ==  0 )   stop (  \"no phenoData column names found in common between x and y.\" ) ## make sure ordering of columns is correct   pdata_x -   pdata_x [ ,  pdata_cols_x , drop =  FALSE ]   pdata_y -   pdata_y [ ,  pdata_cols_y , drop =  FALSE ]   mm -   match (   colnames (  pdata_x ) ,   colnames (  pdata_y ) )   pdata_y -   pdata_y [ ,  mm ]  if (  !   identical (   colnames (  pdata_x ) ,   colnames (  pdata_y ) ) )   stop (  \"phenoData columns specified are not identical for x and y.\" )   new_pdata -   rbind (  pdata_x ,  pdata_y )   new_pdata -   as (  new_pdata ,  \"AnnotatedDataFrame\" ) ## combine exprsData   new_exprs -   Biobase :: combine (   exprs (  x ) ,   exprs (  y ) ) ## new SCESet   merged_sceset -   newSCESet ( exprsData =  new_exprs , featureData =  new_fdata , phenoData =  new_pdata , logged =   x @ logged , logExprsOffset =   x @ logExprsOffset ) ## add remaining assayData to merged SCESet   assay_names -   intersect (   names (   Biobase :: assayData (  x ) ) ,   names (   Biobase :: assayData (  y ) ) )  for  ( assaydat in  assay_names )  {   new_dat -   Biobase :: combine (   get_exprs (  x ,  assaydat ) ,   get_exprs (  y ,  assaydat ) )    set_exprs (  merged_sceset ,  assaydat ) -  new_dat }  merged_sceset } ",
    "filename": "SCESet-methods.txt"
  },
  "new_function": {
    "name": "mergeSCESet",
    "representation": "mergeSCESet",
    "parameters": "function ( x , y , fdata_cols = NULL , pdata_cols = NULL )",
    "body": "{  if (  !   is (  x ,  'SCESet' ) )   stop (  'x must be of type SCESet' )  if (  !   is (  y ,  'SCESet' ) )   stop (  'y must be of type SCESet' )  if (  !   identical (   featureNames (  x ) ,   featureNames (  y ) ) )   stop (  \"feature names of x and y must be identical\" )  for  ( sl in   c (  \"lowerDetectionLimit\" ,  \"logExprsOffset\" ,  \"featureControlInfo\" ) )  {  if (  !   identical (   slot (  x ,  sl ) ,   slot (  y ,  sl ) ) )   stop (   sprintf (  \"x and y do not have the same %s\" ,  sl ) ) } ## check consistent fData  if (   is.null (  fdata_cols ) )  {   fdata_cols -   intersect (   colnames (   fData (  x ) ) ,   colnames (   fData (  y ) ) ) } else  if (  !   is.character (  fdata_cols ) )  {   fdata_cols -    colnames (   fData (  x ) ) [  fdata_cols ] }   fdata1 -    fData (  x ) [ ,  fdata_cols , drop =  FALSE ]   fdata2 -    fData (  y ) [ ,  fdata_cols , drop =  FALSE ]  if (  !   identical (  fdata1 ,  fdata2 ) )   stop (  \"specified featureData columns are not identical for x and y\" )   new_fdata -   as (  fdata1 ,  \"AnnotatedDataFrame\" ) ## combine pData  if (   is.null (  pdata_cols ) )  {   pdata_cols -   intersect (   colnames (   pData (  x ) ) ,   colnames (   pData (  y ) ) ) } else  if (  !   is.character (  pdata_cols ) )  {   pdata_cols -    colnames (   pData (  x ) ) [  pdata_cols ] }   pdata_x -    pData (  x ) [ ,  pdata_cols , drop =  FALSE ]   pdata_y -    pData (  y ) [ ,  pdata_cols , drop =  FALSE ]  if (  !   identical (   colnames (  pdata_x ) ,   colnames (  pdata_y ) ) )   stop (  \"phenoData column names are not identical for x and y\" )  if (   ncol (  pdata_x ) )  {   new_pdata -   rbind (  pdata_x ,  pdata_y ) } else  {   new_pdata -   data.frame ( row.names =   c (   rownames (  pdata_x ) ,   rownames (  pdata_y ) ) ) }   new_pdata -   as (  new_pdata ,  \"AnnotatedDataFrame\" ) ## combine exprsData   new_exprs -   Biobase :: combine (   exprs (  x ) ,   exprs (  y ) ) ## new SCESet   merged_sceset -   newSCESet ( exprsData =  new_exprs , featureData =  new_fdata , phenoData =  new_pdata , lowerDetectionLimit =   x @ lowerDetectionLimit , logExprsOffset =   x @ logExprsOffset ) ## checking that the controls actually exist in the merged object   all.fnames -   .fcontrol_names (  x )   discard -   logical (   length (  all.fnames ) )  for  ( f in   seq_along (  all.fnames ) )  {   fc -   all.fnames [  f ]   which.current -    fData (  merged_sceset ) [[   paste0 (  \"is_feature_control_\" ,  fc ) ] ]  if (   is.null (  which.current ) )  {   warning (   sprintf (  \"removing undefined feature control set '%s'\" ,  fc ) )    discard [  f ] -  TRUE } }    featureControlInfo (  merged_sceset ) -    featureControlInfo (  x ) [  !  discard , ] ## add remaining assayData to merged SCESet   assay_names -   intersect (   names (   Biobase :: assayData (  x ) ) ,   names (   Biobase :: assayData (  y ) ) )  for  ( assaydat in  assay_names )  {   new_dat -   Biobase :: combine (   get_exprs (  x ,  assaydat ) ,   get_exprs (  y ,  assaydat ) )    set_exprs (  merged_sceset ,  assaydat ) -  new_dat }  merged_sceset } ",
    "filename": "SCESet-methods.txt"
  }
}

11.
{
  "old_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "signature ( object =  SCESet )",
    "parameters": "function ( object , exprs_values = \"counts\" , logExprsOffset = NULL , recompute_cpm = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet.\" ) ## Define expression values to be used   exprs_values -   match.arg (  exprs_values ,   c (  \"tpm\" ,  \"fpkm\" ,  \"counts\" ) )   isCount -   exprs_values ==  \"counts\"   exprs_mat -   get_exprs (  object ,  exprs_values ) ## extract existing size factors   size_factors -   sizeFactors (  object )  if (   is.null (  size_factors ) )  {   message (  \"No size factors defined in object$size_factor so returning\r\n                original object\" )   return (  object ) } ## figuring out how many controls have their own size factors   control_list -   list ( )  for  ( fc in   .get_feature_control_names (  object ) )  {   specific_sf -   suppressWarnings (   sizeFactors (  object , type =  fc ) )  if (  !   is.null (  specific_sf ) )  {   which.current -    fData (  object ) [[   paste0 (  \"is_feature_control_\" ,  fc ) ] ]    control_list [[  fc ] ] -   list ( SF =  specific_sf , ID =  which.current ) } } ## extract logExprsOffset if argument is NULL  if (   is.null (  logExprsOffset ) )   logExprsOffset -   object @ logExprsOffset ## recompute cpm if desired  if (    !   is.null (   cpm (  object ) ) undefined  recompute_cpm undefined  isCount )  {   lib_size -   colSums (  exprs_mat )   new_cpm -   .recompute_cpm_fun ( exprs_mat =  exprs_mat , size_factors =  size_factors , lib_size =  lib_size , logExprsOffset =  logExprsOffset )  for  ( alt in  control_list )  {    new_cpm [   alt $ ID , ] -   .recompute_cpm_fun ( exprs_mat =   exprs_mat [   alt $ ID , , drop =  FALSE ] , size_factors =   alt $ SF , lib_size =  lib_size , logExprsOffset =  logExprsOffset ) }    cpm (  object ) -  new_cpm } ## compute normalised expression values   norm_exprs_mat -   .recompute_expr_fun ( exprs_mat =  exprs_mat , size_factors =  size_factors , logExprsOffset =  logExprsOffset , isCount =  isCount )  for  ( alt in  control_list )  {    norm_exprs_mat [   alt $ ID , ] -   .recompute_expr_fun (   exprs_mat [   alt $ ID , , drop =  FALSE ] , size_factors =   alt $ SF , logExprsOffset =  logExprsOffset , isCount =  isCount ) } ## add normalised values to object    norm_exprs (  object ) -  norm_exprs_mat  if (  return_norm_as_exprs )    exprs (  object ) -  norm_exprs_mat ## return object   return (  object ) } ",
    "replacementFunction": "normalize.SCESet",
    "filename": "normalisation.txt"
  },
  "new_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "signature ( object =  SCESet )",
    "parameters": "function ( object , exprs_values = NULL , logExprsOffset = NULL , centre_size_factors = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"'object' must be an SCESet\" ) ## Define expression values to be used   exprs_values -   .exprs_hunter (  object ,  exprs_values )  if (   exprs_values ==  \"exprs\" )  {   stop (  \"cannot compute normalized values from 'exprs'\" ) }   exprs_mat -   get_exprs (  object ,  exprs_values , warning =  FALSE )  if (   exprs_values ==  \"counts\" )  { ## extract existing size factors   size_factors -   suppressWarnings (   sizeFactors (  object ) )  if (   is.null (  size_factors ) )  {   warning (  \"skipping normalization of counts as size factors were not defined\" )   return (  object ) } ## figuring out how many controls have their own size factors   control_list -   .find_control_SF (  object )   spike.names -   .spike_fcontrol_names (  object )   no.spike.sf -  !   spike.names %in%   names (  control_list )  if (   any (  no.spike.sf ) )  {   warning (   sprintf (  \"spike-in transcripts in '%s' should have their own size factors\" ,    spike.names [  no.spike.sf ] [  1 ] ) ) } } else  {   size_factors -   rep (  1 ,   ncol (  object ) ) # ignoring size factors for non-count data.   control_list -   list ( ) } ## extract logExprsOffset if argument is NULL  if (   is.null (  logExprsOffset ) )   logExprsOffset -   object @ logExprsOffset ## compute normalised expression values   norm_exprs_mat -   .recompute_expr_fun ( exprs_mat =  exprs_mat , size_factors =  size_factors , logExprsOffset =  logExprsOffset )  for  ( alt in  control_list )  {    norm_exprs_mat [   alt $ ID , ] -   .recompute_expr_fun (  exprs_mat , size_factors =   alt $ SF , logExprsOffset =  logExprsOffset , subset_row =   alt $ ID ) } ## add normalised values to object    norm_exprs (  object ) -  norm_exprs_mat  if (  return_norm_as_exprs )    exprs (  object ) -  norm_exprs_mat ## centering all existing size factors if requested  if (    exprs_values ==  \"counts\" undefined  centre_size_factors )  {   all.sf.fields -   c (  \"size_factor\" ,   sprintf (  \"size_factor_%s\" ,   names (  control_list ) ) )  for  ( sf in  all.sf.fields )  {   cur.sf -    pData (  object ) [[  sf ] ]   cur.sf -   cur.sf /   mean (  cur.sf )     pData (  object ) [[  sf ] ] -  cur.sf } } ## return object   return (  object ) } ",
    "replacementFunction": "normalize.SCESet",
    "filename": "normalisation.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_5 scater_release_3_6

{
    "package": "scater",
    "release_versions": "scater_release_3_5 scater_release_3_6",
    "desc_release_old": "1.4.0",
    "desc_release_new": "1.6.3",
    "old_release_number": 2,
    "new_release_number": 3,
    "function_removals": 79,
    "function_additions": 14,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 3,
    "total_count": 82
}

##########
Functions Removed
##########

cellDist<-
cellNames<-
cellPairwiseDistances<-
cpm<-
featDist<-
featureControlInfo<-
featurePairwiseDistances<-
is_exprs<-
norm_counts<-
norm_cpm<-
norm_fpkm<-
norm_tpm<-
redDim<-
reducedDimension<-
setSpike<-
set_exprs<-
tpm<-
arrange.SCESet
bootstraps.SCESet
cellDist
cellDistSCESet
cellNames
cellPairwiseDistances
cellPairwiseDistances.SCESet
counts.SCESet
cpm
cpmSCESet
featDist
featDistSCESet
featureControlInfo
featureControlInfo.SCESet
featurePairwiseDistances
featurePairwiseDistancesSCESet
filter.SCESet
fpkm.SCESet
get_exprs
get_exprs.SCESet
isSpike
is_exprs
is_exprs.SCESet
mergeSCESet
mutate.SCESet
norm_counts
norm_counts.SCESet
norm_cpm
norm_cpm.SCESet
norm_exprs.SCESet
norm_fpkm
norm_fpkm.SCESet
norm_tpm
norm_tpm.SCESet
normalize.SCESet
plotDiffusionMapSCESet
plotExpressionSCESet
plotMDSSCESet
plotPCASCESet
plotReducedDim.SCESet
plotReducedDim.default
plotSCESet
redDim
redDim.SCESet
reducedDimension
reducedDimension.SCESet
rename.SCESet
sizeFactors.SCESet
spikes
stand_exprs.SCESet
tpm
tpm.SCESet
whichSpike
writeSCESet
[
counts<-
fData<-
pData<-
sizeFactors<-
counts
plot
sizeFactors


##########
Functions Added
##########

downsampleCounts
exprs
normalizeSCE
plotCellData
plotColData
plotPCASCE
plotReducedDimDefault
plotRowData
plotScater
read10xResults
runDiffusionMap
runPCA
runTSNE
toSingleCellExperiment


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "scater_gui",
    "representation": "scater_gui",
    "parameters": "function ( sce_set )",
    "body": "{   pd -   names (   pData (  sce_set ) )   pd.plot -   pd [   !   grepl (  \"filter_\" ,  pd ) undefined  !   grepl (  \"is_\" ,  pd ) ]   featurenames -   featureNames (  sce_set )   exprs_values -   names (   Biobase :: assayData (  sce_set ) )   exprs_values -   exprs_values [  !   grepl (  \"is_exprs\" ,  exprs_values ) ]   shinyApp (   ui -   dashboardPage (   dashboardHeader ( title =  \"scater\" ) ,   dashboardSidebar (   sidebarMenu (   menuItem (  \"plot\" , tabName =  \"plot\" ) ,   menuItem (  \"plotQC\" , tabName =  \"plotQC\" ) ,   menuItem (  \"plotPCA - QC metrics\" , tabName =  \"plotPCA_QC\" ) ,   menuItem (  \"plotPCA - expression\" , tabName =  \"plotPCA\" ) ,   menuItem (  \"plotTSNE\" , tabName =  \"plotTSNE\" ) ,   menuItem (  \"plotDiffusionMap\" , tabName =  \"plotDiffusionMap\" ) ,   menuItem (  \"plotExpression\" , tabName =  \"plotExpression\" ) ) ) ,   dashboardBody (   tabItems (   tabItem ( tabName =  \"plot\" ,   fluidRow (   box (   HTML (  [1311 chars quoted with '\"'] ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plot\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   selectInput (  \"block1\" ,  \"block1:\" ,  pd.plot , selected =   pd.plot [  2 ] ) ,   selectInput (  \"block2\" ,  \"block2:\" ,  pd.plot , selected =   pd.plot [  3 ] ) ,   selectInput (  \"colour_by\" ,  \"colour_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"exprs\" ) ) ) ) ,   tabItem ( tabName =  \"plotQC\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   box (   plotOutput (  \"plotQC\" , height =  600 ) , width =  8 ) ,   box (   radioButtons (  \"QCtype\" , label =  \"Choose a type of QC plot\" , choices =   c (  \"highest-expression\" ,  \"explanatory-variables\" ,  \"exprs-freq-vs-mean\" ) , selected =  \"highest-expression\" ) , width =  4 ) ) ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ,   box (   plotOutput (  \"plotQCfindpc\" , height =  600 ) , width =  8 ) ,   box (   radioButtons (  \"QCvar\" , label =  \"Choose a variable of interest\" , choices =  pd.plot , selected =  \"total_features\" ) , width =  4 ) ) ) ,   tabItem ( tabName =  \"plotPCA_QC\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotPCA_QC\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   checkboxInput (  \"pcaqc_detect_outliers\" ,  \"detect outliers?\" , value =  TRUE ) ,   selectInput (  \"pcaqc_selected_vars\" ,  \"variables to use for PCA:\" ,  pd.plot , selected =   c (  \"pct_counts_top_100_features\" ,  \"total_features\" ,  \"pct_counts_feature_controls\" ,  \"n_detected_feature_controls\" ,  \"log10_counts_endogenous_features\" ,  \"log10_counts_feature_controls\" ) , multiple =  TRUE ) ,   textInput (  \"pcaqc_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"pcaqc_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"pcaqc_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   numericInput (  \"pcaqc_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"pcaqc_scale_features\" ,  \"scale_features\" , value =  TRUE ) ) ) ) ,   tabItem ( tabName =  \"plotPCA\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotPCA\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 , # selectInput(\"pca_colour_by\", \"colour_by (either cell metadata or feature expression):\", #             pd.plot, #             selected = pd.plot[4]),   textInput (  \"pca_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"pca_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"pca_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"pca_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"exprs\" ) ,   numericInput (  \"pca_ntop\" ,  \"number of most variable features to use:\" ,  500 , min =  50 , max =  10000 , step =  25 ) ,   numericInput (  \"pca_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"pca_scale_features\" ,  \"scale_features\" , value =  TRUE ) ) ) ) ,   tabItem ( tabName =  \"plotTSNE\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotTSNE\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   textInput (  \"tsne_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"tsne_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"tsne_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"tsne_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"exprs\" ) ,   numericInput (  \"tsne_ntop\" ,  \"number of most variable features to use:\" ,  500 , min =  50 , max =  10000 , step =  25 ) ,   numericInput (  \"tsne_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"tsne_scale_features\" ,  \"scale_features\" , value =  TRUE ) ,   numericInput (  \"tsne_rand_seed\" ,  \"random seed to make plot reproducible:\" ,  5000 ) ) ) ) ,   tabItem ( tabName =  \"plotDiffusionMap\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotDiffusionMap\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   textInput (  \"diffmap_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"diffmap_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"diffmap_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"diffmap_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"exprs\" ) ,   numericInput (  \"diffmap_ntop\" ,  \"number of most variable features to use:\" ,  500 , min =  50 , max =  10000 , step =  25 ) ,   numericInput (  \"diffmap_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"diffmap_scale_features\" ,  \"scale_features\" , value =  TRUE ) ,   numericInput (  \"diffmap_rand_seed\" ,  \"random seed to make plot reproducible:\" ,  5000 ) ,   radioButtons (  \"diffmap_distance\" , label =  \"Choose a distance metric\" , choices =   c (  \"euclidean\" ,  \"cosine\" ,  \"rankcor\" ) , selected =  \"euclidean\" ) ) ) ) ,   tabItem ( tabName =  \"plotExpression\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotExpression\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   selectInput (  \"exprs_features\" ,  \"features:\" ,  featurenames , selected =   featurenames [   1 :  6 ] , multiple =  TRUE ) , # selectInput(\"exprs_x\", \"x-axis variable:\", #             pd.plot, #             selected = pd.plot[4]),   textInput (  \"exprs_x\" ,  \"x-axis variable (either cell metadata variable or feature name):\" ,   pd.plot [  4 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"exprs_colour_by\" ,  \"colour_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"exprs_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"exprs_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"exprs_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"exprs\" ) ,   numericInput (  \"exprs_ncols\" ,  \"number of columns:\" ,  2 , min =  1 , max =  8 ) ,   checkboxInput (  \"exprs_show_median\" ,  \"show median?\" , value =  FALSE ) ,   checkboxInput (  \"exprs_show_violin\" ,  \"show violin?\" , value =  TRUE ) ,   checkboxInput (  \"exprs_show_smooth\" ,  \"show smoothed fit?\" , value =  FALSE ) ,   checkboxInput (  \"exprs_log2\" ,  \"transform expression values to log2 scale?\" , value =  FALSE ) ) ) ) ) ) ) ,   server -  function ( input , output , session )  {    output $ plot -   renderPlot (  {   plot (  sce_set , exprs_values =   input $ exprs_values , block1 =   input $ block1 , block2 =   input $ block2 , colour_by =   input $ colour_by ) } )    output $ plotQC -   renderPlot (  {   plotQC (  sce_set , type =   input $ QCtype ) } )    output $ plotQCfindpc -   renderPlot (  {   plotQC (  sce_set , type =  \"find-pcs\" , variable =   input $ QCvar ) } )    output $ plotPCA_QC -   renderPlot (  {    plotPCA (  sce_set , ncomponents =   input $ pcaqc_ncomponents , pca_data_input =  \"pdata\" , selected_variables =   input $ pcaqc_selected_vars , detect_outliers =   input $ pcaqc_detect_outliers , colour_by =   input $ pcaqc_colour_by , size_by =   input $ pcaqc_size_by , shape_by =   input $ pcaqc_shape_by , scale_features =   input $ pcaqc_scale_features , legend =  \"all\" ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotPCA -   renderPlot (  {    plotPCA (  sce_set , ntop =   input $ pca_ntop , ncomponents =   input $ pca_ncomponents , exprs_values =   input $ pca_exprs_values , colour_by =   input $ pca_colour_by , size_by =   input $ pca_size_by , shape_by =   input $ pca_shape_by , scale_features =   input $ pca_scale_features ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotTSNE -   renderPlot (  {    plotTSNE (  sce_set , ntop =   input $ tsne_ntop , ncomponents =   input $ tsne_ncomponents , exprs_values =   input $ tsne_exprs_values , colour_by =   input $ tsne_colour_by , size_by =   input $ tsne_size_by , shape_by =   input $ tsne_shape_by , scale_features =   input $ tsne_scale_features , rand_seed =   input $ tsne_rand_seed ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotDiffusionMap -   renderPlot (  {    plotDiffusionMap (  sce_set , ntop =   input $ diffmap_ntop , ncomponents =   input $ diffmap_ncomponents , exprs_values =   input $ diffmap_exprs_values , colour_by =   input $ diffmap_colour_by , size_by =   input $ diffmap_size_by , shape_by =   input $ diffmap_shape_by , scale_features =   input $ diffmap_scale_features , rand_seed =   input $ diffmap_rand_seed , distance =   input $ diffmap_distance ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotExpression -   renderPlot (  {    plotExpression (  sce_set , features =   input $ exprs_features , x =   input $ exprs_x , exprs_values =   input $ exprs_exprs_values , colour_by =   input $ exprs_colour_by , size_by =   input $ exprs_size_by , shape_by =   input $ exprs_shape_by , ncol =   input $ exprs_ncols , show_median =   input $ exprs_show_median , show_violin =   input $ exprs_show_violin , show_smooth =   input $ exprs_show_smooth , log2_values =   input $ exprs_log2 ) +   theme ( legend.position =  \"bottom\" ) } )    session $ onSessionEnded (  function ( )  {   stopApp ( ) } ) } , options =   list ( launch.browser =  TRUE ) ) } ",
    "filename": "gui.txt"
  },
  "new_function": {
    "name": "scater_gui",
    "representation": "scater_gui",
    "parameters": "function ( object )",
    "body": "{   pd -   colnames (   colData (  object ) )   pd.plot -   pd [   !   grepl (  \"filter_\" ,  pd ) undefined  !   grepl (  \"is_\" ,  pd ) ]   featurenames -   rownames (  object )   exprs_values -   assayNames (  object )   exprs_values -   exprs_values [  !   grepl (  \"is_exprs\" ,  exprs_values ) ]   shinyApp (   ui -   dashboardPage (   dashboardHeader ( title =  \"scater\" ) ,   dashboardSidebar (   sidebarMenu (   menuItem (  \"plot\" , tabName =  \"plot\" ) ,   menuItem (  \"plotQC\" , tabName =  \"plotQC\" ) ,   menuItem (  \"plotPCA - QC metrics\" , tabName =  \"plotPCA_QC\" ) ,   menuItem (  \"plotPCA - expression\" , tabName =  \"plotPCA\" ) ,   menuItem (  \"plotTSNE\" , tabName =  \"plotTSNE\" ) ,   menuItem (  \"plotDiffusionMap\" , tabName =  \"plotDiffusionMap\" ) ,   menuItem (  \"plotExpression\" , tabName =  \"plotExpression\" ) ) ) ,   dashboardBody (   tabItems (   tabItem ( tabName =  \"plot\" ,   fluidRow (   box (   HTML (  [1311 chars quoted with '\"'] ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plot\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   selectInput (  \"block1\" ,  \"block1:\" ,  pd.plot , selected =   pd.plot [  2 ] ) ,   selectInput (  \"block2\" ,  \"block2:\" ,  pd.plot , selected =   pd.plot [  3 ] ) ,   selectInput (  \"colour_by\" ,  \"colour_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"logcounts\" ) ) ) ) ,   tabItem ( tabName =  \"plotQC\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   box (   plotOutput (  \"plotQC\" , height =  600 ) , width =  8 ) ,   box (   radioButtons (  \"QCtype\" , label =  \"Choose a type of QC plot\" , choices =   c (  \"highest-expression\" ,  \"explanatory-variables\" ,  \"exprs-freq-vs-mean\" ) , selected =  \"highest-expression\" ) , width =  4 ) ) ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ,   box (   plotOutput (  \"plotQCfindpc\" , height =  600 ) , width =  8 ) ,   box (   radioButtons (  \"QCvar\" , label =  \"Choose a variable of interest\" , choices =  pd.plot , selected =  \"total_features\" ) , width =  4 ) ) ) ,   tabItem ( tabName =  \"plotPCA_QC\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotPCA_QC\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   checkboxInput (  \"pcaqc_detect_outliers\" ,  \"detect outliers?\" , value =  TRUE ) ,   selectInput (  \"pcaqc_selected_vars\" ,  \"variables to use for PCA:\" ,  pd.plot , selected =   c (  \"pct_counts_top_100_features\" ,  \"total_features\" ,  \"pct_counts_feature_controls\" ,  \"n_detected_feature_controls\" ,  \"log10_counts_endogenous_features\" ,  \"log10_counts_feature_controls\" ) , multiple =  TRUE ) ,   textInput (  \"pcaqc_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"pcaqc_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"pcaqc_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   numericInput (  \"pcaqc_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"pcaqc_scale_features\" ,  \"scale_features\" , value =  TRUE ) ) ) ) ,   tabItem ( tabName =  \"plotPCA\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotPCA\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 , # selectInput(\"pca_colour_by\", \"colour_by (either cell metadata or feature expression):\", #             pd.plot, #             selected = pd.plot[4]),   textInput (  \"pca_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"pca_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"pca_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"pca_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"logcounts\" ) ,   numericInput (  \"pca_ntop\" ,  \"number of most variable features to use:\" ,  500 , min =  50 , max =  10000 , step =  25 ) ,   numericInput (  \"pca_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"pca_scale_features\" ,  \"scale_features\" , value =  TRUE ) ) ) ) ,   tabItem ( tabName =  \"plotTSNE\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotTSNE\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   textInput (  \"tsne_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"tsne_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"tsne_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"tsne_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"logcounts\" ) ,   numericInput (  \"tsne_ntop\" ,  \"number of most variable features to use:\" ,  500 , min =  50 , max =  10000 , step =  25 ) ,   numericInput (  \"tsne_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"tsne_scale_features\" ,  \"scale_features\" , value =  TRUE ) ,   numericInput (  \"tsne_rand_seed\" ,  \"random seed to make plot reproducible:\" ,  5000 ) ) ) ) ,   tabItem ( tabName =  \"plotDiffusionMap\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotDiffusionMap\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   textInput (  \"diffmap_colour_by\" ,  \"colour_by (either cell metadata or feature expression):\" ,   pd.plot [  3 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"diffmap_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"diffmap_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"diffmap_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"logcounts\" ) ,   numericInput (  \"diffmap_ntop\" ,  \"number of most variable features to use:\" ,  500 , min =  50 , max =  10000 , step =  25 ) ,   numericInput (  \"diffmap_ncomponents\" ,  \"number of components:\" ,  2 , min =  2 , max =  15 ) ,   checkboxInput (  \"diffmap_scale_features\" ,  \"scale_features\" , value =  TRUE ) ,   numericInput (  \"diffmap_rand_seed\" ,  \"random seed to make plot reproducible:\" ,  5000 ) ,   radioButtons (  \"diffmap_distance\" , label =  \"Choose a distance metric\" , choices =   c (  \"euclidean\" ,  \"cosine\" ,  \"rankcor\" ) , selected =  \"euclidean\" ) ) ) ) ,   tabItem ( tabName =  \"plotExpression\" ,   fluidRow (   box (   HTML (  \" ) , width =  12 , status =  \"success\" ) ) ,   fluidRow (   column ( width =  8 ,   box (   plotOutput (  \"plotExpression\" , height =  700 ) , width =  NULL ) ) ,   column ( width =  4 ,   selectInput (  \"exprs_features\" ,  \"features:\" ,  featurenames , selected =   featurenames [   1 :  6 ] , multiple =  TRUE ) , # selectInput(\"exprs_x\", \"x-axis variable:\", #             pd.plot, #             selected = pd.plot[4]),   textInput (  \"exprs_x\" ,  \"x-axis variable (either cell metadata variable or feature name):\" ,   pd.plot [  4 ] , placeholder =  \"Gene_0082\" ) ,   selectInput (  \"exprs_colour_by\" ,  \"colour_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"exprs_shape_by\" ,  \"shape_by:\" ,  pd.plot , selected =   pd.plot [  4 ] ) ,   selectInput (  \"exprs_size_by\" ,  \"size_by:\" ,  pd.plot , selected =   pd.plot [  7 ] ) ,   selectInput (  \"exprs_exprs_values\" ,  \"exprs_values:\" ,  exprs_values , selected =  \"logcounts\" ) ,   numericInput (  \"exprs_ncols\" ,  \"number of columns:\" ,  2 , min =  1 , max =  8 ) ,   checkboxInput (  \"exprs_show_median\" ,  \"show median?\" , value =  FALSE ) ,   checkboxInput (  \"exprs_show_violin\" ,  \"show violin?\" , value =  TRUE ) ,   checkboxInput (  \"exprs_show_smooth\" ,  \"show smoothed fit?\" , value =  FALSE ) ,   checkboxInput (  \"exprs_log2\" ,  \"transform expression values to log2 scale?\" , value =  FALSE ) ) ) ) ) ) ) ,   server -  function ( input , output , session )  {    output $ plot -   renderPlot (  {   plotScater (  object , exprs_values =   input $ exprs_values , block1 =   input $ block1 , block2 =   input $ block2 , colour_by =   input $ colour_by ) } )    output $ plotQC -   renderPlot (  {   plotQC (  object , type =   input $ QCtype ) } )    output $ plotQCfindpc -   renderPlot (  {   plotQC (  object , type =  \"find-pcs\" , variable =   input $ QCvar ) } )    output $ plotPCA_QC -   renderPlot (  {    plotPCA (  object , ncomponents =   input $ pcaqc_ncomponents , pca_data_input =  \"pdata\" , selected_variables =   input $ pcaqc_selected_vars , detect_outliers =   input $ pcaqc_detect_outliers , colour_by =   input $ pcaqc_colour_by , size_by =   input $ pcaqc_size_by , shape_by =   input $ pcaqc_shape_by , scale_features =   input $ pcaqc_scale_features , legend =  \"all\" ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotPCA -   renderPlot (  {    plotPCA (  object , ntop =   input $ pca_ntop , ncomponents =   input $ pca_ncomponents , exprs_values =   input $ pca_exprs_values , colour_by =   input $ pca_colour_by , size_by =   input $ pca_size_by , shape_by =   input $ pca_shape_by , scale_features =   input $ pca_scale_features ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotTSNE -   renderPlot (  {    plotTSNE (  object , ntop =   input $ tsne_ntop , ncomponents =   input $ tsne_ncomponents , exprs_values =   input $ tsne_exprs_values , colour_by =   input $ tsne_colour_by , size_by =   input $ tsne_size_by , shape_by =   input $ tsne_shape_by , scale_features =   input $ tsne_scale_features , rand_seed =   input $ tsne_rand_seed ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotDiffusionMap -   renderPlot (  {    plotDiffusionMap (  object , ntop =   input $ diffmap_ntop , ncomponents =   input $ diffmap_ncomponents , exprs_values =   input $ diffmap_exprs_values , colour_by =   input $ diffmap_colour_by , size_by =   input $ diffmap_size_by , shape_by =   input $ diffmap_shape_by , scale_features =   input $ diffmap_scale_features , rand_seed =   input $ diffmap_rand_seed , distance =   input $ diffmap_distance ) +   theme ( legend.position =  \"bottom\" ) } )    output $ plotExpression -   renderPlot (  {    plotExpression (  object , features =   input $ exprs_features , x =   input $ exprs_x , exprs_values =   input $ exprs_exprs_values , colour_by =   input $ exprs_colour_by , size_by =   input $ exprs_size_by , shape_by =   input $ exprs_shape_by , ncol =   input $ exprs_ncols , show_median =   input $ exprs_show_median , show_violin =   input $ exprs_show_violin , show_smooth =   input $ exprs_show_smooth , log2_values =   input $ exprs_log2 ) +   theme ( legend.position =  \"bottom\" ) } )    session $ onSessionEnded (  function ( )  {   stopApp ( ) } ) } , options =   list ( launch.browser =  TRUE ) ) } ",
    "filename": "gui.txt"
  }
}

1.
{
  "old_function": {
    "name": "plotExpressionDefault",
    "representation": "plotExpressionDefault",
    "parameters": "function ( object , aesth , ncol = 2 , xlab = NULL , ylab = NULL , show_median = FALSE , show_violin = TRUE , show_smooth = FALSE , alpha = 0.6 , size = NULL , scales = \"fixed\" , one_facet = FALSE , se = TRUE , jitter = \"swarm\" )",
    "body": "{  if (  !  (   \"Feature\" %in%   names (  object ) ) )   stop (  \"object needs a column named 'Feature' to define the feature(s) by which to plot expression.\" ) ## use x as group for violin plot if discrete   group_by_x -  (   show_violin undefined  (   !   is.numeric (   object [[   as.character (   aesth $ x ) ] ] ) ||    nlevels (   as.factor (   object [[   as.character (   aesth $ x ) ] ] ) ) =  5 ) )  if (  group_by_x )    aesth $ group -   aesth $ x else    aesth $ group -  1 ## Define the plot  if (  one_facet )  {  if (   is.null (   aesth $ colour ) )    aesth $ colour -   as.symbol (  \"Feature\" )   plot_out -     ggplot (  object ,  aesth ) +   xlab (  xlab ) +   ylab (  ylab ) } else  {   plot_out -      ggplot (  object ,  aesth ) +   facet_wrap (  ~  Feature , ncol =  ncol , scales =  scales ) +   xlab (  xlab ) +   ylab (  ylab ) } ## if colour aesthetic is defined, then choose sensible colour palette  if (  !   is.null (   aesth $ colour ) )   plot_out -   .resolve_plot_colours (  plot_out ,   object [[   as.character (   aesth $ colour ) ] ] ,   as.character (   aesth $ colour ) ) ## if x axis variable is not numeric, then jitter points horizontally  if (   is.numeric (   aesth $ x ) )  {  if (    is.null (   aesth $ size ) undefined  !   is.null (  size ) )   plot_out -   plot_out +   geom_point ( size =  size , alpha =  alpha ) else   plot_out -   plot_out +   geom_point ( alpha =  alpha ) } else  {  if (    is.null (   aesth $ size ) undefined  !   is.null (  size ) )  {  if (   jitter ==  \"swarm\" )   plot_out -   plot_out +   ggbeeswarm :: geom_quasirandom ( alpha =  alpha , size =  size ) else   plot_out -   plot_out +   geom_jitter ( alpha =  alpha , size =  size , position =   position_jitter ( height =  0 ) ) } else  {  if (   jitter ==  \"swarm\" )   plot_out -   plot_out +   ggbeeswarm :: geom_quasirandom ( alpha =  alpha ) else   plot_out -   plot_out +   geom_jitter ( alpha =  alpha , position =   position_jitter ( height =  0 ) ) } } ## show optional decorations on plot if desired  if (  show_violin )  {  if (   one_facet undefined  (    aesth $ colour ==   as.symbol (  \"Feature\" ) ) )  {   plot_out -   plot_out +   geom_violin (   aes_string ( fill =  \"Feature\" ) , colour =  \"gray60\" , alpha =  0.2 , scale =  \"width\" )   plot_out -   .resolve_plot_colours (  plot_out ,   object [[   as.character (   aesth $ colour ) ] ] ,   as.character (   aesth $ colour ) , fill =  TRUE ) } else   plot_out -   plot_out +   geom_violin ( colour =  \"gray60\" , alpha =  0.3 , fill =  \"gray80\" , scale =  \"width\" ) }  if (  show_median )  {   plot_out -   plot_out +   stat_summary ( fun.y =  median , fun.ymin =  median , fun.ymax =  median , geom =  \"crossbar\" , width =  0.3 , alpha =  0.8 ) }  if (  show_smooth )  {   plot_out -   plot_out +   stat_smooth ( colour =  \"firebrick\" , linetype =  2 , se =  se ) }  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotExpressionDefault",
    "representation": "plotExpressionDefault",
    "parameters": "function ( object , aesth , ncol = 2 , xlab = NULL , ylab = NULL , show_median = FALSE , show_violin = TRUE , show_smooth = FALSE , theme_size = 10 , alpha = 0.6 , size = NULL , scales = \"fixed\" , one_facet = FALSE , se = TRUE , jitter = \"swarm\" )",
    "body": "{  if (  !  (   \"Feature\" %in%   names (  object ) ) )   stop (  \"object needs a column named 'Feature' to define the feature(s) by which to plot expression.\" ) ## use x as group for violin plot if discrete   group_by_x -  (   show_violin undefined  (   !   is.numeric (   object [[   as.character (   aesth $ x ) ] ] ) ||    nlevels (   as.factor (   object [[   as.character (   aesth $ x ) ] ] ) ) =  5 ) )  if (  group_by_x )    aesth $ group -   aesth $ x else    aesth $ group -  1 ## Define the plot  if (  one_facet )  {  if (   is.null (   aesth $ colour ) )    aesth $ colour -   as.symbol (  \"Feature\" )   plot_out -     ggplot (  object ,  aesth ) +   xlab (  xlab ) +   ylab (  ylab ) } else  {   plot_out -      ggplot (  object ,  aesth ) +   facet_wrap (  ~  Feature , ncol =  ncol , scales =  scales ) +   xlab (  xlab ) +   ylab (  ylab ) } ## if colour aesthetic is defined, then choose sensible colour palette  if (  !   is.null (   aesth $ colour ) )   plot_out -   .resolve_plot_colours (  plot_out ,   object [[   as.character (   aesth $ colour ) ] ] ,   as.character (   aesth $ colour ) ) ## if x axis variable is not numeric, then jitter points horizontally  if (   is.numeric (   aesth $ x ) )  {  if (    is.null (   aesth $ size ) undefined  !   is.null (  size ) )   plot_out -   plot_out +   geom_point ( size =  size , alpha =  alpha ) else   plot_out -   plot_out +   geom_point ( alpha =  alpha ) } else  {  if (    is.null (   aesth $ size ) undefined  !   is.null (  size ) )  {  if (   jitter ==  \"swarm\" )   plot_out -   plot_out +   ggbeeswarm :: geom_quasirandom ( alpha =  alpha , size =  size , groupOnX =  TRUE ) else   plot_out -   plot_out +   geom_jitter ( alpha =  alpha , size =  size , position =   position_jitter ( height =  0 ) ) } else  {  if (   jitter ==  \"swarm\" )   plot_out -   plot_out +   ggbeeswarm :: geom_quasirandom ( alpha =  alpha , groupOnX =  TRUE ) else   plot_out -   plot_out +   geom_jitter ( alpha =  alpha , position =   position_jitter ( height =  0 ) ) } } ## show optional decorations on plot if desired  if (  show_violin )  {  if (   one_facet undefined  (    aesth $ colour ==   as.symbol (  \"Feature\" ) ) )  {   plot_out -   plot_out +   geom_violin (   aes_string ( fill =  \"Feature\" ) , colour =  \"gray60\" , alpha =  0.2 , scale =  \"width\" )   plot_out -   .resolve_plot_colours (  plot_out ,   object [[   as.character (   aesth $ colour ) ] ] ,   as.character (   aesth $ colour ) , fill =  TRUE ) } else   plot_out -   plot_out +   geom_violin ( colour =  \"gray60\" , alpha =  0.3 , fill =  \"gray80\" , scale =  \"width\" ) }  if (  show_median )  {   plot_out -   plot_out +   stat_summary ( fun.y =  median , fun.ymin =  median , fun.ymax =  median , geom =  \"crossbar\" , width =  0.3 , alpha =  0.8 ) }  if (  show_smooth )  {   plot_out -   plot_out +   stat_smooth ( colour =  \"firebrick\" , linetype =  2 , se =  se ) } ## Define plotting theme  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size )  plot_out } ",
    "filename": "plotting.txt"
  }
}

2.
{
  "old_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , feature_controls = NULL , cell_controls = NULL , nmads = 5 , pct_feature_controls_threshold = 80 )",
    "body": "{ ## We must have an SCESet object  if (  !   is (  object ,  \"SCESet\" ) )   stop (  \"object must be an SCESet object.\" ) ## the object must have some samples  if (    ncol (  object ) undefined  1 )   stop (  \"object must have at least one sample (column)\" )  if (    nrow (  object ) undefined  1 )   stop (  \"object must have at least one feature (row)\" ) ## See what versions of the expression data are available in the object   exprs_mat -   exprs (  object )   counts_mat -   counts (  object )   tpm_mat -   tpm (  object )   fpkm_mat -   fpkm (  object ) ## get number of sets of feature controls, and name them  if (   is.null (  feature_controls ) )  {   feature_controls -   list ( ) } else  if (  !   is.list (  feature_controls ) )  {   feature_controls -   list (  feature_controls ) }   n_sets_feature_controls -   length (  feature_controls )   counter -  1L  for  ( i in   seq_len (  n_sets_feature_controls ) )  {   curname -    names (  feature_controls ) [  i ]  if (    is.null (  curname ) ||   curname ==  \"\" )  {     names (  feature_controls ) [  i ] -   paste0 (  \"unnamed\" ,  counter )   counter -   counter +  1L } }    object @ featureControlInfo -   AnnotatedDataFrame (   data.frame ( name =   names (  feature_controls ) , stringsAsFactors =  FALSE ) )  if (  n_sets_feature_controls )  { ## Contributions from technical control features   tech_features -   .process_feature_controls (  object ,  feature_controls ,  pct_feature_controls_threshold ,  exprs_mat ,  counts_mat ,  tpm_mat ,  fpkm_mat )   feature_controls_pdata -   tech_features $ pData   feature_controls_fdata -   tech_features $ fData ## Combine all feature controls   is_feature_control -   apply (  feature_controls_fdata ,  1 ,  any )   feature_controls_fdata -   cbind (  feature_controls_fdata ,  is_feature_control ) } else  {   is_feature_control -   logical (   nrow (  object ) )   feature_controls_fdata -   data.frame (  is_feature_control )   feature_controls_pdata -   data.frame (   matrix (  0 , nrow =   ncol (  object ) , ncol =  0 ) ) }   n_detected_feature_controls -   nexprs (  object , subset_row =  is_feature_control )   df_pdata_this -   data.frame (  n_detected_feature_controls ) ## Compute metrics using all feature controls   okay.expr.vals -   c (  \"counts\" ,  \"cpm\" ,  \"tpm\" ,  \"fpkm\" )  for  ( ex in  okay.expr.vals )  {   cur_mat -   switch (  ex , counts =  counts_mat , tpm =  tpm_mat , fpkm =  fpkm_mat )  if (   is.null (  cur_mat ) )  {  next }   df_pdata_current -   .get_qc_metrics_exprs_mat (  cur_mat ,  is_feature_control ,  pct_feature_controls_threshold , calc_top_features =  TRUE , exprs_type =  ex , compute_endog =  TRUE )   df_pdata_this -   cbind (  df_pdata_this ,  df_pdata_current ) }   feature_controls_pdata -   cbind (  feature_controls_pdata ,  df_pdata_this ) ## Compute total_features and find outliers   total_features -   nexprs (  object , subset_row =  !  is_feature_control )   filter_on_total_features -   isOutlier (  total_features ,  nmads , type =  \"lower\" ) ## Compute total_counts if counts are present  if (  !   is.null (  counts_mat ) )  {   total_counts -   colSums (  counts_mat )   filter_on_total_counts -   isOutlier (  total_counts ,  nmads , log =  TRUE ) } else  {   total_counts -   colSums (  exprs_mat )   filter_on_total_counts -   isOutlier (  total_counts ,  nmads , log =  FALSE ) } ## Define counts from endogenous features   qc_pdata -  feature_controls_pdata  for  ( ex in  okay.expr.vals )  {   cur_mat -   switch (  ex , counts =  counts_mat , tpm =  tpm_mat , fpkm =  fpkm_mat )  if (   is.null (  cur_mat ) )  {  next }   cur_totals -   switch (  ex , counts =  total_counts ,   colSums (  cur_mat ) )    qc_pdata [[   paste0 (  ex ,  \"_endogenous_features\" ) ] ] -   cur_totals -   feature_controls_pdata [[   paste0 (  ex ,  \"_feature_controls\" ) ] ] } ## Define log10 read counts from feature controls   stat.cols -   sub (  \"_.*\" ,  \"\" ,   colnames (  qc_pdata ) )   cols_to_log -   which (   stat.cols %in%  okay.expr.vals )  if (   length (  cols_to_log ) )  {   log10_cols -   log10 (    qc_pdata [ ,  cols_to_log , drop =  FALSE ] +  1 )    colnames (  log10_cols ) -   paste0 (  \"log10_\" ,    colnames (  qc_pdata ) [  cols_to_log ] ) ## Combine into a big pdata object   qc_pdata -   cbind (  qc_pdata ,  log10_cols ) } ## Define cell controls ### Determine if vector or list  if (    is.null (  cell_controls ) |    length (  cell_controls ) ==  0 )  {   is_cell_control -   rep (  FALSE ,   ncol (  object ) )   cell_controls_pdata -   data.frame (  is_cell_control )   n_sets_cell_controls -  1 } else  {  if (   is.list (  cell_controls ) )  {   cell_controls_list -  cell_controls   n_sets_cell_controls -   length (  cell_controls ) } else  {   cell_controls_list -   list (  cell_controls )   n_sets_cell_controls -  1 }  for  ( i in   seq_len (  n_sets_cell_controls ) )  {   cc_set -   cell_controls_list [[  i ] ]   set_name -    names (  cell_controls_list ) [  i ]  if (   is.logical (  cc_set ) )  {   is_cell_control -  cc_set   cc_set -   which (  cc_set ) } else  {   is_cell_control -   rep (  FALSE ,   ncol (  object ) ) }  if (   is.character (  cc_set ) )   cc_set -   which (    cellNames (  object ) %in%  cc_set )    is_cell_control [  cc_set ] -  TRUE ## Construct data.frame for pData from this feature control set   is_cell_control -   as.data.frame (  is_cell_control )    colnames (  is_cell_control ) -   paste0 (  \"is_cell_control_\" ,  set_name )  if (   i undefined  1L )  {   cell_controls_pdata -   data.frame (  cell_controls_pdata ,  is_cell_control ) } else   cell_controls_pdata -  is_cell_control } } ## Check column names and get cell controls across all sets  if (   n_sets_cell_controls ==  1 )  {    colnames (  cell_controls_pdata ) -  \"is_cell_control\" } else  { ## Combine all cell controls   is_cell_control -   apply (  cell_controls_pdata ,  1 ,  any )   cell_controls_pdata -   cbind (  cell_controls_pdata ,  is_cell_control ) } ## Add cell-level QC metrics to pData   new_pdata -   as.data.frame (   pData (  object ) ) ### Remove columns to be replaced   to_replace -    colnames (  new_pdata ) %in%   c (   colnames (  qc_pdata ) ,   colnames (  cell_controls_pdata ) )   new_pdata -   new_pdata [ ,  !  to_replace , drop =  FALSE ] ### Add new QC metrics  if (  !   is.null (  counts_mat ) )  {    new_pdata $ total_counts -  total_counts    new_pdata $ log10_total_counts -   log10 (  total_counts )    new_pdata $ filter_on_total_counts -  filter_on_total_counts }    new_pdata $ total_features -  total_features    new_pdata $ log10_total_features -   log10 (  total_features )    new_pdata $ filter_on_total_features -  filter_on_total_features    new_pdata $ pct_dropout -   100 *  (   1 -    nexprs (  object , subset_row =  NULL ) /   nrow (  object ) )   new_pdata -   cbind (  new_pdata ,  qc_pdata ,  cell_controls_pdata )    pData (  object ) -   new (  \"AnnotatedDataFrame\" ,  new_pdata ) ## Add feature-level QC metrics to fData   new_fdata -   as.data.frame (   fData (  object ) ) ### Remove columns that are to be replaced   to_replace -    colnames (  new_fdata ) %in%   colnames (  feature_controls_fdata )   new_fdata -   new_fdata [ ,  !  to_replace , drop =  FALSE ] ### Add new QC information    new_fdata $ mean_exprs -   rowMeans (   exprs (  object ) )    new_fdata $ exprs_rank -   rank (   rowMeans (   exprs (  object ) ) )    new_fdata $ n_cells_exprs -   nexprs (  object , byrow =  TRUE )   total_exprs -   sum (  exprs_mat )    new_fdata $ total_feature_exprs -   rowSums (  exprs_mat )    new_fdata $ pct_total_exprs -    100 *   rowSums (  exprs_mat ) /  total_exprs    new_fdata $ pct_dropout -   100 *  (   1 -    new_fdata $ n_cells_exprs /   ncol (  object ) )  for  ( ex in  okay.expr.vals )  {   cur_mat -   switch (  ex , counts =  counts_mat , tpm =  tpm_mat , fpkm =  fpkm_mat )  if (   is.null (  cur_mat ) )  {  next }   cur_totals -   sum (   as.double (   colSums (  cur_mat ) ) ) # avoid integer overflow   cur_feature_totals -   rowSums (  cur_mat )    new_fdata [[   paste0 (  \"total_feature_\" ,  ex ) ] ] -  cur_feature_totals    new_fdata [[   paste0 (  \"log10_total_feature_\" ,  ex ) ] ] -   log10 (   cur_feature_totals +  1 )    new_fdata [[   paste0 (  \"pct_total_\" ,  ex ) ] ] -    100 *  cur_feature_totals /  cur_totals } ## Add new fdata to object   new_fdata -   cbind (  new_fdata ,  feature_controls_fdata )    fData (  object ) -   new (  \"AnnotatedDataFrame\" ,  new_fdata ) ## Ensure sample names are correct and return object    sampleNames (  object ) -   colnames (   exprs (  object ) )  object } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , exprs_values = \"counts\" , feature_controls = NULL , cell_controls = NULL , nmads = 5 , pct_feature_controls_threshold = 80 )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"object must be a SingleCellExperiment\" )   exprs_mat -   assay (  object , i =  exprs_values )  if (    exprs_values ==  \"counts\" ||   exprs_values ==  \"cpm\" )  {   linear -  TRUE } else  {   linear -  FALSE } ##Adding general metrics for each cell.   cd -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  NULL , subset_type =  NULL , linear =  TRUE )   rd -   DataFrame ( is_feature_control =   logical (   nrow (  exprs_mat ) ) , row.names =   rownames (  exprs_mat ) ) ## Adding metrics for the technical controls.   n_feature_sets -   length (  feature_controls )  if (  n_feature_sets )  {  if (   is.null (   names (  feature_controls ) ) )  {   stop (  \"feature_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  feature_controls , FUN =  .subset2index , target =  exprs_mat )   is_fcon -   Reduce (  union ,  reindexed )     rd $ is_feature_control [  is_fcon ] -  TRUE # Adding feature controls.  for  ( f in   seq_len (  n_feature_sets ) )  {   cur.index -   logical (   nrow (  exprs_mat ) )    cur.index [   reindexed [[  f ] ] ] -  TRUE    rd [[   paste0 (  \"is_feature_control_\" ,    names (  reindexed ) [  f ] ) ] ] -  cur.index } # Running through all endogenous genes.   is_endog -   which (  !   rd $ is_feature_control )   cd_endog -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  is_endog , subset_type =  \"endogenous\" , linear =  linear ) # Running through all feature controls.   cd_fcon -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  is_fcon , subset_type =  \"feature_control\" , linear =  linear ) # Running through each of the feature controls.   cd_per_fcon -   vector (  \"list\" ,  n_feature_sets )  for  ( f in   seq_len (  n_feature_sets ) )  {    cd_per_fcon [[  f ] ] -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =   reindexed [[  f ] ] , subset_type =    names (  reindexed ) [  f ] , linear =  linear ) }   cd -   do.call (  cbind ,   c (   list (  cd ,  cd_endog ,  cd_fcon ) ,  cd_per_fcon ) ) } ## Define cell controls ### Determine if vector or list   rd_all -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  NULL , subset_type =  NULL , linear =  linear )   rd -   cbind (  rd ,  rd_all )    cd $ is_cell_control -   logical (   ncol (  exprs_mat ) )   n_cell_sets -   length (  cell_controls )  if (  n_cell_sets )  { # Converting indices to integer.   reindexed -   lapply (  cell_controls , FUN =  .subset2index , target =  exprs_mat , byrow =  FALSE )   is_ccon -   Reduce (  union ,  reindexed )     cd $ is_cell_control [  is_ccon ] -  TRUE # Adding sets to the colData.  for  ( cx in   seq_len (  n_cell_sets ) )  {   current_control -   logical (   ncol (  exprs_mat ) )    current_control [   reindexed [[  cx ] ] ] -  TRUE    cd [[   paste0 (  \"is_cell_control_\" ,    names (  reindexed ) [  cx ] ) ] ] -  current_control } # Adding statistics for non-control cells.   is_noncon -   which (  !   cd $ is_cell_control )   rd_noncon -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  is_noncon , subset_type =  \"non_control\" , linear =  linear ) # Adding statistics for all control cells.   rd_con -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  is_ccon , subset_type =  \"cell_control\" , linear =  linear ) # Adding statistics for each set of control cells.   rd_collected -   vector (  \"list\" ,  n_cell_sets )  for  ( cx in   seq_len (  n_cell_sets ) )  {   rd_current -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =   reindexed [[  cx ] ] , subset_type =    names (  reindexed ) [  cx ] , linear =  linear )    rd_collected [[  cx ] ] -  rd_current }   rd -   do.call (  cbind ,   c (   list (  rd ,  rd_con ,  rd_noncon ) ,  rd_collected ) ) } ### Remove columns to be replaced   old_rd -   rowData (  object )   old_rd -   old_rd [ ,  !  (    colnames (  old_rd ) %in%   colnames (  rd ) ) , drop =  FALSE ]   rd -   cbind (  old_rd ,  rd )    rowData (  object ) -  rd   old_cd -   colData (  object )   old_cd -   old_cd [ ,  !  (    colnames (  old_cd ) %in%   colnames (  cd ) ) , drop =  FALSE ]   cd -   cbind (  old_cd ,  cd )    colData (  object ) -  cd   return (  object ) } ",
    "filename": "qc.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_6 scater_release_3_7

{
    "package": "scater",
    "release_versions": "scater_release_3_6 scater_release_3_7",
    "desc_release_old": "1.6.3",
    "desc_release_new": "1.8.4",
    "old_release_number": 3,
    "new_release_number": 4,
    "function_removals": 7,
    "function_additions": 5,
    "parameter_removals": 0,
    "parameter_additions": 1,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 12,
    "total_count": 19
}

##########
Functions Removed
##########

exprs
fromCellDataSet
newSCESet
plotExpressionDefault
plotMetadata
plotReducedDimDefault
toCellDataSet


##########
Functions Added
##########

centreSizeFactors
librarySizeFactors
plotHeatmap
runMDS
uniquifyFeatureNames


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########

0.
{
  "old_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , col_by_variable = \"total_features\" , n = 50 , drop_features = NULL , exprs_values = \"counts\" , feature_names_to_plot = NULL )",
    "body": "{ ## Check that variable to colour points exists  if (  !  (   col_by_variable %in%   colnames (   colData (  object ) ) ) )  {   stop (  \"col_by_variable not found in colData(object).\r\n             Please make sure colData(object)[, variable] exists.\" )   plot_cols -  FALSE } else   plot_cols -  TRUE   x -    colData (  object ) [ ,  col_by_variable ] #     x_na #     x ## Determine type of variable   typeof_x -   .getTypeOfVariable (  object ,  col_by_variable ) ## Figure out which features to drop  if (  !  (    is.null (  drop_features ) |    length (  drop_features ) ==  0 ) )  {  if (   is.character (  drop_features ) )   drop_features -   which (    rownames (  object ) %in%  drop_features )  if (   is.logical (  drop_features ) )   object -   object [  !  drop_features , ] else   object -   object [  -  drop_features , ] } ## Compute QC metrics on the (possibly) modified SingleCellExperiment object to make sure ## we have the relevant values for this set of features  if (  !   is.null (    rowData (  object ) $ is_feature_control ) )   object -   calculateQCMetrics (  object , feature_controls =   list ( all =    rowData (  object ) $ is_feature_control ) ) else   object -   calculateQCMetrics (  object ) ## Define expression values to be used   exprs_values -   match.arg (  exprs_values ,   c (  \"logcounts\" ,  \"tpm\" ,  \"cpm\" ,  \"fpkm\" ,  \"counts\" ) )   exprs_mat -   assay (  object ,  exprs_values )  if (    is.null (  exprs_mat ) undefined  !   is.null (   counts (  object ) ) )  {   exprs_mat -   counts (  object )   message (  \"Using counts as expression values.\" )   exprs_values -  \"counts\" } else  if (   is.null (  exprs_mat ) )  {   exprs_mat -   exprs (  object )   message (  \"Using exprs(object) values as expression values.\" )   exprs_values -  \"logcounts\" }  if (   exprs_values ==  \"logcounts\" )   exprs_mat -    2 ^  exprs_mat -   object @ logExprsOffset ## Find the most highly expressed features in this dataset ### Order by total feature counts across whole dataset   rdata -   rowData (  object )  if (    paste0 (  \"rank_\" ,  exprs_values ) %in%   colnames (  rdata ) )   oo -   order (   rdata [[   paste0 (  \"rank_\" ,  exprs_values ) ] ] , decreasing =  TRUE ) else  {  if (   \"rank_counts\" %in%   colnames (  rdata ) )  {   oo -   order (   rdata [[  \"rank_counts\" ] ] , decreasing =  TRUE )   exprs_values -  \"counts\"   message (  \"Using counts to order total expression of features.\" ) } else  {   exprs_values -  \"logcounts\"   oo -   order (   rdata [[  \"rank_exprs\" ] ] , decreasing =  TRUE )   message (  \"Using 'exprs' to order total expression of features.\" ) } } ## define feature names for plot  if (    is.null (  feature_names_to_plot ) ||   is.null (    rowData (  object ) [[  feature_names_to_plot ] ] ) )    rdata $ feature -   factor (   rownames (  object ) , levels =    rownames (  object ) [   rev (  oo ) ] ) else    rdata $ feature -   factor (    rowData (  object ) [[  feature_names_to_plot ] ] , levels =     rowData (  object ) [[  feature_names_to_plot ] ] [   rev (  oo ) ] )    rdata $ Feature -   rdata $ feature ## Check if is_feature_control is defined  if (   is.null (   rdata $ is_feature_control ) )    rdata $ is_feature_control -   rep (  FALSE ,   nrow (  rdata ) ) ## Determine percentage expression accounted for by top features across all ## cells   total_exprs -   sum (  exprs_mat )   top50_pctage -    100 *   sum (    .general_rowSums (  exprs_mat ) [   oo [   1 :  n ] ] ) /  total_exprs ## Determine percentage of counts for top features by cell   df_pct_exprs_by_cell -  (    100 *   t (   exprs_mat [   oo [   1 :  n ] , ] ) /   .general_colSums (  exprs_mat ) )   pct_total -    100 *   .general_rowSums (  exprs_mat ) /  total_exprs    rdata [[  \"pct_total\" ] ] -  pct_total ## Melt dataframe so it is conducive to ggplot  if (   is.null (   rownames (  rdata ) ) )    rownames (  rdata ) -   as.character (   rdata $ feature )   df_pct_exprs_by_cell -   as.matrix (  df_pct_exprs_by_cell ) # coercing to a normal matrix.   df_pct_exprs_by_cell_long -   reshape2 :: melt (  df_pct_exprs_by_cell )    colnames (  df_pct_exprs_by_cell_long ) -   c (  \"Cell\" ,  \"Tags\" ,  \"value\" )    df_pct_exprs_by_cell_long $ Feature -   rdata [   as.character (   df_pct_exprs_by_cell_long $ Tags ) ,  \"feature\" ]    df_pct_exprs_by_cell_long $ Tags -   factor (   df_pct_exprs_by_cell_long $ Tags , levels =    rownames (  object ) [   rev (   oo [   1 :  n ] ) ] )    df_pct_exprs_by_cell_long $ Feature -   factor (   df_pct_exprs_by_cell_long $ Feature , levels =    rdata $ feature [   rev (   oo [   1 :  n ] ) ] ) ## Add colour variable information  if (   typeof_x ==  \"discrete\" )    df_pct_exprs_by_cell_long $ colour_by -   factor (  x ) else    df_pct_exprs_by_cell_long $ colour_by -  x ## Make plot   plot_most_expressed -         ggplot (  df_pct_exprs_by_cell_long ,   aes_string ( y =  \"Feature\" , x =  \"value\" , colour =  \"colour_by\" ) ) +   geom_point ( alpha =  0.6 , shape =  124 ) +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top50_pctage , digits =  3 ) ,  \"% of total\" ) ) +   ylab (  \"Feature\" ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (   typeof_x ==  \"discrete\" )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_pct_exprs_by_cell_long $ colour_by ,  col_by_variable ) #         plot_most_expressed #             ggthemes::scale_colour_tableau(name = col_by_variable) } else  {   plot_most_expressed -   plot_most_expressed +   scale_colour_gradient ( name =  col_by_variable , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) }     plot_most_expressed +   geom_point (   aes_string ( x =  \"as.numeric(pct_total)\" , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =   as.data.frame (   rdata [   oo [   1 :  n ] , ] ) , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , controls , colour_cells_by , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , by_show_single = TRUE , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{  if (   is.null (   rownames (  object ) ) )  {    rownames (  object ) -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) }  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   object -   object [  -  to_discard , ] } ## Define expression values to be used ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   .rowSums (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )   chosen -   head (  oo ,  n ) ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object ) } else  {   feature_names -    .choose_vis_values (  object ,  feature_names_to_plot , search =  \"metadata\" , mode =  \"row\" ) $ val }    rownames (  exprs_mat ) -  feature_names ## Compute expression values and reshape them for ggplot.   df_exprs_by_cell -   t (   exprs_mat [  chosen , ] )   df_exprs_by_cell -   as.matrix (  df_exprs_by_cell )  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top50_pctage -    100 *   sum (   ave_exprs [  chosen ] ) /  total_exprs   df_exprs_by_cell -    100 *  df_exprs_by_cell /   .colSums (  exprs_mat ) }   df_exprs_by_cell_long -   reshape2 :: melt (  df_exprs_by_cell )    colnames (  df_exprs_by_cell_long ) -   c (  \"Cell\" ,  \"Tag\" ,  \"value\" )    df_exprs_by_cell_long $ Tag -   factor (   df_exprs_by_cell_long $ Tag ,   rev (   feature_names [  chosen ] ) ) ## Colouring the individual dashes for the cells.  if (   missing (  colour_cells_by ) )  {   colour_cells_by -   .qc_hunter (  object ,   paste0 (  \"total_features_by_\" ,  exprs_values ) , mode =  \"column\" ) }  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   .choose_vis_values (  object ,  colour_cells_by , mode =  \"column\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )  if (  as_percentage )  {   plot_most_expressed -    plot_most_expressed +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top50_pctage , digits =  3 ) ,  \"% of total\" ) ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) } else  {   plot_most_expressed -   plot_most_expressed +   xlab (  exprs_values ) }   plot_most_expressed -     plot_most_expressed +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (  !   is.null (  colour_cells_by ) )  {  if (  !   is.numeric (   df_exprs_by_cell_long $ colour_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } else  {   plot_most_expressed -   plot_most_expressed +   scale_colour_gradient ( name =  colour_cells_by , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) } } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =   factor (  feature_names , levels =   rev (  feature_names ) ) )  if (  as_percentage )  {   pct_total -    100 *  ave_exprs /  total_exprs    df_to_plot $ pct_total -  pct_total   legend_val -  \"as.numeric(pct_total)\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  ave_exprs   legend_val -   sprintf (  \"as.numeric(ave_%s)\" ,  exprs_values ) } ## Check if is_feature_control is defined, and using it for colouring of the points.  if (   missing (  controls ) )  {   controls -   .qc_hunter (  object ,  \"is_feature_control\" , mode =  \"row\" ) }  if (  !   is.null (  controls ) )  {   cont_out -   .choose_vis_values (  object ,  controls , mode =  \"row\" , search =  \"metadata\" )    df_to_plot $ is_feature_control -   cont_out $ val   plot_most_expressed -     plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =   df_to_plot [  chosen , ] , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } else  {   plot_most_expressed -   plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" ) , data =   df_to_plot [  chosen , ] , fill =  \"grey80\" , colour =  \"grey30\" , shape =  21 ) }  plot_most_expressed } ",
    "filename": "plotHighestExprs.txt"
  }
}



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "calcIsExprs",
    "representation": "calcIsExprs",
    "parameters": "function ( object , lowerDetectionLimit = 0 , exprs_values = \"counts\" )",
    "body": "{    assay (  object , i =  exprs_values ) undefined  lowerDetectionLimit } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calcIsExprs",
    "representation": "calcIsExprs",
    "parameters": "function ( object , detection_limit = 0 , exprs_values = \"counts\" )",
    "body": "{    assay (  object , i =  exprs_values ) undefined  detection_limit } ",
    "filename": "calculate-expression.txt"
  }
}

1.
{
  "old_function": {
    "name": "nexprs",
    "representation": "nexprs",
    "parameters": "function ( object , lowerDetectionLimit = 0 , exprs_values = \"counts\" , byrow = FALSE , subset_row = NULL , subset_col = NULL )",
    "body": "{   exprs_mat -   assay (  object , i =  exprs_values )   subset_row -   .subset2index (  subset_row , target =  exprs_mat , byrow =  TRUE )   subset_col -   .subset2index (  subset_col , target =  exprs_mat , byrow =  FALSE )  if (  !  byrow )  {   margin.stats -   .Call (  cxx_margin_summary ,  exprs_mat ,  lowerDetectionLimit ,   subset_row -  1L ,  FALSE )   return (    margin.stats [[  2 ] ] [  subset_col ] ) } else  {   margin.stats -   .Call (  cxx_margin_summary ,  exprs_mat ,  lowerDetectionLimit ,   subset_col -  1L ,  TRUE )   return (    margin.stats [[  2 ] ] [  subset_row ] ) } } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "nexprs",
    "representation": "nexprs",
    "parameters": "function ( object , detection_limit = 0 , exprs_values = \"counts\" , byrow = FALSE , subset_row = NULL , subset_col = NULL )",
    "body": "{  if (   methods :: is (  object ,  \"SingleCellExperiment\" ) )  {   exprs_mat -   assay (  object , i =  exprs_values ) } else  {   exprs_mat -  object }   subset_row -   .subset2index (  subset_row , target =  exprs_mat , byrow =  TRUE )   subset_col -   .subset2index (  subset_col , target =  exprs_mat , byrow =  FALSE )  if (  !  byrow )  {   return (   .colAbove (  exprs_mat , rows =  subset_row , cols =  subset_col , value =  detection_limit ) ) } else  {   return (   .rowAbove (  exprs_mat , rows =  subset_row , cols =  subset_col , value =  detection_limit ) ) } } ",
    "filename": "calculate-expression.txt"
  }
}

2.
{
  "old_function": {
    "name": "calculateCPM",
    "representation": "calculateCPM",
    "parameters": "function ( object , use.size.factors = TRUE )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"object must be an SingleCellExperiment\" )   counts_mat -   counts (  object )   subset_row -   .subset2index (  NULL , target =  counts_mat , byrow =  TRUE )   margin.stats -   .Call (  cxx_margin_summary ,  counts_mat ,  0 ,   subset_row -  1L ,  FALSE )  if (  use.size.factors )  {   sf.list -   .get_all_sf_sets (  object )  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {   warning (  \"size factors requested but not specified, \r\n                    using library sizes instead\" )     sf.list $ size.factors [[  1 ] ] -   margin.stats [[  1 ] ] } } else  {   sf.list -   list ( size.factors =   list (   margin.stats [[  1 ] ] ) , index =   rep (  1 ,   nrow (  object ) ) ) } # Scaling the size factors to the library size.   cpm_mat -  counts_mat   mean.lib.size -   mean (   margin.stats [[  1 ] ] )   by.type -   split (   seq_along (   sf.list $ index ) ,   sf.list $ index )  for  ( g in   seq_along (  by.type ) )  {   chosen -   by.type [[  g ] ]   sf -    sf.list $ size.factors [[  g ] ]   scaled.sf -    sf /   mean (  sf ) *  mean.lib.size    cpm_mat [  chosen , ] -   .compute_exprs (   counts_mat [  chosen , , drop =  FALSE ] ,  sf , sf_to_use =  NULL , log =  FALSE , sum =  FALSE , subset_row =  NULL , logExprsOffset =  0 ) } # Restoring attributes.    rownames (  cpm_mat ) -   rownames (  object )    colnames (  cpm_mat ) -   colnames (  object )   return (  cpm_mat ) } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calculateCPM",
    "representation": "calculateCPM",
    "parameters": "function ( object , exprs_values = \"counts\" , use_size_factors = TRUE , size_factor_grouping = NULL , subset_row = NULL )",
    "body": "{  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   assays -   list (  object )    names (  assays ) -  exprs_values   object -   SingleCellExperiment (  assays ) } # Setting up the size factors.   object -   .replace_size_factors (  object ,  use_size_factors )  if (   is.null (   sizeFactors (  object ) ) )  {    sizeFactors (  object ) -   librarySizeFactors (  object ) }   object -   centreSizeFactors (  object , grouping =  size_factor_grouping )   sf_list -   .get_all_sf_sets (  object ) # Computes the average count, adjusting for size factors or library size.   extracted -   assay (  object ,  exprs_values )   normed -   .compute_exprs (  extracted , size_factor_val =   sf_list $ size.factors , size_factor_idx =   sf_list $ index , log =  FALSE , sum =  FALSE , logExprsOffset =  0 , subset_row =  subset_row )   lib_sizes -   colSums2 (   DelayedArray (  extracted ) )   cpm_mat -   normed /  (    mean (  lib_sizes ) /  1e6 )   return (  cpm_mat ) } ",
    "filename": "calculateCPM.txt"
  }
}

3.
{
  "old_function": {
    "name": "calcAverage",
    "representation": "calcAverage",
    "parameters": "function ( object , size.factors = NULL )",
    "body": "{  if (   methods :: is (  object ,  \"SingleCellExperiment\" ) )  {   sf.list -   .get_all_sf_sets (  object )   mat -   counts (  object ) } else  { # Using the lone set of size factors, if provided.   sf.list -   list ( index =   rep (  1L ,   nrow (  object ) ) , size.factors =   list (  size.factors ) )   mat -  object }   subset_row -   .subset2index (  NULL , target =  mat , byrow =  TRUE )   margin.stats -   .Call (  cxx_margin_summary ,  mat ,  0 ,   subset_row -  1L ,  FALSE ) # Set size factors to library sizes if not available.  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {     sf.list $ size.factors [[  1 ] ] -   margin.stats [[  1 ] ] } # Computes the average count, adjusting for size factors or library size.   all.ave -   .compute_exprs (  mat ,   sf.list $ size.factors , sf_to_use =   sf.list $ index , log =  FALSE , sum =  TRUE , logExprsOffset =  0 , subset_row =  NULL )    names (  all.ave ) -   rownames (  mat )   return (   all.ave /   ncol (  mat ) ) } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calcAverage",
    "representation": "calcAverage",
    "parameters": "function ( object , exprs_values = \"counts\" , use_size_factors = TRUE , size_factor_grouping = NULL , subset_row = NULL )",
    "body": "{  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   assays -   list (  object )    names (  assays ) -  exprs_values   object -   SingleCellExperiment (  assays ) } # Setting up the size factors.   object -   .replace_size_factors (  object ,  use_size_factors )  if (   is.null (   sizeFactors (  object ) ) )  {    sizeFactors (  object ) -   librarySizeFactors (  object ) }   object -   centreSizeFactors (  object , grouping =  size_factor_grouping )   sf_list -   .get_all_sf_sets (  object ) # Computes the average count, adjusting for size factors or library size.   all.ave -   .compute_exprs (   assay (  object ,  exprs_values ) , size_factor_val =   sf_list $ size.factors , size_factor_idx =   sf_list $ index , log =  FALSE , sum =  TRUE , logExprsOffset =  0 , subset_row =  subset_row )   return (   all.ave /   ncol (  object ) ) } ",
    "filename": "calcAverage.txt"
  }
}

4.
{
  "old_function": {
    "name": "readKallistoResults",
    "representation": "readKallistoResults",
    "parameters": "function ( kallisto_log = NULL , samples = NULL , directories = NULL , read_h5 = FALSE , kallisto_version = \"current\" , logExprsOffset = 1 , verbose = TRUE )",
    "body": "{   kallisto_fail -   rep (  FALSE ,   length (  samples ) ) ## Checks on arguments  if (  !   is.null (  kallisto_log ) )  {   cat (  \"Using kallisto_log argument to define samples and results directories.\" )  if (  !   is.list (  kallisto_log ) )   stop (  \"The kallisto_log argument should be a list returned by runKallisto()\" )   samples -   names (  kallisto_log )   directories -   sapply (  kallisto_log ,  function ( x )  {   x $ output_dir } )   logs -   lapply (  kallisto_log ,  function ( x )  {   x $ kallisto_log } ) ## Can only check kallisto fail if log provided   kallisto_fail -   sapply (  logs ,  function ( x )  {   any (   grepl (  \"[wW]arning|[eE]rror\" ,  x ) ) } )  if (   any (  kallisto_fail ) )  {   warning (   paste0 (  \"The kallisto job failed for the following samples:\\n \" ,   paste0 (    names (  logs ) [  kallisto_fail ] , collapse =  \"\\n\" ) ,  \"\\n It is recommended that you inspect kallisto_log for these samples.\" ) ) } } else  {   cat (  \"Kallisto log not provided - assuming all runs successful\" )  if (    is.null (  samples ) |   is.null (  directories ) )   stop (  \"If kallisto_log argument is not used, then both samples and directories must be provided.\" )  if (    length (  samples ) !=   length (  directories ) )   stop (  \"samples and directories arguments must be the same length\" ) }   samples -   samples [  !  kallisto_fail ]   directories -   directories [  !  kallisto_fail ]  if (  !   all (   dir.exists (  directories ) ) )   stop (  \"Some of the desired directories to import do not exist!\" ) ## Read first file to get size of feature set   s1 -   readKallistoResultsOneSample (   directories [  1 ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version )   nsamples -   length (  samples )   nfeatures -   nrow (   s1 $ abundance )   nbootstraps -    s1 $ run_info $ n_bootstraps   navec_samples -   rep (  NA ,  nsamples ) ## Set up results objects   pdata -   data.frame ( n_targets =  navec_samples , n_bootstraps =  navec_samples , kallisto_version =  navec_samples , index_version =  navec_samples , start_time =  navec_samples , call =  navec_samples )    rownames (  pdata ) -  samples   fdata -   data.frame ( feature_id =    s1 $ abundance $ target_id , feature_length =    s1 $ abundance $ length , feature_eff_length =    s1 $ abundance $ eff_length )    rownames (  fdata ) -    s1 $ abundance $ target_id   est_counts -   tpm -   feat_eff_len -   matrix (  NA , nrow =  nfeatures , ncol =  nsamples )    colnames (  est_counts ) -    colnames (  tpm ) -    colnames (  feat_eff_len ) -  samples    rownames (  est_counts ) -    rownames (  tpm ) -    rownames (  feat_eff_len ) -    s1 $ abundance $ target_id  if (  read_h5 )  {   bootstraps -   array (  NA , dim =   c (  nfeatures ,  nsamples ,  nbootstraps ) )    rownames (  bootstraps ) -    s1 $ abundance $ target_id    colnames (  bootstraps ) -  samples } ## Read kallisto results into results objects  if (  verbose )   cat (   paste (  \"\\nReading results for\" ,  nsamples ,  \"samples:\\n\" ) )  for  ( i in   seq_len (  nsamples ) )  {   tmp_samp -   readKallistoResultsOneSample (   directories [  i ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version ) ## counts  if (    length (    tmp_samp $ abundance $ est_counts ) !=  nfeatures )   warning (   paste (  \"Results for directory\" ,   directories [  i ] ,  \"do not match dimensions of other samples.\" ) ) else    est_counts [ ,  i ] -    tmp_samp $ abundance $ est_counts ## tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm ## feature effective length  if (    length (    tmp_samp $ abundance $ eff_length ) ==  nfeatures )    feat_eff_len [ ,  i ] -    tmp_samp $ abundance $ eff_length ## run info     pdata $ n_targets [  i ] -    tmp_samp $ run_info $ n_targets     pdata $ n_processed [  i ] -    tmp_samp $ run_info $ n_processed     pdata $ n_bootstraps [  i ] -    tmp_samp $ run_info $ n_bootstraps     pdata $ kallisto_version [  i ] -    tmp_samp $ run_info $ kallisto_version     pdata $ index_version [  i ] -    tmp_samp $ run_info $ index_version     pdata $ start_time [  i ] -    tmp_samp $ run_info $ start_time     pdata $ call [  i ] -    tmp_samp $ run_info $ call ## bootstraps  if (  read_h5 )    bootstraps [ ,  i , ] -   as.matrix (    tmp_samp $ abundance [ ,  -   c (   1 :  5 ) ] )  if (  verbose )  {   cat (  \".\" )  if (    i %%  80 ==  0 )   cat (  \"\\n\" ) } } ## Add median feature effective length to fData    fdata $ median_effective_length -   matrixStats :: rowMedians (  feat_eff_len )  if (  verbose )   cat (  \"\\n\" ) ## Produce SingleCellExperiment object   sce_out -   SingleCellExperiment (   list ( exprs =   log2 (   tpm +  logExprsOffset ) , counts =  est_counts , tpm =  tpm , feature_effective_length =  feat_eff_len ) , colData =  pdata , rowData =  fdata )  if (  verbose )   cat (  \"Using log2(TPM + 1) as 'exprs' values in output.\" )  if (  read_h5 )    bootstraps (  sce_out ) -  bootstraps ## Return SCESet object  sce_out } ",
    "filename": "kallisto-wrapper.txt"
  },
  "new_function": {
    "name": "readKallistoResults",
    "representation": "readKallistoResults",
    "parameters": "function ( kallisto_log = NULL , samples = NULL , directories = NULL , read_h5 = FALSE , kallisto_version = \"current\" , verbose = TRUE )",
    "body": "{   kallisto_fail -   rep (  FALSE ,   length (  samples ) ) ## Checks on arguments  if (  !   is.null (  kallisto_log ) )  {   cat (  \"Using kallisto_log argument to define samples and results directories.\" )  if (  !   is.list (  kallisto_log ) )   stop (  \"The kallisto_log argument should be a list returned by runKallisto()\" )   samples -   names (  kallisto_log )   directories -   sapply (  kallisto_log ,  function ( x )  {   x $ output_dir } )   logs -   lapply (  kallisto_log ,  function ( x )  {   x $ kallisto_log } ) ## Can only check kallisto fail if log provided   kallisto_fail -   sapply (  logs ,  function ( x )  {   any (   grepl (  \"[wW]arning|[eE]rror\" ,  x ) ) } )  if (   any (  kallisto_fail ) )  {   warning (   paste0 (  \"The kallisto job failed for the following samples:\\n \" ,   paste0 (    names (  logs ) [  kallisto_fail ] , collapse =  \"\\n\" ) ,  \"\\n It is recommended that you inspect kallisto_log for these samples.\" ) ) } } else  {   cat (  \"Kallisto log not provided - assuming all runs successful\" )  if (    is.null (  samples ) |   is.null (  directories ) )   stop (  \"If kallisto_log argument is not used, then both samples and directories must be provided.\" )  if (    length (  samples ) !=   length (  directories ) )   stop (  \"samples and directories arguments must be the same length\" ) }   samples -   samples [  !  kallisto_fail ]   directories -   directories [  !  kallisto_fail ]  if (  !   all (   dir.exists (  directories ) ) )   stop (  \"Some of the desired directories to import do not exist!\" ) ## Read first file to get size of feature set   s1 -   readKallistoResultsOneSample (   directories [  1 ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version )   nsamples -   length (  samples )   nfeatures -   nrow (   s1 $ abundance )   nbootstraps -    s1 $ run_info $ n_bootstraps   navec_samples -   rep (  NA ,  nsamples ) ## Set up results objects   pdata -   data.frame ( n_targets =  navec_samples , n_bootstraps =  navec_samples , kallisto_version =  navec_samples , index_version =  navec_samples , start_time =  navec_samples , call =  navec_samples )    rownames (  pdata ) -  samples   fdata -   data.frame ( feature_id =    s1 $ abundance $ target_id , feature_length =    s1 $ abundance $ length , feature_eff_length =    s1 $ abundance $ eff_length )    rownames (  fdata ) -    s1 $ abundance $ target_id   est_counts -   tpm -   feat_eff_len -   matrix (  NA , nrow =  nfeatures , ncol =  nsamples )    colnames (  est_counts ) -    colnames (  tpm ) -    colnames (  feat_eff_len ) -  samples    rownames (  est_counts ) -    rownames (  tpm ) -    rownames (  feat_eff_len ) -    s1 $ abundance $ target_id  if (  read_h5 )  {   bootstraps -   array (  NA , dim =   c (  nfeatures ,  nsamples ,  nbootstraps ) )    rownames (  bootstraps ) -    s1 $ abundance $ target_id    colnames (  bootstraps ) -  samples } ## Read kallisto results into results objects  if (  verbose )   cat (   paste (  \"\\nReading results for\" ,  nsamples ,  \"samples:\\n\" ) )  for  ( i in   seq_len (  nsamples ) )  {   tmp_samp -   readKallistoResultsOneSample (   directories [  i ] , read_h5 =  read_h5 , kallisto_version =  kallisto_version ) ## counts  if (    length (    tmp_samp $ abundance $ est_counts ) !=  nfeatures )   warning (   paste (  \"Results for directory\" ,   directories [  i ] ,  \"do not match dimensions of other samples.\" ) ) else    est_counts [ ,  i ] -    tmp_samp $ abundance $ est_counts ## tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm  if (    length (    tmp_samp $ abundance $ est_counts ) ==  nfeatures )    tpm [ ,  i ] -    tmp_samp $ abundance $ tpm ## feature effective length  if (    length (    tmp_samp $ abundance $ eff_length ) ==  nfeatures )    feat_eff_len [ ,  i ] -    tmp_samp $ abundance $ eff_length ## run info     pdata $ n_targets [  i ] -    tmp_samp $ run_info $ n_targets     pdata $ n_processed [  i ] -    tmp_samp $ run_info $ n_processed     pdata $ n_bootstraps [  i ] -    tmp_samp $ run_info $ n_bootstraps     pdata $ kallisto_version [  i ] -    tmp_samp $ run_info $ kallisto_version     pdata $ index_version [  i ] -    tmp_samp $ run_info $ index_version     pdata $ start_time [  i ] -    tmp_samp $ run_info $ start_time     pdata $ call [  i ] -    tmp_samp $ run_info $ call ## bootstraps  if (  read_h5 )    bootstraps [ ,  i , ] -   as.matrix (    tmp_samp $ abundance [ ,  -   c (   1 :  5 ) ] )  if (  verbose )  {   cat (  \".\" )  if (    i %%  80 ==  0 )   cat (  \"\\n\" ) } } ## Add median feature effective length to fData    fdata $ median_effective_length -   DelayedMatrixStats :: rowMedians (   DelayedArray (  feat_eff_len ) )  if (  verbose )   cat (  \"\\n\" ) ## Produce SingleCellExperiment object   sce_out -   SingleCellExperiment (   list ( counts =  est_counts , tpm =  tpm , feature_effective_length =  feat_eff_len ) , colData =  pdata , rowData =  fdata )  if (  read_h5 )    bootstraps (  sce_out ) -  bootstraps ## Return SCESet object  sce_out } ",
    "filename": "kallisto-wrapper.txt"
  }
}

5.
{
  "old_function": {
    "name": "normalizeSCE",
    "representation": "normalizeSCE",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (   exprs_values ==  \"exprs\" )  {   exprs_values -  \"logcounts\" }   exprs_mat -   assay (  object , i =  exprs_values )  if (   exprs_values ==  \"counts\" )  {   sf.list -   .get_all_sf_sets (  object )  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {   warning (  \"using library sizes as size factors\" )     sf.list $ size.factors [[  1 ] ] -   .general_colSums (  exprs_mat ) } ## figuring out how many controls have their own size factors   spike.names -   spikeNames (  object )   no.spike.sf -  !   spike.names %in%   sf.list $ available  if (   any (  no.spike.sf ) )  {   warning (   sprintf (  \"spike-in transcripts in '%s' should have their own size factors\" ,    spike.names [  no.spike.sf ] [  1 ] ) ) } } else  { # ignoring size factors for non-count data.   sf.list -   list ( size.factors =   rep (  1 ,   ncol (  object ) ) , index =  NULL ) } ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (  exprs_mat ,   sf.list $ size.factors , sf_to_use =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {  if (  return_norm_as_exprs )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"norm_exprs\" ) -  norm_exprs } } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## centering all existing size factors if requested  if (    exprs_values ==  \"counts\" undefined  centre_size_factors )  {   sf -   sizeFactors (  object )  if (  !   is.null (  sf ) )  {   sf -   sf /   mean (  sf )    sizeFactors (  object ) -  sf } # ... and for all controls.  for  ( type in   sf.list $ available )  {   sf -   sizeFactors (  object , type =  type )   sf -   sf /   mean (  sf )    sizeFactors (  object , type =  type ) -  sf } } ## return object   return (  object ) } ",
    "filename": "normalisation.txt"
  },
  "new_function": {
    "name": "normalizeSCE",
    "representation": "normalizeSCE",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , size_factor_grouping = NULL )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) }  if (  centre_size_factors )  {   object -   centreSizeFactors (  object , grouping =  size_factor_grouping ) }   sf.list -   .get_all_sf_sets (  object ) ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (   assay (  object , i =  exprs_values ) , size_factor_val =   sf.list $ size.factors , size_factor_idx =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "filename": "normalizeSCE.txt"
  }
}

6.
{
  "old_function": {
    "name": "plotScater",
    "representation": "plotScater",
    "parameters": "function ( x , block1 = NULL , block2 = NULL , colour_by = NULL , nfeatures = 500 , exprs_values = \"counts\" , ncol = 3 , linewidth = 1.5 , theme_size = 10 )",
    "body": "{  if (  !   is (  x ,  \"SingleCellExperiment\" ) )   stop (  \"x must be of class SingleCellExperiment\" )  if (  !   is.null (  block1 ) )  {  if (  !  (   block1 %in%   colnames (   colData (  x ) ) ) )   stop (  \"The block1 argument must either be NULL or a column of colData(x).\" ) }  if (  !   is.null (  block2 ) )  {  if (  !  (   block2 %in%   colnames (   colData (  x ) ) ) )   stop (  \"The block2 argument must either be NULL or a column of colData(x).\" ) } ## Setting values to colour by.   colour_by_out -   .choose_vis_values (  x ,  colour_by )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## Define an expression matrix depending on which values we're using   exprs_mat -   assay (  x , i =  exprs_values ) ## Use plyr to get the sequencing real estate accounted for by features   nfeatures_total -   nrow (  exprs_mat )   seq_real_estate -   t (   plyr :: aaply (  exprs_mat ,  2 , .fun =  function ( x )  {   cumsum (   sort (  x , decreasing =  TRUE ) ) } ) )    rownames (  seq_real_estate ) -   seq_len (  nfeatures_total )   nfeatures_to_plot -  nfeatures   to_plot -   seq_len (  nfeatures_to_plot )   seq_real_estate_long -   reshape2 :: melt (   seq_real_estate [  to_plot , ] , value.name =  exprs_values ) ## Get the proportion of the library accounted for by the top features   prop_library -   reshape2 :: melt (   t (    t (   seq_real_estate [  to_plot , ] ) /   .general_colSums (  exprs_mat ) ) , value.name =  \"prop_library\" )    colnames (  seq_real_estate_long ) -   c (  \"Feature\" ,  \"Cell\" ,  exprs_values )    seq_real_estate_long $ Proportion_Library -   prop_library $ prop_library ## Add block and colour_by information if provided  if (  !   is.null (  block1 ) )   seq_real_estate_long -   dplyr :: mutate (  seq_real_estate_long , block1 =   as.factor (   rep (   x [[  block1 ] ] , each =  nfeatures_to_plot ) ) )  if (  !   is.null (  block2 ) )   seq_real_estate_long -   dplyr :: mutate (  seq_real_estate_long , block2 =   as.factor (   rep (   x [[  block2 ] ] , each =  nfeatures_to_plot ) ) )  if (  !   is.null (  colour_by ) )   seq_real_estate_long -   dplyr :: mutate (  seq_real_estate_long , colour_by =   rep (  colour_by_vals , each =  nfeatures_to_plot ) ) ## Set up plot  if (   is.null (  colour_by ) )  {   plot_out -    ggplot (  seq_real_estate_long ,   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" ) ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  linewidth ) } else  {   plot_out -    ggplot (  seq_real_estate_long ,   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" , colour =  \"colour_by\" ) ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  linewidth ) } ## Deal with blocks for grid  if (  !  (    is.null (  block1 ) |   is.null (  block2 ) ) )   plot_out -   plot_out +   facet_grid (   block2 ~  block1 ) else  {  if (   !   is.null (  block1 ) undefined   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block1 , ncol =  ncol ) }  if (    is.null (  block1 ) undefined  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block2 , ncol =  ncol ) } } ## Add extra plot theme and details  if (  !   is.null (   seq_real_estate_long $ colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   seq_real_estate_long $ colour_by ,  colour_by ) }   plot_out -    plot_out +   xlab (  \"Number of features\" ) +   ylab (  \"Cumulative proportion of library\" )  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) else   plot_out -   plot_out +   theme_bw (  theme_size ) ## Return plot  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotScater",
    "representation": "plotScater",
    "parameters": "function ( x , nfeatures = 500 , exprs_values = \"counts\" , colour_by = NULL , by_exprs_values = exprs_values , by_show_single = FALSE , block1 = NULL , block2 = NULL , ncol = 3 , line_width = 1.5 , theme_size = 10 )",
    "body": "{  if (  !   is (  x ,  \"SingleCellExperiment\" ) )  {   stop (  \"x must be of class SingleCellExperiment\" ) }   block1_out -   .choose_vis_values (  x ,  block1 , mode =  \"column\" , search =  \"metadata\" )   block1 -   block1_out $ name   block1_vals -   block1_out $ val   block2_out -   .choose_vis_values (  x ,  block2 , mode =  \"column\" , search =  \"metadata\" )   block2 -   block2_out $ name   block2_vals -   block2_out $ val ## Setting values to colour by.   colour_by_out -   .choose_vis_values (  x ,  colour_by , mode =  \"column\" , search =  \"any\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## Define an expression matrix depending on which values we're using   exprs_mat -   assay (  x , i =  exprs_values )   nfeatures -   min (  nfeatures ,   nrow (  exprs_mat ) ) ## Use C++ to get the sequencing real estate accounted for by features   to_plot -   seq_len (  nfeatures )   ncells -   ncol (  exprs_mat )   seq_real_estate -   .Call (  cxx_calc_top_features ,  exprs_mat ,  to_plot ,  NULL )   seq_real_estate_long -   data.frame ( Feature =   rep (  to_plot , each =  ncells ) , Cell =   rep (   seq_len (  ncells ) ,  nfeatures ) )    seq_real_estate_long $ Proportion_Library -    unlist (  seq_real_estate ) /  100 ## Add block and colour_by information if provided    seq_real_estate_long $ block1 -   rep (  block1_vals ,  nfeatures )    seq_real_estate_long $ block2 -   rep (  block2_vals ,  nfeatures )    seq_real_estate_long $ colour_by -   rep (  colour_by_vals ,  nfeatures ) ## Set up plot   aes -   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" )  if (  !   is.null (  colour_by ) )  {    aes $ colour -   as.symbol (  \"colour_by\" ) }   plot_out -    ggplot (  seq_real_estate_long ,  aes ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  line_width ) ## Deal with blocks for grid  if (   !   is.null (  block1 ) undefined  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_grid (   block2 ~  block1 ) } else  {  if (  !   is.null (  block1 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block1 , ncol =  ncol ) } else  if (  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block2 , ncol =  ncol ) } } ## Add extra plot theme and details  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   seq_real_estate_long $ colour_by ,  colour_by ) }   plot_out -    plot_out +   xlab (  \"Number of features\" ) +   ylab (  \"Cumulative proportion of library\" )  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )  {   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) } else  {   plot_out -   plot_out +   theme_bw (  theme_size ) }  plot_out } ",
    "filename": "plotScater.txt"
  }
}

7.
{
  "old_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , x_position = NULL , y_position = NULL , exprs_values = \"logcounts\" , theme_size = 24 , legend = \"auto\" )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"Object must be of class SingleCellExperiment\" ) ## check legend argument   legend -   match.arg (  legend ,   c (  \"auto\" ,  \"none\" ,  \"all\" ) ) ## Checking colour validity   colour_by_out -   .choose_vis_values (  object ,  colour_by , cell_control_default =  TRUE , check_features =  TRUE , exprs_values =  exprs_values )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## obtain well positions  if (  !   is.null (  plate_position ) )  {  if (    length (  plate_position ) !=   ncol (  object ) )   stop (  \"Supplied plate_position argument must have same length as number of columns of SingleCellExperiment object.\" )   plate_position_char -  plate_position } else   plate_position_char -   object $ plate_position  if (   is.null (  plate_position_char ) )  {  if (    is.null (  x_position ) ||   is.null (  y_position ) )   stop (  \"If plate_position is NULL then both x_position and y_position must be supplied.\" )   plate_position_x -  x_position   plate_position_y -  y_position } else  {   plate_position_y -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position_char )   plate_position_y -   factor (  plate_position_y ,   rev (   sort (   unique (  plate_position_y ) ) ) )   plate_position_x -   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position_char )   plate_position_x -   ordered (   as.integer (  plate_position_x ) ) } ## Define data.frame for plotting   df_to_plot -   data.frame (  plate_position_x ,  plate_position_y )  if (  !   is.null (  plate_position_char ) )    df_to_plot [[  \"plate_position_char\" ] ] -  plate_position_char    df_to_plot $ colour_by -  colour_by_vals ## make the plot   aesth -   aes ( x =  plate_position_x , y =  plate_position_y , fill =  colour_by )  if (  !   is.null (  plate_position_char ) )    aesth $ label -   as.symbol (  \"plate_position_char\" )   plot_out -    ggplot (  df_to_plot ,  aesth ) +   geom_point ( shape =  21 , size =  theme_size , colour =  \"gray50\" )  if (  !   is.null (  plate_position_char ) )   plot_out -   plot_out +   geom_text ( colour =  \"gray90\" ) ## make sure colours are nice   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =  TRUE ) ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired  if (   legend ==  \"none\" )   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) ## return plot  plot_out } ",
    "filename": "plotting.txt"
  },
  "new_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , size_by = NULL , shape_by = NULL , by_exprs_values = \"logcounts\" , by_show_single = FALSE , legend = TRUE , theme_size = 24 , alpha = 0.6 , size = 24 )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"Object must be of class SingleCellExperiment\" ) } ## obtain well positions  if (  !   is.list (  plate_position ) )  {  if (   is.null (  plate_position ) )  {   plate_position -   object $ plate_position }  if (   any (  !   grepl (  \"^[A-Z][0-9]+$\" ,  plate_position ) ) )  {   stop (  \"invalid format specified in 'plate_position'\" ) }   y_position -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position )   x_position -   as.integer (   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position ) ) } else  {   x_position -   as.integer (   plate_position $ column )   y_position -   as.character (   plate_position $ row )   plate_position -  NULL }   x_position -   as.factor (  x_position )   y_position -   factor (  y_position , levels =   rev (  LETTERS ) ) # Up-   df_to_plot -   data.frame ( X =  x_position , Y =  y_position ) ## checking visualization arguments   vis_out -   .incorporate_common_vis (  df_to_plot , se =  object , mode =  \"column\" , colour_by =  colour_by , shape_by =  shape_by , size_by =  size_by , by_exprs_values =  by_exprs_values , by_show_single =  by_show_single )   df_to_plot -   vis_out $ df   colour_by -   vis_out $ colour_by   shape_by -   vis_out $ shape_by   size_by -   vis_out $ size_by ## make the plot with appropriate colours.   plot_out -   ggplot (  df_to_plot ,   aes_string ( x =  \"X\" , y =  \"Y\" ) )   point_out -   .get_point_args (  colour_by ,  shape_by ,  size_by , alpha =  alpha , size =  size )   plot_out -   plot_out +   do.call (  geom_point ,   point_out $ args )  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =   point_out $ fill ) } ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired   plot_out -   .add_extra_guide (  plot_out ,  shape_by ,  size_by )  if (  !  legend )  {   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) }  plot_out } ",
    "filename": "plotPlatePosition.txt"
  }
}

8.
{
  "old_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , exprs_values = \"counts\" , feature_controls = NULL , cell_controls = NULL , nmads = 5 , pct_feature_controls_threshold = 80 )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"object must be a SingleCellExperiment\" )   exprs_mat -   assay (  object , i =  exprs_values )  if (    exprs_values ==  \"counts\" ||   exprs_values ==  \"cpm\" )  {   linear -  TRUE } else  {   linear -  FALSE } ##Adding general metrics for each cell.   cd -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  NULL , subset_type =  NULL , linear =  TRUE )   rd -   DataFrame ( is_feature_control =   logical (   nrow (  exprs_mat ) ) , row.names =   rownames (  exprs_mat ) ) ## Adding metrics for the technical controls.   n_feature_sets -   length (  feature_controls )  if (  n_feature_sets )  {  if (   is.null (   names (  feature_controls ) ) )  {   stop (  \"feature_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  feature_controls , FUN =  .subset2index , target =  exprs_mat )   is_fcon -   Reduce (  union ,  reindexed )     rd $ is_feature_control [  is_fcon ] -  TRUE # Adding feature controls.  for  ( f in   seq_len (  n_feature_sets ) )  {   cur.index -   logical (   nrow (  exprs_mat ) )    cur.index [   reindexed [[  f ] ] ] -  TRUE    rd [[   paste0 (  \"is_feature_control_\" ,    names (  reindexed ) [  f ] ) ] ] -  cur.index } # Running through all endogenous genes.   is_endog -   which (  !   rd $ is_feature_control )   cd_endog -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  is_endog , subset_type =  \"endogenous\" , linear =  linear ) # Running through all feature controls.   cd_fcon -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =  is_fcon , subset_type =  \"feature_control\" , linear =  linear ) # Running through each of the feature controls.   cd_per_fcon -   vector (  \"list\" ,  n_feature_sets )  for  ( f in   seq_len (  n_feature_sets ) )  {    cd_per_fcon [[  f ] ] -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =   reindexed [[  f ] ] , subset_type =    names (  reindexed ) [  f ] , linear =  linear ) }   cd -   do.call (  cbind ,   c (   list (  cd ,  cd_endog ,  cd_fcon ) ,  cd_per_fcon ) ) } ## Define cell controls ### Determine if vector or list   rd_all -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  NULL , subset_type =  NULL , linear =  linear )   rd -   cbind (  rd ,  rd_all )    cd $ is_cell_control -   logical (   ncol (  exprs_mat ) )   n_cell_sets -   length (  cell_controls )  if (  n_cell_sets )  { # Converting indices to integer.   reindexed -   lapply (  cell_controls , FUN =  .subset2index , target =  exprs_mat , byrow =  FALSE )   is_ccon -   Reduce (  union ,  reindexed )     cd $ is_cell_control [  is_ccon ] -  TRUE # Adding sets to the colData.  for  ( cx in   seq_len (  n_cell_sets ) )  {   current_control -   logical (   ncol (  exprs_mat ) )    current_control [   reindexed [[  cx ] ] ] -  TRUE    cd [[   paste0 (  \"is_cell_control_\" ,    names (  reindexed ) [  cx ] ) ] ] -  current_control } # Adding statistics for non-control cells.   is_noncon -   which (  !   cd $ is_cell_control )   rd_noncon -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  is_noncon , subset_type =  \"non_control\" , linear =  linear ) # Adding statistics for all control cells.   rd_con -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =  is_ccon , subset_type =  \"cell_control\" , linear =  linear ) # Adding statistics for each set of control cells.   rd_collected -   vector (  \"list\" ,  n_cell_sets )  for  ( cx in   seq_len (  n_cell_sets ) )  {   rd_current -   .get_qc_metrics_per_gene (  exprs_mat , exprs_type =  exprs_values , subset_col =   reindexed [[  cx ] ] , subset_type =    names (  reindexed ) [  cx ] , linear =  linear )    rd_collected [[  cx ] ] -  rd_current }   rd -   do.call (  cbind ,   c (   list (  rd ,  rd_con ,  rd_noncon ) ,  rd_collected ) ) } ### Remove columns to be replaced   old_rd -   rowData (  object )   old_rd -   old_rd [ ,  !  (    colnames (  old_rd ) %in%   colnames (  rd ) ) , drop =  FALSE ]   rd -   cbind (  old_rd ,  rd )    rowData (  object ) -  rd   old_cd -   colData (  object )   old_cd -   old_cd [ ,  !  (    colnames (  old_cd ) %in%   colnames (  cd ) ) , drop =  FALSE ]   cd -   cbind (  old_cd ,  cd )    colData (  object ) -  cd   return (  object ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "calculateQCMetrics",
    "representation": "calculateQCMetrics",
    "parameters": "function ( object , exprs_values = \"counts\" , feature_controls = NULL , cell_controls = NULL , percent_top = c ( 50 , 100 , 200 , 500 ) , detection_limit = 0 , use_spikes = TRUE , compact = FALSE )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"object must be a SingleCellExperiment\" ) }   exprs_mat -   assay (  object , i =  exprs_values )   percent_top -   as.integer (  percent_top ) ### Adding general metrics for each cell. ### # We first assemble the list of all metrics (all MUST be first). # We also add any existing spike-ins to this set unless otherwise specified.   all_feature_sets -   list ( all =  NULL )   existing_spikes -   spikeNames (  object )  if (   use_spikes undefined   length (  existing_spikes ) )  {   existing -   vector (  \"list\" ,   length (  existing_spikes ) )    names (  existing ) -  existing_spikes  for  ( spset in  existing_spikes )  {    existing [[  spset ] ] -   isSpike (  object , type =  spset ) }   already_there -    names (  existing ) %in%   names (  feature_controls )  if (   any (  already_there ) )  {   warning (   sprintf (  \"spike-in set '%s' overwritten by feature_controls set of the same name\" ,     names (  existing ) [  already_there ] [  1 ] ) ) }   feature_controls -   c (  feature_controls ,   existing [  !  already_there ] ) }   feature_set_rdata -   list ( )  if (   length (  feature_controls ) )  {  if (   is.null (   names (  feature_controls ) ) )  {   stop (  \"feature_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  feature_controls , FUN =  .subset2index , target =  exprs_mat )    names (  reindexed ) -   sprintf (  \"feature_control_%s\" ,   names (  reindexed ) )   is_fcon -   Reduce (  union ,  reindexed )   is_endog -    seq_len (   nrow (  exprs_mat ) ) [  -  is_fcon ]   all_feature_sets -   c (  all_feature_sets ,   list ( endogenous =  is_endog , feature_control =  is_fcon ) ,  reindexed ) # But storing logical vectors in the metadata.   feature_set_rdata -   vector (  \"list\" ,    length (  reindexed ) +  1 )    names (  feature_set_rdata ) -   c (  \"feature_control\" ,   names (  reindexed ) )  for  ( set in   names (  feature_set_rdata ) )  {   new_set -   logical (   nrow (  exprs_mat ) )    new_set [   all_feature_sets [[  set ] ] ] -  TRUE    feature_set_rdata [[  set ] ] -  new_set } } else  {   feature_set_rdata -   list ( feature_control =   logical (   nrow (  object ) ) ) } # Computing the cell-level metrics for each set.   cell_stats_by_feature_set -  all_feature_sets   total_exprs -  NULL  for  ( set in   names (  all_feature_sets ) )  {    cell_stats_by_feature_set [[  set ] ] -   .get_qc_metrics_per_cell (  exprs_mat , exprs_type =  exprs_values , subset_row =   all_feature_sets [[  set ] ] , percent_top =  percent_top , detection_limit =  detection_limit , total_exprs =  total_exprs , legacy =  !  compact )  if (   set ==  \"all\" )  {   total_exprs -    cell_stats_by_feature_set [[  set ] ] [[   paste0 (  \"total_\" ,  exprs_values ) ] ] } } ### Adding general metrics for each feature. ### # We first assemble thie list of all metrics (all MUST be first).   all_cell_sets -   list ( all =  NULL )   cell_set_cdata -   list ( )  if (   length (  cell_controls ) )  {  if (   is.null (   names (  cell_controls ) ) )  {   stop (  \"cell_controls should be named\" ) } # Converting to integer indices for all applications.   reindexed -   lapply (  cell_controls , FUN =  .subset2index , target =  exprs_mat , byrow =  FALSE )    names (  reindexed ) -   sprintf (  \"cell_control_%s\" ,   names (  reindexed ) )   is_ccon -   Reduce (  union ,  reindexed )   is_ncon -    seq_len (   ncol (  exprs_mat ) ) [  -  is_ccon ]   all_cell_sets -   c (  all_cell_sets ,   list ( non_control =  is_ncon , cell_control =  is_ccon ) ,  reindexed ) # But storing logical vectors in the metadata.   cell_set_cdata -   vector (  \"list\" ,    length (  reindexed ) +  1 )    names (  cell_set_cdata ) -   c (  \"cell_control\" ,   names (  reindexed ) )  for  ( set in   names (  cell_set_cdata ) )  {   new_set -   logical (   ncol (  exprs_mat ) )    new_set [   all_cell_sets [[  set ] ] ] -  TRUE    cell_set_cdata [[  set ] ] -  new_set } } else  {   cell_set_cdata -   list ( cell_control =   logical (   ncol (  object ) ) ) } # Computing the feature-level metrics for each set.   feature_stats_by_cell_set -  all_cell_sets   total_exprs -  NULL  for  ( set in   names (  all_cell_sets ) )  {    feature_stats_by_cell_set [[  set ] ] -   .get_qc_metrics_per_feature (  exprs_mat , exprs_type =  exprs_values , subset_col =   all_cell_sets [[  set ] ] , detection_limit =  detection_limit , total_exprs =  total_exprs , legacy =  !  compact )  if (   set ==  \"all\" )  {   total_exprs -    feature_stats_by_cell_set [[  set ] ] [[   paste0 (  \"total_\" ,  exprs_values ) ] ] } } ### Formatting output depending on whether we're compacting or not. ###  if (  compact )  {   scater_cd -   .convert_to_nested_DataFrame (    colData (  object ) $ scater_qc ,  cell_set_cdata ,  cell_stats_by_feature_set )   scater_rd -   .convert_to_nested_DataFrame (    rowData (  object ) $ scater_qc ,  feature_set_rdata ,  feature_stats_by_cell_set )     colData (  object ) $ scater_qc -  scater_cd     rowData (  object ) $ scater_qc -  scater_rd } else  {   message (  \"Note that the names of some metrics have changed, see 'Renamed metrics' in ?calculateQCMetrics.\r\nOld names are currently maintained for back-compatibility, but may be removed in future releases.\" )   scater_cd -   .convert_to_full_DataFrame (   colData (  object ) ,  cell_set_cdata ,  cell_stats_by_feature_set , trim.fun =  function ( x )   sub (  \"^feature_control_\" ,  \"\" ,  x ) )   scater_rd -   .convert_to_full_DataFrame (   rowData (  object ) ,  feature_set_rdata ,  feature_stats_by_cell_set , trim.fun =  function ( x )   sub (  \"^cell_control_\" ,  \"\" ,  x ) )    colData (  object ) -  scater_cd    rowData (  object ) -  scater_rd }   return (  object ) } ",
    "filename": "calculateQCMetrics.txt"
  }
}

9.
{
  "old_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL , min.diff = NA )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }  if (   any (   is.na (  metric ) ) )  {   warning (  \"missing values ignored during outlier detection\" ) }  if (  !   is.null (  batch ) )  {   N -   length (  metric )  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) } # Coercing non-NULL subset into a logical vector.  if (  !   is.null (  subset ) )  {   new.subset -   logical (  N )    names (  new.subset ) -   names (  metric )    new.subset [  subset ] -  TRUE   subset -  new.subset } # Computing QC metrics for each batch.   by.batch -   split (   seq_len (  N ) ,  batch )   collected -   logical (  N )  for  ( b in  by.batch )  {    collected [  b ] -   Recall (   metric [  b ] , nmads =  nmads , type =  type , log =  FALSE , subset =   subset [  b ] , batch =  NULL , min.diff =  min.diff ) }   return (  collected ) } # Computing median/MAD (possibly based on subset of the data).  if (  !   is.null (  subset ) )  {   submetric -   metric [  subset ]  if (    length (  submetric ) ==  0L )  {   warning (  \"no observations remaining after subsetting\" ) } } else  {   submetric -  metric }   cur.med -   median (  submetric , na.rm =  TRUE )   cur.mad -   mad (  submetric , center =  cur.med , na.rm =  TRUE )   diff.val -   max (  min.diff ,   nmads *  cur.mad , na.rm =  TRUE )   upper.limit -   cur.med +  diff.val   lower.limit -   cur.med -  diff.val   type -   match.arg (  type )  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   return (    metric undefined  lower.limit |   upper.limit undefined  metric ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL , min_diff = NA )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }  if (   any (   is.na (  metric ) ) )  {   warning (  \"missing values ignored during outlier detection\" ) }  if (  !   is.null (  batch ) )  {   N -   length (  metric )  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) } # Coercing non-NULL subset into a logical vector.  if (  !   is.null (  subset ) )  {   new.subset -   logical (  N )    names (  new.subset ) -   names (  metric )    new.subset [  subset ] -  TRUE   subset -  new.subset } # Computing QC metrics for each batch.   by.batch -   split (   seq_len (  N ) ,  batch )   collected -   logical (  N )  for  ( b in  by.batch )  {    collected [  b ] -   Recall (   metric [  b ] , nmads =  nmads , type =  type , log =  FALSE , subset =   subset [  b ] , batch =  NULL , min_diff =  min_diff ) }   return (  collected ) } # Computing median/MAD (possibly based on subset of the data).  if (  !   is.null (  subset ) )  {   submetric -   metric [  subset ]  if (    length (  submetric ) ==  0L )  {   warning (  \"no observations remaining after subsetting\" ) } } else  {   submetric -  metric }   cur.med -   median (  submetric , na.rm =  TRUE )   cur.mad -   mad (  submetric , center =  cur.med , na.rm =  TRUE )   diff.val -   max (  min_diff ,   nmads *  cur.mad , na.rm =  TRUE )   upper.limit -   cur.med +  diff.val   lower.limit -   cur.med -  diff.val   type -   match.arg (  type )  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   return (    metric undefined  lower.limit |   upper.limit undefined  metric ) } ",
    "filename": "isOutlier.txt"
  }
}

10.
{
  "old_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , col_by_variable = \"total_features\" , n = 50 , drop_features = NULL , exprs_values = \"counts\" , feature_names_to_plot = NULL )",
    "body": "{ ## Check that variable to colour points exists  if (  !  (   col_by_variable %in%   colnames (   colData (  object ) ) ) )  {   stop (  \"col_by_variable not found in colData(object).\r\n             Please make sure colData(object)[, variable] exists.\" )   plot_cols -  FALSE } else   plot_cols -  TRUE   x -    colData (  object ) [ ,  col_by_variable ] #     x_na #     x ## Determine type of variable   typeof_x -   .getTypeOfVariable (  object ,  col_by_variable ) ## Figure out which features to drop  if (  !  (    is.null (  drop_features ) |    length (  drop_features ) ==  0 ) )  {  if (   is.character (  drop_features ) )   drop_features -   which (    rownames (  object ) %in%  drop_features )  if (   is.logical (  drop_features ) )   object -   object [  !  drop_features , ] else   object -   object [  -  drop_features , ] } ## Compute QC metrics on the (possibly) modified SingleCellExperiment object to make sure ## we have the relevant values for this set of features  if (  !   is.null (    rowData (  object ) $ is_feature_control ) )   object -   calculateQCMetrics (  object , feature_controls =   list ( all =    rowData (  object ) $ is_feature_control ) ) else   object -   calculateQCMetrics (  object ) ## Define expression values to be used   exprs_values -   match.arg (  exprs_values ,   c (  \"logcounts\" ,  \"tpm\" ,  \"cpm\" ,  \"fpkm\" ,  \"counts\" ) )   exprs_mat -   assay (  object ,  exprs_values )  if (    is.null (  exprs_mat ) undefined  !   is.null (   counts (  object ) ) )  {   exprs_mat -   counts (  object )   message (  \"Using counts as expression values.\" )   exprs_values -  \"counts\" } else  if (   is.null (  exprs_mat ) )  {   exprs_mat -   exprs (  object )   message (  \"Using exprs(object) values as expression values.\" )   exprs_values -  \"logcounts\" }  if (   exprs_values ==  \"logcounts\" )   exprs_mat -    2 ^  exprs_mat -   object @ logExprsOffset ## Find the most highly expressed features in this dataset ### Order by total feature counts across whole dataset   rdata -   rowData (  object )  if (    paste0 (  \"rank_\" ,  exprs_values ) %in%   colnames (  rdata ) )   oo -   order (   rdata [[   paste0 (  \"rank_\" ,  exprs_values ) ] ] , decreasing =  TRUE ) else  {  if (   \"rank_counts\" %in%   colnames (  rdata ) )  {   oo -   order (   rdata [[  \"rank_counts\" ] ] , decreasing =  TRUE )   exprs_values -  \"counts\"   message (  \"Using counts to order total expression of features.\" ) } else  {   exprs_values -  \"logcounts\"   oo -   order (   rdata [[  \"rank_exprs\" ] ] , decreasing =  TRUE )   message (  \"Using 'exprs' to order total expression of features.\" ) } } ## define feature names for plot  if (    is.null (  feature_names_to_plot ) ||   is.null (    rowData (  object ) [[  feature_names_to_plot ] ] ) )    rdata $ feature -   factor (   rownames (  object ) , levels =    rownames (  object ) [   rev (  oo ) ] ) else    rdata $ feature -   factor (    rowData (  object ) [[  feature_names_to_plot ] ] , levels =     rowData (  object ) [[  feature_names_to_plot ] ] [   rev (  oo ) ] )    rdata $ Feature -   rdata $ feature ## Check if is_feature_control is defined  if (   is.null (   rdata $ is_feature_control ) )    rdata $ is_feature_control -   rep (  FALSE ,   nrow (  rdata ) ) ## Determine percentage expression accounted for by top features across all ## cells   total_exprs -   sum (  exprs_mat )   top50_pctage -    100 *   sum (    .general_rowSums (  exprs_mat ) [   oo [   1 :  n ] ] ) /  total_exprs ## Determine percentage of counts for top features by cell   df_pct_exprs_by_cell -  (    100 *   t (   exprs_mat [   oo [   1 :  n ] , ] ) /   .general_colSums (  exprs_mat ) )   pct_total -    100 *   .general_rowSums (  exprs_mat ) /  total_exprs    rdata [[  \"pct_total\" ] ] -  pct_total ## Melt dataframe so it is conducive to ggplot  if (   is.null (   rownames (  rdata ) ) )    rownames (  rdata ) -   as.character (   rdata $ feature )   df_pct_exprs_by_cell -   as.matrix (  df_pct_exprs_by_cell ) # coercing to a normal matrix.   df_pct_exprs_by_cell_long -   reshape2 :: melt (  df_pct_exprs_by_cell )    colnames (  df_pct_exprs_by_cell_long ) -   c (  \"Cell\" ,  \"Tags\" ,  \"value\" )    df_pct_exprs_by_cell_long $ Feature -   rdata [   as.character (   df_pct_exprs_by_cell_long $ Tags ) ,  \"feature\" ]    df_pct_exprs_by_cell_long $ Tags -   factor (   df_pct_exprs_by_cell_long $ Tags , levels =    rownames (  object ) [   rev (   oo [   1 :  n ] ) ] )    df_pct_exprs_by_cell_long $ Feature -   factor (   df_pct_exprs_by_cell_long $ Feature , levels =    rdata $ feature [   rev (   oo [   1 :  n ] ) ] ) ## Add colour variable information  if (   typeof_x ==  \"discrete\" )    df_pct_exprs_by_cell_long $ colour_by -   factor (  x ) else    df_pct_exprs_by_cell_long $ colour_by -  x ## Make plot   plot_most_expressed -         ggplot (  df_pct_exprs_by_cell_long ,   aes_string ( y =  \"Feature\" , x =  \"value\" , colour =  \"colour_by\" ) ) +   geom_point ( alpha =  0.6 , shape =  124 ) +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top50_pctage , digits =  3 ) ,  \"% of total\" ) ) +   ylab (  \"Feature\" ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (   typeof_x ==  \"discrete\" )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_pct_exprs_by_cell_long $ colour_by ,  col_by_variable ) #         plot_most_expressed #             ggthemes::scale_colour_tableau(name = col_by_variable) } else  {   plot_most_expressed -   plot_most_expressed +   scale_colour_gradient ( name =  col_by_variable , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) }     plot_most_expressed +   geom_point (   aes_string ( x =  \"as.numeric(pct_total)\" , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =   as.data.frame (   rdata [   oo [   1 :  n ] , ] ) , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } ",
    "filename": "qc.txt"
  },
  "new_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , controls , colour_cells_by , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , by_show_single = TRUE , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{  if (   is.null (   rownames (  object ) ) )  {    rownames (  object ) -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) }  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   object -   object [  -  to_discard , ] } ## Define expression values to be used ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   .rowSums (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )   chosen -   head (  oo ,  n ) ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object ) } else  {   feature_names -    .choose_vis_values (  object ,  feature_names_to_plot , search =  \"metadata\" , mode =  \"row\" ) $ val }    rownames (  exprs_mat ) -  feature_names ## Compute expression values and reshape them for ggplot.   df_exprs_by_cell -   t (   exprs_mat [  chosen , ] )   df_exprs_by_cell -   as.matrix (  df_exprs_by_cell )  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top50_pctage -    100 *   sum (   ave_exprs [  chosen ] ) /  total_exprs   df_exprs_by_cell -    100 *  df_exprs_by_cell /   .colSums (  exprs_mat ) }   df_exprs_by_cell_long -   reshape2 :: melt (  df_exprs_by_cell )    colnames (  df_exprs_by_cell_long ) -   c (  \"Cell\" ,  \"Tag\" ,  \"value\" )    df_exprs_by_cell_long $ Tag -   factor (   df_exprs_by_cell_long $ Tag ,   rev (   feature_names [  chosen ] ) ) ## Colouring the individual dashes for the cells.  if (   missing (  colour_cells_by ) )  {   colour_cells_by -   .qc_hunter (  object ,   paste0 (  \"total_features_by_\" ,  exprs_values ) , mode =  \"column\" ) }  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   .choose_vis_values (  object ,  colour_cells_by , mode =  \"column\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )  if (  as_percentage )  {   plot_most_expressed -    plot_most_expressed +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top50_pctage , digits =  3 ) ,  \"% of total\" ) ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) } else  {   plot_most_expressed -   plot_most_expressed +   xlab (  exprs_values ) }   plot_most_expressed -     plot_most_expressed +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (  !   is.null (  colour_cells_by ) )  {  if (  !   is.numeric (   df_exprs_by_cell_long $ colour_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } else  {   plot_most_expressed -   plot_most_expressed +   scale_colour_gradient ( name =  colour_cells_by , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) } } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =   factor (  feature_names , levels =   rev (  feature_names ) ) )  if (  as_percentage )  {   pct_total -    100 *  ave_exprs /  total_exprs    df_to_plot $ pct_total -  pct_total   legend_val -  \"as.numeric(pct_total)\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  ave_exprs   legend_val -   sprintf (  \"as.numeric(ave_%s)\" ,  exprs_values ) } ## Check if is_feature_control is defined, and using it for colouring of the points.  if (   missing (  controls ) )  {   controls -   .qc_hunter (  object ,  \"is_feature_control\" , mode =  \"row\" ) }  if (  !   is.null (  controls ) )  {   cont_out -   .choose_vis_values (  object ,  controls , mode =  \"row\" , search =  \"metadata\" )    df_to_plot $ is_feature_control -   cont_out $ val   plot_most_expressed -     plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =   df_to_plot [  chosen , ] , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } else  {   plot_most_expressed -   plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" ) , data =   df_to_plot [  chosen , ] , fill =  \"grey80\" , colour =  \"grey30\" , shape =  21 ) }  plot_most_expressed } ",
    "filename": "plotHighestExprs.txt"
  }
}

11.
{
  "old_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "SingleCellExperiment",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , return_norm_as_exprs = TRUE )",
    "body": "{  if (   exprs_values ==  \"exprs\" )  {   exprs_values -  \"logcounts\" }   exprs_mat -   assay (  object , i =  exprs_values )  if (   exprs_values ==  \"counts\" )  {   sf.list -   .get_all_sf_sets (  object )  if (   is.null (    sf.list $ size.factors [[  1 ] ] ) )  {   warning (  \"using library sizes as size factors\" )     sf.list $ size.factors [[  1 ] ] -   .general_colSums (  exprs_mat ) } ## figuring out how many controls have their own size factors   spike.names -   spikeNames (  object )   no.spike.sf -  !   spike.names %in%   sf.list $ available  if (   any (  no.spike.sf ) )  {   warning (   sprintf (  \"spike-in transcripts in '%s' should have their own size factors\" ,    spike.names [  no.spike.sf ] [  1 ] ) ) } } else  { # ignoring size factors for non-count data.   sf.list -   list ( size.factors =   rep (  1 ,   ncol (  object ) ) , index =  NULL ) } ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (  exprs_mat ,   sf.list $ size.factors , sf_to_use =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {  if (  return_norm_as_exprs )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"norm_exprs\" ) -  norm_exprs } } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## centering all existing size factors if requested  if (    exprs_values ==  \"counts\" undefined  centre_size_factors )  {   sf -   sizeFactors (  object )  if (  !   is.null (  sf ) )  {   sf -   sf /   mean (  sf )    sizeFactors (  object ) -  sf } # ... and for all controls.  for  ( type in   sf.list $ available )  {   sf -   sizeFactors (  object , type =  type )   sf -   sf /   mean (  sf )    sizeFactors (  object , type =  type ) -  sf } } ## return object   return (  object ) } ",
    "replacementFunction": "normalizeSCE",
    "filename": "normalisation.txt"
  },
  "new_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "SingleCellExperiment",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , size_factor_grouping = NULL )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) }  if (  centre_size_factors )  {   object -   centreSizeFactors (  object , grouping =  size_factor_grouping ) }   sf.list -   .get_all_sf_sets (  object ) ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (   assay (  object , i =  exprs_values ) , size_factor_val =   sf.list $ size.factors , size_factor_idx =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "replacementFunction": "normalizeSCE",
    "filename": "normalizeSCE.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_7 scater_release_3_8

{
    "package": "scater",
    "release_versions": "scater_release_3_7 scater_release_3_8",
    "desc_release_old": "1.8.4",
    "desc_release_new": "1.10.1",
    "old_release_number": 4,
    "new_release_number": 5,
    "function_removals": 11,
    "function_additions": 9,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 7,
    "total_count": 18
}

##########
Functions Removed
##########

areSizeFactorsCentred
downsampleCounts
normaliseExprs
normalizeExprs
read10XResults
read10xResults
readKallistoResultsOneSample
readSalmonResultsOneSample
runKallisto
runSalmon
scater_gui


##########
Functions Added
##########

calculateAverage
getExplanatoryPCs
getVarianceExplained
normalizeCounts
plotExplanatoryPCs
plotUMAP
readSparseCounts
runUMAP
sumCountsAcrossFeatures


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "centreSizeFactors",
    "representation": "centreSizeFactors",
    "parameters": "function ( object , centre = 1 , grouping = NULL )",
    "body": "{ # Setting up a function to centre the size factors by group.  if (   is.null (  grouping ) )  {   centrefun -  function ( x )  {    x /   mean (  x ) *  centre } } else  {   by_group -   split (   seq_len (   ncol (  object ) ) ,  grouping )   centrefun -  function ( x )  {  for  ( g in  by_group )  {   current -   x [  g ]    x [  g ] -    current /   mean (  current ) *  centre }   return (  x ) } } # Running through the sets of size factors and centering them as necessary.   sf -   sizeFactors (  object )  if (  !   is.null (  sf ) )  {    sizeFactors (  object ) -   centrefun (  sf ) }  for  ( sf_name in   sizeFactorNames (  object ) )  {   sf -   sizeFactors (  object ,  sf_name )  if (  !   is.null (  sf ) )  {    sizeFactors (  object ,  sf_name ) -   centrefun (  sf ) } }   return (  object ) } ",
    "filename": "areSizeFactorsCentred.txt"
  },
  "new_function": {
    "name": "centreSizeFactors",
    "representation": "centreSizeFactors",
    "parameters": "function ( object , centre = 1 )",
    "body": "{   centrefun -  function ( x )  {    x /   mean (  x ) *  centre }   .apply_to_size_factors (  object ,  centrefun ) } ",
    "filename": "centreSizeFactors.txt"
  }
}

1.
{
  "old_function": {
    "name": "calculateTPM",
    "representation": "calculateTPM",
    "parameters": "function ( object , effective_length = NULL , calc_from = \"counts\" )",
    "body": "{  if (  !   methods :: is (  object ,  \"SingleCellExperiment\" ) )   stop (  \"object must be an SingleCellExperiment\" ) ## Check that arguments are correct   calc_from -   match.arg (  calc_from ,   c (  \"counts\" ,  \"normcounts\" ,  \"fpkm\" ) , several.ok =  FALSE )  if (    calc_from ==  \"counts\" ||   calc_from ==  \"normcounts\" )  {  if (   is.null (  effective_length ) )   stop (  \"effective_length argument is required if computing \r\n                 TPM from counts\" ) } ## Compute values to return   tpm_to_add -   switch (  calc_from , counts =   .countToTpm (   counts (  object ) ,  effective_length ) , normcounts =   .countToTpm (   normcounts (  object ) ,  effective_length ) , fpkm =   .fpkmToTpm (   fpkm (  object ) ) ) ## Return TPM values    rownames (  tpm_to_add ) -   rownames (  object )    colnames (  tpm_to_add ) -   colnames (  object )  tpm_to_add } ",
    "filename": "calculate-expression.txt"
  },
  "new_function": {
    "name": "calculateTPM",
    "representation": "calculateTPM",
    "parameters": "function ( object , effective_length = NULL , exprs_values = \"counts\" , subset_row = NULL )",
    "body": "{  if (   is (  object ,  \"SingleCellExperiment\" ) )  {   object -   assay (  object , i =  exprs_values ) }  if (  !   is.null (  effective_length ) )  {   object -   object /  effective_length }   calculateCPM (  object , subset_row =  subset_row ) } ",
    "filename": "calculateTPM.txt"
  }
}

2.
{
  "old_function": {
    "name": "calculateCPM",
    "representation": "calculateCPM",
    "parameters": "function ( object , exprs_values = \"counts\" , use_size_factors = TRUE , size_factor_grouping = NULL , subset_row = NULL )",
    "body": "{  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   assays -   list (  object )    names (  assays ) -  exprs_values   object -   SingleCellExperiment (  assays ) } # Setting up the size factors.   object -   .replace_size_factors (  object ,  use_size_factors )  if (   is.null (   sizeFactors (  object ) ) )  {    sizeFactors (  object ) -   librarySizeFactors (  object ) }   object -   centreSizeFactors (  object , grouping =  size_factor_grouping )   sf_list -   .get_all_sf_sets (  object ) # Computes the average count, adjusting for size factors or library size.   extracted -   assay (  object ,  exprs_values )   normed -   .compute_exprs (  extracted , size_factor_val =   sf_list $ size.factors , size_factor_idx =   sf_list $ index , log =  FALSE , sum =  FALSE , logExprsOffset =  0 , subset_row =  subset_row )   lib_sizes -   colSums2 (   DelayedArray (  extracted ) )   cpm_mat -   normed /  (    mean (  lib_sizes ) /  1e6 )   return (  cpm_mat ) } ",
    "filename": "calculateCPM.txt"
  },
  "new_function": {
    "name": "calculateCPM",
    "representation": "calculateCPM",
    "parameters": "function ( object , exprs_values = \"counts\" , use_size_factors = TRUE , subset_row = NULL )",
    "body": "{   subset_row -   .subset2index (  subset_row ,  object , byrow =  TRUE )  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   assays -   list (  object )    names (  assays ) -  exprs_values   object -   SingleCellExperiment (  assays ) } # Setting up the size factors.   object -   .replace_size_factors (  object ,  use_size_factors )   lib_sizes -   colSums2 (   assay (  object ,  exprs_values , withDimnames =  FALSE ) , rows =  subset_row )  if (   is.null (   sizeFactors (  object ) ) )  {    sizeFactors (  object ) -  lib_sizes }   meanlib_millions -    mean (  lib_sizes ) /  1e6   object -   centreSizeFactors (  object , centre =  meanlib_millions )   sf.list -   .get_all_sf_sets (  object ) # Computing the CPM values.   output -   .Call (  cxx_norm_exprs ,   assay (  object , i =  exprs_values , withDimnames =  FALSE ) ,   sf.list $ size.factors ,    sf.list $ index -  1L ,  0 ,  FALSE , subset_row =   subset_row -  1L )    dimnames (  output ) -   list (    rownames (  object ) [  subset_row ] ,   colnames (  object ) )  output } ",
    "filename": "calculateCPM.txt"
  }
}

3.
{
  "old_function": {
    "name": "getBMFeatureAnnos",
    "representation": "getBMFeatureAnnos",
    "parameters": "function ( object , filters = \"ensembl_transcript_id\" , attributes = c ( \"ensembl_transcript_id\" , \"ensembl_gene_id\" , feature_symbol , \"chromosome_name\" , \"transcript_biotype\" , \"transcript_start\" , \"transcript_end\" , \"transcript_count\" ) , feature_symbol = \"mgi_symbol\" , feature_id = \"ensembl_gene_id\" , biomart = \"ENSEMBL_MART_ENSEMBL\" , dataset = \"mmusculus_gene_ensembl\" , host = \"www.ensembl.org\" )",
    "body": "{ ## Define Biomart Mart to use  if (   is.null (  host ) )   bmart -   biomaRt :: useMart ( biomart =  biomart , dataset =  dataset ) else   bmart -   biomaRt :: useMart ( biomart =  biomart , dataset =  dataset , host =  host ) ## Define feature IDs from SingleCellExperiment object   feature_ids -   rownames (  object ) ## Remove transcript ID artifacts from runKallisto (eg. ENSMUST00000201087.11 -   feature_ids -   gsub ( pattern =  \"\\\\.[0-9]+\" , replacement =  \"\" , x =  feature_ids ) ## Get annotations from biomaRt   feature_info -   biomaRt :: getBM ( attributes =  attributes , filters =  filters , values =  feature_ids , mart =  bmart ) ## Match the feature ids to the filters ids used to get info from biomaRt   mm -   match (  feature_ids ,   feature_info [[  filters ] ] )   feature_info_full -   feature_info [  mm , ]    rownames (  feature_info_full ) -  feature_ids ## Define gene symbol and gene id    feature_info_full $ feature_symbol -   feature_info_full [[  feature_symbol ] ]    feature_info_full $ feature_id -   feature_info_full [[  feature_id ] ] ## Use rownames for gene symbol if gene symbol is missing   na_symbol -  (    is.na (   feature_info_full $ feature_symbol ) |    feature_info_full $ feature_symbol ==  \"\" )     feature_info_full $ feature_symbol [  na_symbol ] -    rownames (  feature_info_full ) [  na_symbol ] ## Use rownames from SingleCellExperiment object (feature IDs) for feature_id if na     feature_info_full $ feature_id [   is.na (   feature_info_full $ feature_id ) ] -    rownames (  feature_info_full ) [   is.na (   feature_info_full $ feature_id ) ] ## Need to drop any duplicated columns that we want to replace   old_rdata -   rowData (  object )   keep_cols -  !  (    colnames (  old_rdata ) %in%   c (  \"feature_symbol\" ,  \"feature_id\" ,  attributes ) )  if (    sum (  keep_cols ) undefined  0 )  {   colnames_old_rdata -   colnames (  old_rdata )   old_rdata -   as.data.frame (   old_rdata [ ,  keep_cols ] )    colnames (  old_rdata ) -   colnames_old_rdata [  keep_cols ]   new_rdata -   cbind (  old_rdata ,  feature_info_full ) } else   new_rdata -  feature_info_full ## Add new feature annotations to SingleCellExperiment object    rowData (  object ) -  new_rdata ## Return SingleCellExperiment object  object } ",
    "filename": "feature-preprocessing.txt"
  },
  "new_function": {
    "name": "getBMFeatureAnnos",
    "representation": "getBMFeatureAnnos",
    "parameters": "function ( object , ids = rownames ( object ) , filters = \"ensembl_gene_id\" , attributes = c ( filters , \"mgi_symbol\" , \"chromosome_name\" , \"gene_biotype\" , \"start_position\" , \"end_position\" ) , biomart = \"ENSEMBL_MART_ENSEMBL\" , dataset = \"mmusculus_gene_ensembl\" , host = \"www.ensembl.org\" )",
    "body": "{ ## Define Biomart Mart to use  if (   is.null (  host ) )  {   bmart -   biomaRt :: useMart ( biomart =  biomart , dataset =  dataset ) } else  {   bmart -   biomaRt :: useMart ( biomart =  biomart , dataset =  dataset , host =  host ) } ## Get annotations from biomaRt   feature_info -   biomaRt :: getBM ( attributes =  attributes , filters =  filters , values =  ids , mart =  bmart ) # Match the feature ids to the filters id.   mm -   match (  ids ,   feature_info [[  filters ] ] )   feature_info_full -   feature_info [  mm , ] ## Drop duplicated columns that we want to replace   old_rdata -   rowData (  object )   keep_cols -  !  (    colnames (  old_rdata ) %in%   colnames (  feature_info_full ) )   new_rdata -   cbind (   old_rdata [ ,  keep_cols ] ,  feature_info_full ) ## Add new feature annotations to SingleCellExperiment object    rowData (  object ) -  new_rdata  object } ",
    "filename": "getBMFeatureAnnos.txt"
  }
}

4.
{
  "old_function": {
    "name": "normalizeSCE",
    "representation": "normalizeSCE",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , size_factor_grouping = NULL )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) }  if (  centre_size_factors )  {   object -   centreSizeFactors (  object , grouping =  size_factor_grouping ) }   sf.list -   .get_all_sf_sets (  object ) ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (   assay (  object , i =  exprs_values ) , size_factor_val =   sf.list $ size.factors , size_factor_idx =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "filename": "normalizeSCE.txt"
  },
  "new_function": {
    "name": "normalizeSCE",
    "representation": "normalizeSCE",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , preserve_zeroes = FALSE )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) } ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## centering size factors, with interaction with pseudo-count  if (  centre_size_factors )  {   object -   centreSizeFactors (  object ) }  if (  preserve_zeroes )  {   object -   .apply_to_size_factors (  object , FUN =  function ( sf )   sf *  log_exprs_offset )   log_exprs_offset -  1 }   sf.list -   .get_all_sf_sets (  object ) ## Compute normalized expression values.   norm_exprs -   .Call (  cxx_norm_exprs ,   assay (  object , i =  exprs_values , withDimnames =  FALSE ) ,   sf.list $ size.factors ,    sf.list $ index -  1L ,   as.numeric (  log_exprs_offset ) ,   as.logical (  return_log ) , subset_row =    seq_len (   nrow (  object ) ) -  1L ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "filename": "normalizeSCE.txt"
  }
}

5.
{
  "old_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , size_by = NULL , shape_by = NULL , by_exprs_values = \"logcounts\" , by_show_single = FALSE , legend = TRUE , theme_size = 24 , alpha = 0.6 , size = 24 )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"Object must be of class SingleCellExperiment\" ) } ## obtain well positions  if (  !   is.list (  plate_position ) )  {  if (   is.null (  plate_position ) )  {   plate_position -   object $ plate_position }  if (   any (  !   grepl (  \"^[A-Z][0-9]+$\" ,  plate_position ) ) )  {   stop (  \"invalid format specified in 'plate_position'\" ) }   y_position -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position )   x_position -   as.integer (   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position ) ) } else  {   x_position -   as.integer (   plate_position $ column )   y_position -   as.character (   plate_position $ row )   plate_position -  NULL }   x_position -   as.factor (  x_position )   y_position -   factor (  y_position , levels =   rev (  LETTERS ) ) # Up-   df_to_plot -   data.frame ( X =  x_position , Y =  y_position ) ## checking visualization arguments   vis_out -   .incorporate_common_vis (  df_to_plot , se =  object , mode =  \"column\" , colour_by =  colour_by , shape_by =  shape_by , size_by =  size_by , by_exprs_values =  by_exprs_values , by_show_single =  by_show_single )   df_to_plot -   vis_out $ df   colour_by -   vis_out $ colour_by   shape_by -   vis_out $ shape_by   size_by -   vis_out $ size_by ## make the plot with appropriate colours.   plot_out -   ggplot (  df_to_plot ,   aes_string ( x =  \"X\" , y =  \"Y\" ) )   point_out -   .get_point_args (  colour_by ,  shape_by ,  size_by , alpha =  alpha , size =  size )   plot_out -   plot_out +   do.call (  geom_point ,   point_out $ args )  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =   point_out $ fill ) } ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired   plot_out -   .add_extra_guide (  plot_out ,  shape_by ,  size_by )  if (  !  legend )  {   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) }  plot_out } ",
    "filename": "plotPlatePosition.txt"
  },
  "new_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , size_by = NULL , shape_by = NULL , by_exprs_values = \"logcounts\" , by_show_single = FALSE , add_legend = TRUE , theme_size = 24 , point_alpha = 0.6 , point_size = 24 )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"Object must be of class SingleCellExperiment\" ) } ## obtain well positions  if (  !   is.list (  plate_position ) )  {  if (   is.null (  plate_position ) )  {   plate_position -   object $ plate_position }  if (   any (  !   grepl (  \"^[A-Z][0-9]+$\" ,  plate_position ) ) )  {   stop (  \"invalid format specified in 'plate_position'\" ) }   y_position -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position )   x_position -   as.integer (   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position ) ) } else  {   x_position -   as.integer (   plate_position $ column )   y_position -   as.character (   plate_position $ row )   plate_position -  NULL }   x_position -   as.factor (  x_position )   y_position -   factor (  y_position , levels =   rev (  LETTERS ) ) # Up-   df_to_plot -   data.frame ( X =  x_position , Y =  y_position ) ## checking visualization arguments   vis_out -   .incorporate_common_vis (  df_to_plot , se =  object , mode =  \"column\" , colour_by =  colour_by , shape_by =  shape_by , size_by =  size_by , by_exprs_values =  by_exprs_values , by_show_single =  by_show_single )   df_to_plot -   vis_out $ df   colour_by -   vis_out $ colour_by   shape_by -   vis_out $ shape_by   size_by -   vis_out $ size_by ## make the plot with appropriate colours.   plot_out -   ggplot (  df_to_plot ,   aes_string ( x =  \"X\" , y =  \"Y\" ) )   point_out -   .get_point_args (  colour_by ,  shape_by ,  size_by , alpha =  point_alpha , size =  point_size )   plot_out -   plot_out +   do.call (  geom_point ,   point_out $ args )  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =   point_out $ fill ) } ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired   plot_out -   .add_extra_guide (  plot_out ,  shape_by ,  size_by )  if (  !  add_legend )  {   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) }  plot_out } ",
    "filename": "plotPlatePosition.txt"
  }
}

6.
{
  "old_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "SingleCellExperiment",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , size_factor_grouping = NULL )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) }  if (  centre_size_factors )  {   object -   centreSizeFactors (  object , grouping =  size_factor_grouping ) }   sf.list -   .get_all_sf_sets (  object ) ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## Compute normalized expression values.   norm_exprs -   .compute_exprs (   assay (  object , i =  exprs_values ) , size_factor_val =   sf.list $ size.factors , size_factor_idx =   sf.list $ index , log =  return_log , sum =  FALSE , logExprsOffset =  log_exprs_offset , subset_row =  NULL ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "replacementFunction": "normalizeSCE",
    "filename": "normalizeSCE.txt"
  },
  "new_function": {
    "name": "normalize",
    "representation": "normalize",
    "signature": "SingleCellExperiment",
    "parameters": "function ( object , exprs_values = \"counts\" , return_log = TRUE , log_exprs_offset = NULL , centre_size_factors = TRUE , preserve_zeroes = FALSE )",
    "body": "{ ## setting up the size factors.  if (   is.null (   sizeFactors (  object ) ) )  {   warning (  \"using library sizes as size factors\" )    sizeFactors (  object ) -   librarySizeFactors (  object , exprs_values =  exprs_values ) } ## using logExprsOffset=1 if argument is NULL  if (   is.null (  log_exprs_offset ) )  {  if (  !   is.null (    metadata (  object ) $ log.exprs.offset ) )  {   log_exprs_offset -    metadata (  object ) $ log.exprs.offset } else  {   log_exprs_offset -  1 } } ## centering size factors, with interaction with pseudo-count  if (  centre_size_factors )  {   object -   centreSizeFactors (  object ) }  if (  preserve_zeroes )  {   object -   .apply_to_size_factors (  object , FUN =  function ( sf )   sf *  log_exprs_offset )   log_exprs_offset -  1 }   sf.list -   .get_all_sf_sets (  object ) ## Compute normalized expression values.   norm_exprs -   .Call (  cxx_norm_exprs ,   assay (  object , i =  exprs_values , withDimnames =  FALSE ) ,   sf.list $ size.factors ,    sf.list $ index -  1L ,   as.numeric (  log_exprs_offset ) ,   as.logical (  return_log ) , subset_row =    seq_len (   nrow (  object ) ) -  1L ) ## add normalised values to object  if (  return_log )  {    assay (  object ,  \"logcounts\" ) -  norm_exprs     metadata (  object ) $ log.exprs.offset -  log_exprs_offset } else  {    assay (  object ,  \"normcounts\" ) -  norm_exprs } ## return object   return (  object ) } ",
    "replacementFunction": "normalizeSCE",
    "filename": "normalizeSCE.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_8 scater_release_3_9

{
    "package": "scater",
    "release_versions": "scater_release_3_8 scater_release_3_9",
    "desc_release_old": "1.10.1",
    "desc_release_new": "1.12.2",
    "old_release_number": 5,
    "new_release_number": 6,
    "function_removals": 11,
    "function_additions": 1,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 0,
    "total_count": 11
}

##########
Functions Removed
##########

calcIsExprs
findImportantPCs
normalise
plotCellData
plotFeatureData
plotPhenoData
plotQC
readKallistoResults
readSalmonResults
readTxResults
summariseExprsAcrossFeatures


##########
Functions Added
##########

sumCountsAcrossCells


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_9 scater_release_3_11

{
    "package": "scater",
    "release_versions": "scater_release_3_9 scater_release_3_11",
    "desc_release_old": "1.12.2",
    "desc_release_new": "1.16.2",
    "old_release_number": 6,
    "new_release_number": 7,
    "function_removals": 8,
    "function_additions": 38,
    "parameter_removals": 1,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 4,
    "total_count": 12
}

##########
Functions Removed
##########

arrange
calcAverage
filter
mutate
normalizeSCE
plotExprsFreqVsMean
plotExprsVsTxLength
rename


##########
Functions Added
##########

.assignIndicesToWorkers
.bpNotSharedOrUp
.splitColsByWorkers
.splitRowsByWorkers
.splitVectorByWorkers
.subset2index
addPerCellQC
addPerFeatureQC
aggregateAcrossCells
aggregateAcrossFeatures
annotateBMFeatures
calculateDiffusionMap
calculateMDS
calculateNMF
calculatePCA
calculateTSNE
calculateUMAP
computeLibraryFactors
computeMedianFactors
ggcells
ggfeatures
logNormCounts
makePerCellDF
makePerFeatureDF
medianSizeFactors
mockSCE
numDetectedAcrossCells
numDetectedAcrossFeatures
perCellQCMetrics
perFeatureQCMetrics
plotDots
plotNMF
quickPerCellQC
retrieveCellInfo
retrieveFeatureInfo
runColDataPCA
runMultiUMAP
runNMF


##########
Removed Non Default Parameters
##########

0.
{
  "old_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , controls , colour_cells_by , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , by_show_single = TRUE , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{ ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   rowSums2 (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   oo -   setdiff (  oo ,  to_discard ) }   chosen -   head (  oo ,  n )   sub_mat -   exprs_mat [  chosen , , drop =  FALSE ]   sub_ave -   ave_exprs [  chosen ] ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object )  if (   is.null (  feature_names ) )  {   feature_names -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) } } else  {   feature_names -    .choose_vis_values (  object ,  feature_names_to_plot , search =  \"metadata\" , mode =  \"row\" ) $ val }   sub_names -   feature_names [  chosen ] ## Compute expression values and reshape them for ggplot.  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top_pctage -    100 *   sum (  sub_ave ) /  total_exprs   sub_mat -   100 *   sweep (  sub_mat ,  2 ,   colSums2 (  exprs_mat ) ,  \"/\" , check.margin =  FALSE ) }   ordered_names -   factor (  sub_names ,   rev (  sub_names ) ) # rev() so that most highly expressed is last (i.e., highest y-axis).   df_exprs_by_cell_long -   data.frame ( Cell =   rep (   seq_len (   ncol (  sub_mat ) ) , each =   nrow (  sub_mat ) ) , Tag =   rep (  ordered_names ,   ncol (  sub_mat ) ) , value =   as.numeric (  sub_mat ) # column major collapse. ) ## Colouring the individual dashes for the cells.  if (   missing (  colour_cells_by ) )  {   colour_cells_by -   .qc_hunter (  object ,   paste0 (  \"total_features_by_\" ,  exprs_values ) , mode =  \"column\" ) }  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   .choose_vis_values (  object ,  colour_cells_by , mode =  \"column\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )  if (  as_percentage )  {   plot_most_expressed -    plot_most_expressed +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top_pctage , digits =  3 ) ,  \"% of total\" ) ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) } else  {   plot_most_expressed -   plot_most_expressed +   xlab (  exprs_values ) }   plot_most_expressed -     plot_most_expressed +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (  !   is.null (  colour_cells_by ) )  {  if (  !   is.numeric (   df_exprs_by_cell_long $ colour_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } else  {   plot_most_expressed -   plot_most_expressed +   scale_color_gradient ( name =  colour_cells_by , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) } } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =  ordered_names )  if (  as_percentage )  {    df_to_plot $ pct_total -    100 *  sub_ave /  total_exprs   legend_val -  \"pct_total\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  sub_ave   legend_val -   sprintf (  \"ave_%s\" ,  exprs_values ) } ## Check if is_feature_control is defined, and using it for colouring of the points.  if (   missing (  controls ) )  {   controls -   .qc_hunter (  object ,  \"is_feature_control\" , mode =  \"row\" ) }  if (  !   is.null (  controls ) )  {   cont_out -   .choose_vis_values (  object ,  controls , mode =  \"row\" , search =  \"metadata\" )    df_to_plot $ is_feature_control -    cont_out $ val [  chosen ]   plot_most_expressed -     plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =  df_to_plot , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } else  {   plot_most_expressed -   plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" ) , data =  df_to_plot , fill =  \"grey80\" , colour =  \"grey30\" , shape =  21 ) }  plot_most_expressed } ",
    "filename": "plotHighestExprs.txt"
  },
  "new_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , colour_cells_by = NULL , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{ ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   rowSums2 (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   oo -   setdiff (  oo ,  to_discard ) }   chosen -   head (  oo ,  n )   sub_mat -   exprs_mat [  chosen , , drop =  FALSE ]   sub_ave -   ave_exprs [  chosen ] ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object )  if (   is.null (  feature_names ) )  {   feature_names -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) } } else  {   feature_names -    retrieveFeatureInfo (  object ,  feature_names_to_plot , search =  \"rowData\" ) $ val }   sub_names -   feature_names [  chosen ] ## Compute expression values and reshape them for ggplot.  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top_pctage -    100 *   sum (  sub_ave ) /  total_exprs   sub_mat -   100 *   sweep (  sub_mat ,  2 ,   colSums2 (  exprs_mat ) ,  \"/\" , check.margin =  FALSE ) }   ordered_names -   factor (  sub_names ,   rev (  sub_names ) ) # rev() so that most highly expressed is last (i.e., highest y-axis).   df_exprs_by_cell_long -   data.frame ( Cell =   rep (   seq_len (   ncol (  sub_mat ) ) , each =   nrow (  sub_mat ) ) , Tag =   rep (  ordered_names ,   ncol (  sub_mat ) ) , value =   as.numeric (  sub_mat ) # column major collapse. ) ## Colouring the individual dashes for the cells.  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   retrieveCellInfo (  object ,  colour_cells_by , exprs_values =  by_exprs_values )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )   plot_most_expressed -      plot_most_expressed +   xlab (  exprs_values ) +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) )  if (  !   is.null (  colour_cells_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =  ordered_names )  if (  as_percentage )  {    df_to_plot $ pct_total -    100 *  sub_ave /  total_exprs   legend_val -  \"pct_total\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  sub_ave   legend_val -   sprintf (  \"ave_%s\" ,  exprs_values ) }   aes -   aes_string ( x =  legend_val , y =  \"Feature\" )   plot_most_expressed +   geom_point (  aes , data =  df_to_plot , colour =  \"gray30\" , shape =  21 ) } ",
    "filename": "plotHighestExprs.txt"
  }
}



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 5 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL , min_diff = NA )",
    "body": "{  if (  log )  {   metric -   log10 (  metric ) }  if (   any (   is.na (  metric ) ) )  {   warning (  \"missing values ignored during outlier detection\" ) }  if (  !   is.null (  batch ) )  {   N -   length (  metric )  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) } # Coercing non-NULL subset into a logical vector.  if (  !   is.null (  subset ) )  {   new.subset -   logical (  N )    names (  new.subset ) -   names (  metric )    new.subset [  subset ] -  TRUE   subset -  new.subset } # Computing QC metrics for each batch.   by.batch -   split (   seq_len (  N ) ,  batch )   collected -   logical (  N )   all.threshold -   vector (  \"list\" ,   length (  by.batch ) )  for  ( b in   seq_along (  by.batch ) )  {   bdx -   by.batch [[  b ] ]   current -   Recall (   metric [  bdx ] , nmads =  nmads , type =  type , log =  FALSE , subset =   subset [  bdx ] , batch =  NULL , min_diff =  min_diff )    all.threshold [[  b ] ] -   .get_thresholds (  current )    collected [  bdx ] -  current }   all.threshold -   do.call (  cbind ,  all.threshold )    colnames (  all.threshold ) -   names (  by.batch )   return (   .store_thresholds (  collected ,  all.threshold , logged =  log ) ) } # Computing median/MAD (possibly based on subset of the data).  if (  !   is.null (  subset ) )  {   submetric -   metric [  subset ]  if (    length (  submetric ) ==  0L )  {   warning (  \"no observations remaining after subsetting\" ) } } else  {   submetric -  metric }   cur.med -   median (  submetric , na.rm =  TRUE )   cur.mad -   mad (  submetric , center =  cur.med , na.rm =  TRUE )   diff.val -   max (  min_diff ,   nmads *  cur.mad , na.rm =  TRUE )   upper.limit -   cur.med +  diff.val   lower.limit -   cur.med -  diff.val   type -   match.arg (  type )  if (   type ==  \"lower\" )  {   upper.limit -  Inf } else  if (   type ==  \"higher\" )  {   lower.limit -  -  Inf }   .store_thresholds (  (    metric undefined  lower.limit |   upper.limit undefined  metric ) ,   c ( lower =  lower.limit , higher =  upper.limit ) , logged =  log ) } ",
    "filename": "isOutlier.txt"
  },
  "new_function": {
    "name": "isOutlier",
    "representation": "isOutlier",
    "parameters": "function ( metric , nmads = 3 , type = c ( \"both\" , \"lower\" , \"higher\" ) , log = FALSE , subset = NULL , batch = NULL , share_medians = FALSE , share_mads = FALSE , share_missing = TRUE , min_diff = NA )",
    "body": "{  if (  log )  {   metric -   log2 (  metric ) }   N -   length (  metric )  if (   nobatch -   is.null (  batch ) )  {   batch -   rep (  \"1\" ,  N ) } else  {  if (    length (  batch ) !=  N )  {   stop (  \"length of 'batch' must equal length of 'metric'\" ) }   batch -   as.character (  batch ) }   all_batches -   sort (   unique (  batch ) ) # Subsetting by user-specific factor or by non-NA.  if (  !   is.null (  subset ) )  {   M -   metric [  subset ]   B -   batch [  subset ] } else  {   M -  metric   B -  batch }  if (   any (   na.drop -   is.na (  M ) ) )  {   M -   M [  !  na.drop ]   B -   B [  !  na.drop ]   warning (  \"missing values ignored during outlier detection\" ) } # Defining the mad or median, possibly by sharing information across batches.   by.batch -   split (  M ,  B )   empty -   rep (  NA_real_ ,   length (  all_batches ) )    names (  empty ) -  all_batches   cur.med -  empty  if (  !  share_medians )  {    cur.med [   names (  by.batch ) ] -   unlist (   lapply (  by.batch ,  median ,  0 ) ) # handles no-batch input better than vapply. }   replace -   .determine_sharingness (  share_medians ,  share_missing ,  all_batches ,   names (  by.batch ) )  if (   length (  replace ) )  {    cur.med [  replace ] -   median (  M ) }   cur.mad -  empty  if (  !  share_mads )  {    cur.mad [   names (  by.batch ) ] -   unlist (   mapply (  mad , x =  by.batch , center =   cur.med [   names (  by.batch ) ] , SIMPLIFY =  FALSE ) ) }   replace -   .determine_sharingness (  share_mads ,  share_missing ,  all_batches ,   names (  by.batch ) )  if (   length (  replace ) )  {    cur.mad [  replace ] -    median (   abs (   M -   cur.med [  B ] ) ) *    formals (  mad ) $ constant } # Defining the thresholds.   diff.val -   pmax (  min_diff ,   nmads *  cur.mad , na.rm =  TRUE )   upper.limit -   cur.med +  diff.val   lower.limit -   cur.med -  diff.val   type -   match.arg (  type )  if (   type ==  \"lower\" )  {    upper.limit [ ] -  Inf } else  if (   type ==  \"higher\" )  {    lower.limit [ ] -  -  Inf }   collected -  (    metric undefined   lower.limit [  batch ] |    upper.limit [  batch ] undefined  metric )    names (  collected ) -   names (  metric ) # Formatting output.   all.threshold -   rbind ( lower =  lower.limit , higher =  upper.limit )  if (  nobatch )  {   all.threshold -   drop (  all.threshold ) }  if (  log )  {   all.threshold -   2 ^  all.threshold }    attr (  collected ,  \"thresholds\" ) -  all.threshold  collected } ",
    "filename": "isOutlier.txt"
  }
}

1.
{
  "old_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , controls , colour_cells_by , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , by_show_single = TRUE , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{ ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   rowSums2 (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   oo -   setdiff (  oo ,  to_discard ) }   chosen -   head (  oo ,  n )   sub_mat -   exprs_mat [  chosen , , drop =  FALSE ]   sub_ave -   ave_exprs [  chosen ] ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object )  if (   is.null (  feature_names ) )  {   feature_names -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) } } else  {   feature_names -    .choose_vis_values (  object ,  feature_names_to_plot , search =  \"metadata\" , mode =  \"row\" ) $ val }   sub_names -   feature_names [  chosen ] ## Compute expression values and reshape them for ggplot.  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top_pctage -    100 *   sum (  sub_ave ) /  total_exprs   sub_mat -   100 *   sweep (  sub_mat ,  2 ,   colSums2 (  exprs_mat ) ,  \"/\" , check.margin =  FALSE ) }   ordered_names -   factor (  sub_names ,   rev (  sub_names ) ) # rev() so that most highly expressed is last (i.e., highest y-axis).   df_exprs_by_cell_long -   data.frame ( Cell =   rep (   seq_len (   ncol (  sub_mat ) ) , each =   nrow (  sub_mat ) ) , Tag =   rep (  ordered_names ,   ncol (  sub_mat ) ) , value =   as.numeric (  sub_mat ) # column major collapse. ) ## Colouring the individual dashes for the cells.  if (   missing (  colour_cells_by ) )  {   colour_cells_by -   .qc_hunter (  object ,   paste0 (  \"total_features_by_\" ,  exprs_values ) , mode =  \"column\" ) }  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   .choose_vis_values (  object ,  colour_cells_by , mode =  \"column\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )  if (  as_percentage )  {   plot_most_expressed -    plot_most_expressed +   ggtitle (   paste0 (  \"Top \" ,  n ,  \" account for \" ,   format (  top_pctage , digits =  3 ) ,  \"% of total\" ) ) +   xlab (   paste0 (  \"% of total \" ,  exprs_values ) ) } else  {   plot_most_expressed -   plot_most_expressed +   xlab (  exprs_values ) }   plot_most_expressed -     plot_most_expressed +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) ) ## Sort of colouring of points  if (  !   is.null (  colour_cells_by ) )  {  if (  !   is.numeric (   df_exprs_by_cell_long $ colour_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } else  {   plot_most_expressed -   plot_most_expressed +   scale_color_gradient ( name =  colour_cells_by , low =  \"lightgoldenrod\" , high =  \"firebrick4\" , space =  \"Lab\" ) } } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =  ordered_names )  if (  as_percentage )  {    df_to_plot $ pct_total -    100 *  sub_ave /  total_exprs   legend_val -  \"pct_total\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  sub_ave   legend_val -   sprintf (  \"ave_%s\" ,  exprs_values ) } ## Check if is_feature_control is defined, and using it for colouring of the points.  if (   missing (  controls ) )  {   controls -   .qc_hunter (  object ,  \"is_feature_control\" , mode =  \"row\" ) }  if (  !   is.null (  controls ) )  {   cont_out -   .choose_vis_values (  object ,  controls , mode =  \"row\" , search =  \"metadata\" )    df_to_plot $ is_feature_control -    cont_out $ val [  chosen ]   plot_most_expressed -     plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" , fill =  \"is_feature_control\" ) , data =  df_to_plot , colour =  \"gray30\" , shape =  21 ) +   scale_fill_manual ( values =   c (  \"aliceblue\" ,  \"wheat\" ) ) +   guides ( fill =   guide_legend ( title =  \"Feature control?\" ) ) } else  {   plot_most_expressed -   plot_most_expressed +   geom_point (   aes_string ( x =  legend_val , y =  \"Feature\" ) , data =  df_to_plot , fill =  \"grey80\" , colour =  \"grey30\" , shape =  21 ) }  plot_most_expressed } ",
    "filename": "plotHighestExprs.txt"
  },
  "new_function": {
    "name": "plotHighestExprs",
    "representation": "plotHighestExprs",
    "parameters": "function ( object , n = 50 , colour_cells_by = NULL , drop_features = NULL , exprs_values = \"counts\" , by_exprs_values = exprs_values , feature_names_to_plot = NULL , as_percentage = TRUE )",
    "body": "{ ## Find the most highly expressed features in this dataset   exprs_mat -   assay (  object ,  exprs_values , withDimnames =  FALSE )   ave_exprs -   rowSums2 (  exprs_mat )   oo -   order (  ave_exprs , decreasing =  TRUE )  if (  !   is.null (  drop_features ) )  {   to_discard -   .subset2index (  drop_features ,  object , byrow =  TRUE )   oo -   setdiff (  oo ,  to_discard ) }   chosen -   head (  oo ,  n )   sub_mat -   exprs_mat [  chosen , , drop =  FALSE ]   sub_ave -   ave_exprs [  chosen ] ## define feature names for plot  if (   is.null (  feature_names_to_plot ) )  {   feature_names -   rownames (  object )  if (   is.null (  feature_names ) )  {   feature_names -   sprintf (  \"Feature %i\" ,   seq_len (   nrow (  object ) ) ) } } else  {   feature_names -    retrieveFeatureInfo (  object ,  feature_names_to_plot , search =  \"rowData\" ) $ val }   sub_names -   feature_names [  chosen ] ## Compute expression values and reshape them for ggplot.  if (  as_percentage )  {   total_exprs -   sum (  ave_exprs )   top_pctage -    100 *   sum (  sub_ave ) /  total_exprs   sub_mat -   100 *   sweep (  sub_mat ,  2 ,   colSums2 (  exprs_mat ) ,  \"/\" , check.margin =  FALSE ) }   ordered_names -   factor (  sub_names ,   rev (  sub_names ) ) # rev() so that most highly expressed is last (i.e., highest y-axis).   df_exprs_by_cell_long -   data.frame ( Cell =   rep (   seq_len (   ncol (  sub_mat ) ) , each =   nrow (  sub_mat ) ) , Tag =   rep (  ordered_names ,   ncol (  sub_mat ) ) , value =   as.numeric (  sub_mat ) # column major collapse. ) ## Colouring the individual dashes for the cells.  if (  !   is.null (  colour_cells_by ) )  {   colour_out -   retrieveCellInfo (  object ,  colour_cells_by , exprs_values =  by_exprs_values )   colour_cells_by -   colour_out $ name    df_exprs_by_cell_long $ colour_by -    colour_out $ val [   df_exprs_by_cell_long $ Cell ]   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" , colour =  \"colour_by\" ) } else  {   aes_to_use -   aes_string ( y =  \"Tag\" , x =  \"value\" ) } ## Create the plot and annotations.   plot_most_expressed -    ggplot (  df_exprs_by_cell_long ,  aes_to_use ) +   geom_point ( alpha =  0.6 , shape =  124 )   plot_most_expressed -      plot_most_expressed +   xlab (  exprs_values ) +   ylab (  \"Feature\" ) +   theme_bw (  8 ) +   theme ( legend.position =   c (  1 ,  0 ) , legend.justification =   c (  1 ,  0 ) , axis.text.x =   element_text ( colour =  \"gray35\" ) , axis.text.y =   element_text ( colour =  \"gray35\" ) , axis.title.x =   element_text ( colour =  \"gray35\" ) , axis.title.y =   element_text ( colour =  \"gray35\" ) , title =   element_text ( colour =  \"gray35\" ) )  if (  !   is.null (  colour_cells_by ) )  {   plot_most_expressed -   .resolve_plot_colours (  plot_most_expressed ,   df_exprs_by_cell_long $ colour_by ,  colour_cells_by ) } ## Adding median expression values for each gene.   df_to_plot -   data.frame ( Feature =  ordered_names )  if (  as_percentage )  {    df_to_plot $ pct_total -    100 *  sub_ave /  total_exprs   legend_val -  \"pct_total\" } else  {    df_to_plot [[   paste0 (  \"ave_\" ,  exprs_values ) ] ] -  sub_ave   legend_val -   sprintf (  \"ave_%s\" ,  exprs_values ) }   aes -   aes_string ( x =  legend_val , y =  \"Feature\" )   plot_most_expressed +   geom_point (  aes , data =  df_to_plot , colour =  \"gray30\" , shape =  21 ) } ",
    "filename": "plotHighestExprs.txt"
  }
}

2.
{
  "old_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , size_by = NULL , shape_by = NULL , by_exprs_values = \"logcounts\" , by_show_single = FALSE , add_legend = TRUE , theme_size = 24 , point_alpha = 0.6 , point_size = 24 )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"Object must be of class SingleCellExperiment\" ) } ## obtain well positions  if (  !   is.list (  plate_position ) )  {  if (   is.null (  plate_position ) )  {   plate_position -   object $ plate_position }  if (   any (  !   grepl (  \"^[A-Z][0-9]+$\" ,  plate_position ) ) )  {   stop (  \"invalid format specified in 'plate_position'\" ) }   y_position -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position )   x_position -   as.integer (   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position ) ) } else  {   x_position -   as.integer (   plate_position $ column )   y_position -   as.character (   plate_position $ row )   plate_position -  NULL }   x_position -   as.factor (  x_position )   y_position -   factor (  y_position , levels =   rev (  LETTERS ) ) # Up-   df_to_plot -   data.frame ( X =  x_position , Y =  y_position ) ## checking visualization arguments   vis_out -   .incorporate_common_vis (  df_to_plot , se =  object , mode =  \"column\" , colour_by =  colour_by , shape_by =  shape_by , size_by =  size_by , by_exprs_values =  by_exprs_values , by_show_single =  by_show_single )   df_to_plot -   vis_out $ df   colour_by -   vis_out $ colour_by   shape_by -   vis_out $ shape_by   size_by -   vis_out $ size_by ## make the plot with appropriate colours.   plot_out -   ggplot (  df_to_plot ,   aes_string ( x =  \"X\" , y =  \"Y\" ) )   point_out -   .get_point_args (  colour_by ,  shape_by ,  size_by , alpha =  point_alpha , size =  point_size )   plot_out -   plot_out +   do.call (  geom_point ,   point_out $ args )  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =   point_out $ fill ) } ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired   plot_out -   .add_extra_guide (  plot_out ,  shape_by ,  size_by )  if (  !  add_legend )  {   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) }  plot_out } ",
    "filename": "plotPlatePosition.txt"
  },
  "new_function": {
    "name": "plotPlatePosition",
    "representation": "plotPlatePosition",
    "parameters": "function ( object , plate_position = NULL , colour_by = NULL , size_by = NULL , shape_by = NULL , by_exprs_values = \"logcounts\" , add_legend = TRUE , theme_size = 24 , point_alpha = 0.6 , point_size = 24 , other_fields = list ( ) )",
    "body": "{ ## check object is SingleCellExperiment object  if (  !   is (  object ,  \"SingleCellExperiment\" ) )  {   stop (  \"Object must be of class SingleCellExperiment\" ) } ## obtain well positions  if (  !   is.list (  plate_position ) )  {  if (   is.null (  plate_position ) )  {   plate_position -   object $ plate_position }  if (   any (  !   grepl (  \"^[A-Z][0-9]+$\" ,  plate_position ) ) )  {   stop (  \"invalid format specified in 'plate_position'\" ) }   y_position -   gsub (  \"[0-9]*\" ,  \"\" ,  plate_position )   x_position -   as.integer (   gsub (  \"[A-Z]*\" ,  \"\" ,  plate_position ) ) } else  {   x_position -   as.integer (   plate_position $ column )   y_position -   as.character (   plate_position $ row )   plate_position -  NULL }   x_position -   as.factor (  x_position )   y_position -   factor (  y_position , levels =   rev (  LETTERS ) ) # Up-   df_to_plot -   data.frame ( X =  x_position , Y =  y_position ) ## checking visualization arguments   vis_out -   .incorporate_common_vis_col (  df_to_plot , se =  object , colour_by =  colour_by , shape_by =  shape_by , size_by =  size_by , by_exprs_values =  by_exprs_values , other_fields =  other_fields )   df_to_plot -   vis_out $ df   colour_by -   vis_out $ colour_by   shape_by -   vis_out $ shape_by   size_by -   vis_out $ size_by ## make the plot with appropriate colours.   plot_out -   ggplot (  df_to_plot ,   aes_string ( x =  \"X\" , y =  \"Y\" ) )   point_out -   .get_point_args (  colour_by ,  shape_by ,  size_by , alpha =  point_alpha , size =  point_size )   plot_out -   plot_out +   do.call (  geom_point ,   point_out $ args )  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   df_to_plot $ colour_by ,  colour_by , fill =   point_out $ fill ) } ## Define plotting theme   plot_out -     plot_out +   theme_bw (  theme_size ) +   theme ( axis.title =   element_blank ( ) , axis.ticks =   element_blank ( ) , legend.text =   element_text ( size =   theme_size /  2 ) , legend.title =   element_text ( size =   theme_size /  2 ) ) +   guides ( fill =   guide_legend ( override.aes =   list ( size =   theme_size /  2 ) ) ) ## remove legend if so desired   plot_out -   .add_extra_guide (  plot_out ,  shape_by ,  size_by )  if (  !  add_legend )  {   plot_out -   plot_out +   theme ( legend.position =  \"none\" ) }  plot_out } ",
    "filename": "plotPlatePosition.txt"
  }
}

3.
{
  "old_function": {
    "name": "plotScater",
    "representation": "plotScater",
    "parameters": "function ( x , nfeatures = 500 , exprs_values = \"counts\" , colour_by = NULL , by_exprs_values = exprs_values , by_show_single = FALSE , block1 = NULL , block2 = NULL , ncol = 3 , line_width = 1.5 , theme_size = 10 )",
    "body": "{  if (  !   is (  x ,  \"SingleCellExperiment\" ) )  {   stop (  \"x must be of class SingleCellExperiment\" ) }   block1_out -   .choose_vis_values (  x ,  block1 , mode =  \"column\" , search =  \"metadata\" )   block1 -   block1_out $ name   block1_vals -   block1_out $ val   block2_out -   .choose_vis_values (  x ,  block2 , mode =  \"column\" , search =  \"metadata\" )   block2 -   block2_out $ name   block2_vals -   block2_out $ val ## Setting values to colour by.   colour_by_out -   .choose_vis_values (  x ,  colour_by , mode =  \"column\" , search =  \"any\" , exprs_values =  by_exprs_values , discard_solo =  !  by_show_single )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## Define an expression matrix depending on which values we're using   exprs_mat -   assay (  x , i =  exprs_values , withDimnames =  FALSE )   nfeatures -   min (  nfeatures ,   nrow (  exprs_mat ) ) ## Use C++ to get the sequencing real estate accounted for by features   to_plot -   seq_len (  nfeatures )   ncells -   ncol (  exprs_mat )   seq_real_estate -   .Call (  cxx_top_cumprop ,  exprs_mat ,  to_plot )   seq_real_estate_long -   data.frame ( Feature =   rep (  to_plot , each =  ncells ) , Cell =   rep (   seq_len (  ncells ) ,  nfeatures ) )    seq_real_estate_long $ Proportion_Library -   as.vector (   t (  seq_real_estate ) ) ## Add block and colour_by information if provided    seq_real_estate_long $ block1 -   rep (  block1_vals ,  nfeatures )    seq_real_estate_long $ block2 -   rep (  block2_vals ,  nfeatures )    seq_real_estate_long $ colour_by -   rep (  colour_by_vals ,  nfeatures ) ## Set up plot   aes -   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" )  if (  !   is.null (  colour_by ) )  {    aes $ colour -   as.symbol (  \"colour_by\" ) }   plot_out -    ggplot (  seq_real_estate_long ,  aes ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  line_width ) ## Deal with blocks for grid  if (   !   is.null (  block1 ) undefined  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_grid (   block2 ~  block1 ) } else  {  if (  !   is.null (  block1 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block1 , ncol =  ncol ) } else  if (  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block2 , ncol =  ncol ) } } ## Add extra plot theme and details  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   seq_real_estate_long $ colour_by ,  colour_by ) }   plot_out -    plot_out +   xlab (  \"Number of features\" ) +   ylab (  \"Cumulative proportion of library\" )  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )  {   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) } else  {   plot_out -   plot_out +   theme_bw (  theme_size ) }  plot_out } ",
    "filename": "plotScater.txt"
  },
  "new_function": {
    "name": "plotScater",
    "representation": "plotScater",
    "parameters": "function ( x , nfeatures = 500 , exprs_values = \"counts\" , colour_by = NULL , by_exprs_values = exprs_values , block1 = NULL , block2 = NULL , ncol = 3 , line_width = 1.5 , theme_size = 10 )",
    "body": "{  if (  !   is (  x ,  \"SingleCellExperiment\" ) )  {   stop (  \"x must be of class SingleCellExperiment\" ) }   block1_out -   retrieveCellInfo (  x ,  block1 , search =  \"colData\" )   block1 -   block1_out $ name   block1_vals -   block1_out $ val   block2_out -   retrieveCellInfo (  x ,  block2 , search =  \"colData\" )   block2 -   block2_out $ name   block2_vals -   block2_out $ val ## Setting values to colour by.   colour_by_out -   retrieveCellInfo (  x ,  colour_by , exprs_values =  by_exprs_values )   colour_by -   colour_by_out $ name   colour_by_vals -   colour_by_out $ val ## Define an expression matrix depending on which values we're using   exprs_mat -   assay (  x , i =  exprs_values , withDimnames =  FALSE )   nfeatures -   min (  nfeatures ,   nrow (  exprs_mat ) ) ## Use C++ to get the sequencing real estate accounted for by features   to_plot -   seq_len (  nfeatures )   ncells -   ncol (  exprs_mat )   seq_real_estate -   top_cumprop (  exprs_mat ,  to_plot )   seq_real_estate_long -   data.frame ( Feature =   rep (  to_plot , each =  ncells ) , Cell =   rep (   seq_len (  ncells ) ,  nfeatures ) )    seq_real_estate_long $ Proportion_Library -   as.vector (   t (  seq_real_estate ) ) ## Add block and colour_by information if provided    seq_real_estate_long $ block1 -   rep (  block1_vals ,  nfeatures )    seq_real_estate_long $ block2 -   rep (  block2_vals ,  nfeatures )    seq_real_estate_long $ colour_by -   rep (  colour_by_vals ,  nfeatures ) ## Set up plot   aes -   aes_string ( x =  \"Feature\" , y =  \"Proportion_Library\" , group =  \"Cell\" )  if (  !   is.null (  colour_by ) )  {    aes $ colour -   as.symbol (  \"colour_by\" ) }   plot_out -    ggplot (  seq_real_estate_long ,  aes ) +   geom_line ( linetype =  \"solid\" , alpha =  0.3 , size =  line_width ) ## Deal with blocks for grid  if (   !   is.null (  block1 ) undefined  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_grid (   block2 ~  block1 ) } else  {  if (  !   is.null (  block1 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block1 , ncol =  ncol ) } else  if (  !   is.null (  block2 ) )  {   plot_out -   plot_out +   facet_wrap (  ~  block2 , ncol =  ncol ) } } ## Add extra plot theme and details  if (  !   is.null (  colour_by ) )  {   plot_out -   .resolve_plot_colours (  plot_out ,   seq_real_estate_long $ colour_by ,  colour_by ) }   plot_out -    plot_out +   xlab (  \"Number of features\" ) +   ylab (  \"Cumulative proportion of library\" )  if (   requireNamespace (  \"cowplot\" , quietly =  TRUE ) )  {   plot_out -   plot_out +   cowplot :: theme_cowplot (  theme_size ) } else  {   plot_out -   plot_out +   theme_bw (  theme_size ) }  plot_out } ",
    "filename": "plotScater.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_11 scater_release_3_12

{
    "package": "scater",
    "release_versions": "scater_release_3_11 scater_release_3_12",
    "desc_release_old": "1.16.2",
    "desc_release_new": "1.18.6",
    "old_release_number": 7,
    "new_release_number": 8,
    "function_removals": 6,
    "function_additions": 3,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 2,
    "total_count": 8
}

##########
Functions Removed
##########

.assignIndicesToWorkers
.bpNotSharedOrUp
.splitColsByWorkers
.splitRowsByWorkers
.splitVectorByWorkers
.subset2index


##########
Functions Added
##########

batchCorrectedAverages
calculateMultiUMAP
plotGroupedHeatmap


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "getVarianceExplained",
    "representation": "getVarianceExplained",
    "signature": "ANY",
    "parameters": "function ( x , variables , subset_row = NULL , chunk = 1000 )",
    "body": "{ # Chunk-wise processing to keep memory usage low.   subset_row -   .subset2index (  subset_row ,  x , byrow =  TRUE )   ngenes -   length (  subset_row )  if (   ngenes undefined  chunk )  {   by.chunk -   cut (   seq_len (  ngenes ) ,   ceiling (   ngenes /  chunk ) ) } else  {   by.chunk -   factor (   integer (  ngenes ) ) } # Initialise matrix to store R^2 values for each feature for each variable   rsquared_mat -   matrix (  NA_real_ ,  ngenes ,   ncol (  variables ) , dimnames =   list (    rownames (  x ) [  subset_row ] ,   colnames (  variables ) ) )   tss.all -    rowVars (   DelayedArray (  x ) , rows =  subset_row ) *  (    ncol (  x ) -  1 ) # Get R^2 values for each feature and each variable  for  ( V in   colnames (  variables ) )  {   curvar -   variables [[  V ] ]  if (    length (   unique (  curvar ) ) =  1L )  {   warning (   sprintf (  \"ignoring '%s' with fewer than 2 unique levels\" ,  V ) )  next } # Protect against NAs in the metadata.   keep -  !   is.na (  curvar )  if (   all (  keep ) )  {   tss -  tss.all } else  {   curvar -   curvar [  keep ]   tss -    rowVars (   DelayedArray (  x ) , rows =  subset_row , cols =  keep ) *  (    sum (  keep ) -  1 ) }   design -   model.matrix (  ~  curvar )   QR -   qr (  design )   rss -   numeric (  ngenes )  for  ( element in   levels (  by.chunk ) )  {   chunked -   by.chunk ==  element   cur.exprs -   x [   subset_row [  chunked ] ,  keep , drop =  FALSE ]   effects -   qr.qty (  QR ,   as.matrix (   t (  cur.exprs ) ) ) # no need for special colSums, as this is always dense.    rss [  chunked ] -   colSums (    effects [  -   seq_len (   QR $ rank ) , , drop =  FALSE ] ^  2 ) }    rsquared_mat [ ,  V ] -   1 -   rss /  tss }   rsquared_mat *  100 } ",
    "replacementFunction": ".get_variance_explained",
    "filename": "getVarianceExplained.txt"
  },
  "new_function": {
    "name": "getVarianceExplained",
    "representation": "getVarianceExplained",
    "signature": "ANY",
    "parameters": "function ( x , variables , subset_row = NULL )",
    "body": "{   subset_row -   .subset2index (  subset_row ,  x , byrow =  TRUE )   ngenes -   length (  subset_row )   rsquared_mat -   matrix (  NA_real_ ,  ngenes ,   ncol (  variables ) , dimnames =   list (    rownames (  x ) [  subset_row ] ,   colnames (  variables ) ) )   tss.all -    rowVars (   DelayedArray (  x ) , rows =  subset_row ) *  (    ncol (  x ) -  1 ) # Get R^2 values for each feature and each variable  for  ( V in   colnames (  variables ) )  {   curvar -   variables [[  V ] ]  if (    length (   unique (  curvar ) ) =  1L )  {   warning (   sprintf (  \"ignoring '%s' with fewer than 2 unique levels\" ,  V ) )  next } # Protect against NAs in the metadata.   keep -  !   is.na (  curvar )  if (   all (  keep ) )  {   tss -  tss.all   y -  x } else  {   curvar -   curvar [  keep ]   y -   x [ ,  keep , drop =  FALSE ]   tss -    rowVars (   DelayedArray (  x ) , rows =  subset_row , cols =  keep ) *  (    sum (  keep ) -  1 ) }   design -   model.matrix (  ~  curvar )   fit -   fitLinearModel (  y ,  design , subset.row =  subset_row )   rss -    fit $ variance *  (    nrow (  design ) -   ncol (  design ) )    rsquared_mat [ ,  V ] -   1 -   rss /  tss }   rsquared_mat *  100 } ",
    "replacementFunction": ".get_variance_explained",
    "filename": "getVarianceExplained.txt"
  }
}

1.
{
  "old_function": {
    "name": "plotDots",
    "representation": "plotDots",
    "parameters": "function ( object , features , group = NULL , exprs_values = \"logcounts\" , detection_limit = 0 , low_color = \"white\" , high_color = \"red\" , max_ave = NULL , max_detected = NULL , other_fields = list ( ) , by_exprs_values = exprs_values )",
    "body": "{  if (   is.null (  group ) )  {   group -   rep (  \"all\" ,   ncol (  object ) ) } else  {   group -    retrieveCellInfo (  object ,  group , search =  \"colData\" ) $ value }   group -   factor (  group )   num -   assay (   numDetectedAcrossCells (  object , ids =  group , subset_row =  features , exprs_values =  exprs_values , average =  TRUE , detection_limit =  detection_limit ) )   ave -   assay (   sumCountsAcrossCells (  object , ids =  group , subset_row =  features , exprs_values =  exprs_values , average =  TRUE ) ) # Creating a long-form table.   evals_long -   data.frame ( Feature =   rep (  features ,   ncol (  num ) ) , Group =   rep (   colnames (  num ) , each =   nrow (  num ) ) , NumDetected =   as.numeric (  num ) , Average =   as.numeric (  ave ) )  if (  !   is.null (  max_detected ) )  {    evals_long $ NumDetected -   pmin (  max_detected ,   evals_long $ NumDetected ) }  if (  !   is.null (  max_ave ) )  {    evals_long $ Average -   pmin (  max_ave ,   evals_long $ Average ) } # Adding other fields, if requested.   vis_out -   .incorporate_common_vis_row (  evals_long , se =  object , colour_by =  NULL , shape_by =  NULL , size_by =  NULL , by_exprs_values =  by_exprs_values , other_fields =  other_fields , multiplier =   rep (   .subset2index (  features ,  object ) ,   ncol (  num ) ) )   evals_long -   vis_out $ df       ggplot (  evals_long ) +   geom_point (   aes_string ( x =  \"Group\" , y =  \"Feature\" , size =  \"NumDetected\" , col =  \"Average\" ) ) +   scale_size ( limits =   c (  0 ,   max (   evals_long $ NumDetected ) ) ) +   scale_color_gradient ( limits =   c (  detection_limit ,   max (   evals_long $ Average ) ) , low =  low_color , high =  high_color ) +   theme ( panel.background =   element_rect ( fill =  low_color ) , panel.grid.major =   element_line ( size =  0.5 , colour =  \"grey80\" ) , panel.grid.minor =   element_line ( size =  0.25 , colour =  \"grey80\" ) ) } ",
    "filename": "plotDots.txt"
  },
  "new_function": {
    "name": "plotDots",
    "representation": "plotDots",
    "parameters": "function ( object , features , group = NULL , block = NULL , exprs_values = \"logcounts\" , detection_limit = 0 , low_color = \"white\" , high_color = \"red\" , max_ave = NULL , max_detected = NULL , other_fields = list ( ) , by_exprs_values = exprs_values , swap_rownames = NULL )",
    "body": "{  if (   is.null (  group ) )  {   group -   rep (  \"all\" ,   ncol (  object ) ) } else  {   group -    retrieveCellInfo (  object ,  group , search =  \"colData\" ) $ value }   feature_names -   .swap_rownames (  object ,  features ,  swap_rownames )   group -   factor (  group ) # Computing, possibly also batch correcting.   ids -   DataFrame ( group =  group )  if (  !   is.null (  block ) )  {    ids $ block -    retrieveCellInfo (  object ,  block , search =  \"colData\" ) $ value }   summarized -   summarizeAssayByGroup (   assay (  object ,  exprs_values ) , ids =  ids , subset.row =  feature_names , statistics =   c (  \"mean\" ,  \"prop.detected\" ) , threshold =  detection_limit )   ave -   assay (  summarized ,  \"mean\" )   num -   assay (  summarized ,  \"prop.detected\" )   group.names -   summarized $ group  if (  !   is.null (  block ) )  {   ave -   batchCorrectedAverages (  ave , group =   summarized $ group , block =   summarized $ block )   num -   batchCorrectedAverages (  num , group =   summarized $ group , block =   summarized $ block , transform =  \"logit\" )   group.names -   colnames (  ave ) } # Creating a long-form table.   evals_long -   data.frame ( Feature =   rep (  features ,   ncol (  num ) ) , Group =   rep (  group.names , each =   nrow (  num ) ) , NumDetected =   as.numeric (  num ) , Average =   as.numeric (  ave ) )  if (  !   is.null (  max_detected ) )  {    evals_long $ NumDetected -   pmin (  max_detected ,   evals_long $ NumDetected ) }  if (  !   is.null (  max_ave ) )  {    evals_long $ Average -   pmin (  max_ave ,   evals_long $ Average ) } # Adding other fields, if requested.   vis_out -   .incorporate_common_vis_row (  evals_long , se =  object , colour_by =  NULL , shape_by =  NULL , size_by =  NULL , by_exprs_values =  by_exprs_values , other_fields =  other_fields , multiplier =   rep (   .subset2index (  feature_names ,  object ) ,   ncol (  num ) ) )   evals_long -   vis_out $ df       ggplot (  evals_long ) +   geom_point (   aes_string ( x =  \"Group\" , y =  \"Feature\" , size =  \"NumDetected\" , col =  \"Average\" ) ) +   scale_size ( limits =   c (  0 ,   max (   evals_long $ NumDetected ) ) ) +   scale_color_gradient ( limits =   c (  detection_limit ,   max (   evals_long $ Average ) ) , low =  low_color , high =  high_color ) +   theme ( panel.background =   element_rect ( fill =  low_color ) , panel.grid.major =   element_line ( size =  0.5 , colour =  \"grey80\" ) , panel.grid.minor =   element_line ( size =  0.25 , colour =  \"grey80\" ) ) } ",
    "filename": "plotDots.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_12 scater_release_3_13

{
    "package": "scater",
    "release_versions": "scater_release_3_12 scater_release_3_13",
    "desc_release_old": "1.18.6",
    "desc_release_new": "1.20.1",
    "old_release_number": 8,
    "new_release_number": 9,
    "function_removals": 1,
    "function_additions": 0,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 1,
    "total_count": 2
}

##########
Functions Removed
##########

batchCorrectedAverages


##########
Functions Added
##########



##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "plotDots",
    "representation": "plotDots",
    "parameters": "function ( object , features , group = NULL , block = NULL , exprs_values = \"logcounts\" , detection_limit = 0 , low_color = \"white\" , high_color = \"red\" , max_ave = NULL , max_detected = NULL , other_fields = list ( ) , by_exprs_values = exprs_values , swap_rownames = NULL )",
    "body": "{  if (   is.null (  group ) )  {   group -   rep (  \"all\" ,   ncol (  object ) ) } else  {   group -    retrieveCellInfo (  object ,  group , search =  \"colData\" ) $ value }   feature_names -   .swap_rownames (  object ,  features ,  swap_rownames )   group -   factor (  group ) # Computing, possibly also batch correcting.   ids -   DataFrame ( group =  group )  if (  !   is.null (  block ) )  {    ids $ block -    retrieveCellInfo (  object ,  block , search =  \"colData\" ) $ value }   summarized -   summarizeAssayByGroup (   assay (  object ,  exprs_values ) , ids =  ids , subset.row =  feature_names , statistics =   c (  \"mean\" ,  \"prop.detected\" ) , threshold =  detection_limit )   ave -   assay (  summarized ,  \"mean\" )   num -   assay (  summarized ,  \"prop.detected\" )   group.names -   summarized $ group  if (  !   is.null (  block ) )  {   ave -   batchCorrectedAverages (  ave , group =   summarized $ group , block =   summarized $ block )   num -   batchCorrectedAverages (  num , group =   summarized $ group , block =   summarized $ block , transform =  \"logit\" )   group.names -   colnames (  ave ) } # Creating a long-form table.   evals_long -   data.frame ( Feature =   rep (  features ,   ncol (  num ) ) , Group =   rep (  group.names , each =   nrow (  num ) ) , NumDetected =   as.numeric (  num ) , Average =   as.numeric (  ave ) )  if (  !   is.null (  max_detected ) )  {    evals_long $ NumDetected -   pmin (  max_detected ,   evals_long $ NumDetected ) }  if (  !   is.null (  max_ave ) )  {    evals_long $ Average -   pmin (  max_ave ,   evals_long $ Average ) } # Adding other fields, if requested.   vis_out -   .incorporate_common_vis_row (  evals_long , se =  object , colour_by =  NULL , shape_by =  NULL , size_by =  NULL , by_exprs_values =  by_exprs_values , other_fields =  other_fields , multiplier =   rep (   .subset2index (  feature_names ,  object ) ,   ncol (  num ) ) )   evals_long -   vis_out $ df       ggplot (  evals_long ) +   geom_point (   aes_string ( x =  \"Group\" , y =  \"Feature\" , size =  \"NumDetected\" , col =  \"Average\" ) ) +   scale_size ( limits =   c (  0 ,   max (   evals_long $ NumDetected ) ) ) +   scale_color_gradient ( limits =   c (  detection_limit ,   max (   evals_long $ Average ) ) , low =  low_color , high =  high_color ) +   theme ( panel.background =   element_rect ( fill =  low_color ) , panel.grid.major =   element_line ( size =  0.5 , colour =  \"grey80\" ) , panel.grid.minor =   element_line ( size =  0.25 , colour =  \"grey80\" ) ) } ",
    "filename": "plotDots.txt"
  },
  "new_function": {
    "name": "plotDots",
    "representation": "plotDots",
    "parameters": "function ( object , features , group = NULL , block = NULL , exprs_values = \"logcounts\" , detection_limit = 0 , zlim = NULL , color = NULL , max_detected = NULL , other_fields = list ( ) , by_exprs_values = exprs_values , swap_rownames = NULL , center = FALSE , scale = FALSE , low_color = NULL , high_color = NULL , max_ave = NULL )",
    "body": "{ # Handling all the deprecation.  if (  !   is.null (  low_color ) )  {   .Deprecated ( msg =  \"'low_color=' is deprecated, use 'color=' instead\" ) }  if (  !   is.null (  high_color ) )  {   .Deprecated ( msg =  \"'high_color=' is deprecated, use 'color=' instead\" ) }  if (  !   is.null (  max_ave ) )  {   .Deprecated ( msg =  \"'max_ave=' is deprecated, use 'zlim=' instead\" )   zlim -   c (  detection_limit ,  max_ave ) }  if (   is.null (  group ) )  {   group -   rep (  \"all\" ,   ncol (  object ) ) } else  {   group -    retrieveCellInfo (  object ,  group , search =  \"colData\" ) $ value }   feature_names -   .swap_rownames (  object ,  features ,  swap_rownames )   group -   factor (  group ) # Computing, possibly also batch correcting.   ids -   DataFrame ( group =  group )  if (  !   is.null (  block ) )  {    ids $ block -    retrieveCellInfo (  object ,  block , search =  \"colData\" ) $ value }   summarized -   summarizeAssayByGroup (   assay (  object ,  exprs_values ) , ids =  ids , subset.row =  feature_names , statistics =   c (  \"mean\" ,  \"prop.detected\" ) , threshold =  detection_limit )   ave -   assay (  summarized ,  \"mean\" )   num -   assay (  summarized ,  \"prop.detected\" )   group.names -   summarized $ group  if (  !   is.null (  block ) )  {   ave -   correctGroupSummary (  ave , group =   summarized $ group , block =   summarized $ block )   num -   correctGroupSummary (  num , group =   summarized $ group , block =   summarized $ block , transform =  \"logit\" )   group.names -   colnames (  ave ) }   heatmap_scale -   .heatmap_scale (  ave , center =  center , scale =  scale , color =  color , zlim =  zlim ) # Creating a long-form table.   evals_long -   data.frame ( Feature =   rep (  features ,   ncol (  num ) ) , Group =   rep (  group.names , each =   nrow (  num ) ) , NumDetected =   as.numeric (  num ) , Average =   as.numeric (   heatmap_scale $ x ) )  if (  !   is.null (  max_detected ) )  {    evals_long $ NumDetected -   pmin (  max_detected ,   evals_long $ NumDetected ) } # Adding other fields, if requested.   vis_out -   .incorporate_common_vis_row (  evals_long , se =  object , colour_by =  NULL , shape_by =  NULL , size_by =  NULL , by_exprs_values =  by_exprs_values , other_fields =  other_fields , multiplier =   rep (   .subset2index (  feature_names ,  object ) ,   ncol (  num ) ) )   evals_long -   vis_out $ df       ggplot (  evals_long ) +   geom_point (   aes_string ( x =  \"Group\" , y =  \"Feature\" , size =  \"NumDetected\" , col =  \"Average\" ) ) +   scale_size ( limits =   c (  0 ,   max (   evals_long $ NumDetected ) ) ) +   heatmap_scale $ color_scale +   theme ( panel.background =   element_rect ( fill =  \"white\" ) , panel.grid.major =   element_line ( size =  0.5 , colour =  \"grey80\" ) , panel.grid.minor =   element_line ( size =  0.25 , colour =  \"grey80\" ) ) } ",
    "filename": "plotDots.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_13 scater_release_3_14

{
    "package": "scater",
    "release_versions": "scater_release_3_13 scater_release_3_14",
    "desc_release_old": "1.20.1",
    "desc_release_new": "1.22.0",
    "old_release_number": 9,
    "new_release_number": 10,
    "function_removals": 0,
    "function_additions": 0,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 0,
    "total_count": 0
}

##########
Functions Removed
##########



##########
Functions Added
##########



##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########


###############################
###############################
###############################
###############################
Checking Versions:  scater_release_3_14 scater_master

{
    "package": "scater",
    "release_versions": "scater_release_3_14 scater_master",
    "desc_release_old": "1.22.0",
    "desc_release_new": "1.23.6",
    "old_release_number": 10,
    "new_release_number": 11,
    "function_removals": 0,
    "function_additions": 0,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 0,
    "total_count": 0
}

##########
Functions Removed
##########



##########
Functions Added
##########



##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

