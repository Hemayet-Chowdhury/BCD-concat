
###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_3 scran_release_3_4

{
    "package": "scran",
    "release_versions": "scran_release_3_3 scran_release_3_4",
    "desc_release_old": "1.0.4",
    "desc_release_new": "1.2.2",
    "old_release_number": 0,
    "new_release_number": 1,
    "function_removals": 0,
    "function_additions": 2,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 1,
    "total_count": 1
}

##########
Functions Removed
##########



##########
Functions Added
##########

selectorPlot
technicalCV2


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "trendVar",
    "representation": "trendVar",
    "signature": "matrix",
    "parameters": "function ( x , trend = c ( \"poly\" , \"loess\" ) , df = 5 , span = 0.3 , design = NULL )",
    "body": "{   lmeans -   rowMeans (  x )  if (   is.null (  design ) )  {   design -   .interceptModel (   ncol (  x ) ) }   lvar -   .estimateVariance (  design ,  x )   is.okay -   lvar undefined  1e-8   kept.means -   lmeans [  is.okay ]   llvar -    log2 (  lvar ) [  is.okay ]   trend -   match.arg (  trend )  if (   trend ==  \"loess\" )  {   fit -   loess (   llvar ~  kept.means , span =  span , degree =  1 , family =  \"symmetric\" ) } else  if (   trend ==  \"poly\" )  {   fit -   lm (   llvar ~   poly (  kept.means , df =  df ) ) }   left.edge -   which.min (  kept.means )   right.edge -   which.max (  kept.means )   FUN -  function ( x )  {   out -   predict (  fit ,   data.frame ( kept.means =  x ) )    out [   x undefined   kept.means [  left.edge ] ] -    fitted (  fit ) [  left.edge ]    out [   x undefined   kept.means [  right.edge ] ] -    fitted (  fit ) [  right.edge ]   return (   2 ^  out ) }   return (   list ( mean =  lmeans , var =  lvar , trend =  FUN , design =  design ) ) } ",
    "filename": "trendVar.txt"
  },
  "new_function": {
    "name": "trendVar",
    "representation": "trendVar",
    "signature": "matrix",
    "parameters": "function ( x , trend = c ( \"loess\" , \"semiloess\" ) , span = 0.3 , family = \"symmetric\" , degree = 1 , start = list ( ) , design = NULL , subset.row = NULL )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )  if (   is.null (  design ) )  {   design -   .interceptModel (   ncol (  x ) ) }   QR -   .checkDesign (  design )   lout -   .Call (  cxx_estimate_variance ,   QR $ qr ,   QR $ qraux ,  x ,   subset.row -  1L )  if (   is.character (  lout ) )  {   stop (  lout ) }   lmeans -   lout [[  1 ] ]   lvar -   lout [[  2 ] ]    names (  lmeans ) -    names (  lvar ) -    rownames (  x ) [  subset.row ]   is.okay -   lvar undefined  1e-8   kept.var -   lvar [  is.okay ]   kept.means -   lmeans [  is.okay ]   trend -   match.arg (  trend )  if (   trend ==  \"loess\" )  {   kept.lvar -   log2 (  kept.var )   fit -   loess (   kept.lvar ~  kept.means , span =  span , degree =  degree , family =  family )   SUBFUN -  function ( x )  {   2 ^   predict (  fit ,   data.frame ( kept.means =  x ) ) } } else  if (   trend ==  \"semiloess\" )  { # Setting up starting parameters.  if (   is.null (   start $ a1 ) )  {    start $ a -  5 }  if (   is.null (   start $ n ) )  {    start $ n -  5 }  if (   is.null (   start $ b ) )  {    start $ b -  1 }  if (    length (  kept.var ) =  3L )  {   stop (  \"need at least 4 values for non-linear curve fitting\" ) }   init.fit -   nls (   kept.var ~   (   a *  kept.means ) /  (    kept.means ^  n +  b ) , start =  start , control =   nls.control ( warnOnly =  TRUE , maxiter =  500 ) , algorithm =  \"port\" , lower =   list ( a =  0 , b =  0 , n =  1 ) )   leftovers -   log2 (   kept.var /   fitted (  init.fit ) )   loess.fit -   loess (   leftovers ~  kept.means , span =  span , degree =  degree , family =  family )   SUBFUN -  function ( x )  {   args -   data.frame ( kept.means =  x )    predict (  init.fit ,  args ) *   2 ^   predict (  loess.fit ,  args ) } }   left.edge -   min (  kept.means )   right.edge -   max (  kept.means )   FUN -  function ( x )  {   out -   SUBFUN (  x )    out [   x undefined  left.edge ] -   SUBFUN (  left.edge )    out [   x undefined  right.edge ] -   SUBFUN (  right.edge )   return (  out ) }   return (   list ( mean =  lmeans , var =  lvar , trend =  FUN , design =  design ) ) } ",
    "filename": "trendVar.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_4 scran_release_3_5

{
    "package": "scran",
    "release_versions": "scran_release_3_4 scran_release_3_5",
    "desc_release_old": "1.2.2",
    "desc_release_new": "1.4.5",
    "old_release_number": 1,
    "new_release_number": 2,
    "function_removals": 3,
    "function_additions": 8,
    "parameter_removals": 1,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 2,
    "total_count": 5
}

##########
Functions Removed
##########

normalize
spikes
isSpike


##########
Functions Added
##########

combineVar
exploreData
mnnCorrect
buildSNNGraph
improvedCV2
denoisePCA
findMarkers
overlapExprs


##########
Removed Non Default Parameters
##########

0.
{
  "old_function": {
    "name": "sandbag",
    "representation": "sandbag",
    "signature": "matrix",
    "parameters": "function ( x , is.G1 , is.S , is.G2M , gene.names = rownames ( x ) , fraction = 0.5 , subset.row = NULL )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   data.G1 -   t (   x [  subset.row ,  is.G1 , drop =  FALSE ] )   data.S -   t (   x [  subset.row ,  is.S , drop =  FALSE ] )   data.G2M -   t (   x [  subset.row ,  is.G2M , drop =  FALSE ] )   gene.names -   gene.names [  subset.row ]   G1.marker.pairs -   find.markers (  data.G1 ,  data.S ,  data.G2M , fraction =  fraction , gene.names =  gene.names )   S.marker.pairs -   find.markers (  data.S ,  data.G1 ,  data.G2M , fraction =  fraction , gene.names =  gene.names )   G2M.marker.pairs -   find.markers (  data.G2M ,  data.G1 ,  data.S , fraction =  fraction , gene.names =  gene.names )   return (   list ( G1 =  G1.marker.pairs , S =  S.marker.pairs , G2M =  G2M.marker.pairs ) ) } ",
    "filename": "sandbag.txt"
  },
  "new_function": {
    "name": "sandbag",
    "representation": "sandbag",
    "signature": "matrix",
    "parameters": "function ( x , phases , gene.names = rownames ( x ) , fraction = 0.5 , subset.row = NULL )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   gene.names -   gene.names [  subset.row ]   class.names -   names (  phases )  if (    is.null (  class.names ) ||   is.na (  class.names ) )   stop (  \"'phases' must have non-missing, non-NULL names\" )   gene.data -   lapply (  phases ,  function ( cl )   t (   x [  subset.row ,  cl , drop =  FALSE ] ) )   nclasses -   length (  gene.data )   marker.pairs -   vector (  \"list\" ,  nclasses )  for  ( i in   seq_len (  nclasses ) )  {    marker.pairs [[  i ] ] -   find.markers (   gene.data [[  i ] ] ,   gene.data [  -  i ] , fraction =  fraction , gene.names =  gene.names ) }    names (  marker.pairs ) -  class.names   return (  marker.pairs ) } ",
    "filename": "sandbag.txt"
  }
}



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "matrix",
    "parameters": "function ( x , null.dist = NULL , design = NULL , BPPARAM = SerialParam ( ) , use.names = TRUE , tol = 1e-8 , residuals = FALSE , subset.row = NULL )",
    "body": "{   compute.residuals -  FALSE  if (  !   is.null (  design ) )  {   QR -   .checkDesign (  design )   groupings -   .isOneWay (  design )  if (    is.null (  groupings ) ||  residuals )  {   compute.residuals -  TRUE   groupings -   list (   seq_len (   ncol (  x ) ) ) }  if (   is.null (  null.dist ) )  {   null.dist -   correlateNull ( design =  design , residuals =  residuals ) } } else  {   groupings -   list (   seq_len (   ncol (  x ) ) )  if (   is.null (  null.dist ) )  {   null.dist -   correlateNull (   ncol (  x ) ) } } # Checking that the null distribution is sensible.  if (  !   identical (  design ,   attr (  null.dist ,  \"design\" ) ) )  {   warning (  \"'design' is not the same as that used to generate 'null.dist'\" ) }  if (  !   is.null (  design ) )  {  if (  !   identical (  residuals ,   attr (  null.dist ,  \"residuals\" ) ) )  {   warning (  \"'residuals' is not the same as that used to generate 'null.dist'\" ) } }   null.dist -   as.double (  null.dist )  if (   is.unsorted (  null.dist ) )  {   null.dist -   sort (  null.dist ) } # Checking the subsetting and tolerance   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   tol -   as.double (  tol ) # Generating all pairs of genes   ngenes -   length (  subset.row )  if (   ngenes undefined  2L )  {   stop (  \"need at least two genes to compute correlations\" ) }   all.pairs -   combn (  ngenes ,  2L )   gene1 -   all.pairs [  1 , ]   gene2 -   all.pairs [  2 , ] # Iterating through all subgroups (for one-way layouts; otherwise, this is a loop of length 1).   all.rho -  0L  for  ( subset.col in  groupings )  {  if (  !  compute.residuals )  { # Ranking genes, in an error-tolerant way. This avoids getting untied rankings for zeroes # (which should have the same value +/- precision, as the prior count scaling cancels out).   ranked.exprs -   .Call (  cxx_rank_subset ,  x ,   subset.row -  1L ,   subset.col -  1L ,  tol ) } else  { # If we're computing residuals, we intervene here and replace values with the residuals. # Also need to replace the subset vector, as it'll already be subsetted.   rx -   .Call (  cxx_get_residuals ,  x ,   QR $ qr ,   QR $ qraux ,   subset.row -  1L )  if (   is.character (  rx ) )  {   stop (  rx ) }   ranked.exprs -   .Call (  cxx_rank_subset ,  rx ,    seq_len (   nrow (  rx ) ) -  1L ,   subset.col -  1L ,  tol ) }  if (   is.character (  ranked.exprs ) )  {   stop (  ranked.exprs ) } # Running through each set of jobs   workass -   .workerAssign (   length (  gene1 ) ,  BPPARAM )   out -   bpmapply ( FUN =  .get_correlation , wstart =   workass $ start , wend =   workass $ end , BPPARAM =  BPPARAM , MoreArgs =   list ( gene1 =  gene1 , gene2 =  gene2 , ranked.exprs =  ranked.exprs ) , SIMPLIFY =  FALSE )   current.rho -   unlist (  out ) # Adding a weighted value to the final.   all.rho -   all.rho +   current.rho *  (    length (  subset.col ) /   ncol (  x ) ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning some useful output   gene1 -   subset.row [  gene1 ]   gene2 -   subset.row [  gene2 ]   newnames -  NULL  if (   is.logical (  use.names ) )  {  if (  use.names )  {   newnames -   rownames (  x ) } } else  if (   is.character (  use.names ) )  {  if (    length (  use.names ) !=   nrow (  x ) )  {   stop (  \"length of 'use.names' does not match 'x' nrow\" ) }   newnames -  use.names }  if (  !   is.null (  newnames ) )  {   gene1 -   newnames [  gene1 ]   gene2 -   newnames [  gene2 ] }   out -   data.frame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , stringsAsFactors =  FALSE )   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL   return (  out ) } ",
    "filename": "correlatePairs.txt"
  },
  "new_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "matrix",
    "parameters": "function ( x , null.dist = NULL , design = NULL , BPPARAM = SerialParam ( ) , use.names = TRUE , tol = 1e-8 , iters = 1e6 , residuals = FALSE , subset.row = NULL , per.gene = FALSE )",
    "body": "{   compute.residuals -  FALSE  if (  !   is.null (  design ) )  {   QR -   qr (  design , LAPACK =  TRUE )   blocks -   .isOneWay (  design )  if (    is.null (  blocks ) ||  residuals )  {   compute.residuals -  TRUE   blocks -   list (   seq_len (   ncol (  x ) ) ) }  if (   is.null (  null.dist ) )  {   null.dist -   correlateNull ( design =  design , residuals =  residuals , iters =  iters ) } } else  {   blocks -   list (   seq_len (   ncol (  x ) ) )  if (   is.null (  null.dist ) )  {   null.dist -   correlateNull (   ncol (  x ) , iters =  iters ) } } # Checking that the null distribution is sensible.  if (  !   identical (  design ,   attr (  null.dist ,  \"design\" ) ) )  {   warning (  \"'design' is not the same as that used to generate 'null.dist'\" ) }  if (  !   is.null (  design ) )  {  if (  !   identical (  residuals ,   attr (  null.dist ,  \"residuals\" ) ) )  {   warning (  \"'residuals' is not the same as that used to generate 'null.dist'\" ) } }   null.dist -   as.double (  null.dist )  if (   is.unsorted (  null.dist ) )  {   null.dist -   sort (  null.dist ) } # Checking the subsetting and tolerance   tol -   as.double (  tol )   pairings -   .construct_pair_indices ( subset.row =  subset.row , x =  x )   subset.row -   pairings $ subset.row   gene1 -   pairings $ gene1   gene2 -   pairings $ gene2   reorder -   pairings $ reorder   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names ) # Computing residuals; also replacing the subset vector, as it'll already be subsetted.  if (  compute.residuals )  {   use.x -   .Call (  cxx_get_residuals ,  x ,   QR $ qr ,   QR $ qraux ,   subset.row -  1L )  if (   is.character (  use.x ) )  {   stop (  use.x ) }   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Iterating through all blocking levels (for one-way layouts; otherwise, this is a loop of length 1).   all.rho -  0L  for  ( subset.col in  blocks )  { # Ranking genes in an error-tolerant way. This avoids getting untied rankings for zeroes # (which should have the same value +/- precision, as the prior count scaling cancels out).   ranked.exprs -   .Call (  cxx_rank_subset ,  use.x ,  use.subset.row ,   subset.col -  1L ,  tol )  if (   is.character (  ranked.exprs ) )  {   stop (  ranked.exprs ) } # Running through each set of jobs   workass -   .workerAssign (   length (  gene1 ) ,  BPPARAM )   out -   bpmapply ( FUN =  .get_correlation , wstart =   workass $ start , wend =   workass $ end , BPPARAM =  BPPARAM , MoreArgs =   list ( gene1 =   gene1 -  1L , gene2 =   gene2 -  1L , ranked.exprs =  ranked.exprs ) , SIMPLIFY =  FALSE )   current.rho -   unlist (  out ) # Adding a weighted value to the final.   all.rho -   all.rho +   current.rho *  (    length (  subset.col ) /   ncol (  x ) ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   limited -    left ==  0L |   right ==  0L   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning output on a per-gene basis, testing if each gene is correlated to any other gene.  if (  per.gene )  {   by.gene -   .Call (  cxx_combine_corP ,   length (  subset.row ) ,   gene1 -  1L ,   gene2 -  1L ,  all.rho ,  all.pval ,  limited ,    order (  all.pval ) -  1L )  if (   is.character (  by.gene ) )   stop (  by.gene )   out -   data.frame ( gene =  final.names , rho =   by.gene [[  2 ] ] , p.value =   by.gene [[  1 ] ] , FDR =   p.adjust (   by.gene [[  1 ] ] , method =  \"BH\" ) , limited =   by.gene [[  3 ] ] , stringsAsFactors =  FALSE )    rownames (  out ) -  NULL   .is_sig_limited (  out )   return (  out ) } # Otherwise, returning the pairs themselves.   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   data.frame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  limited , stringsAsFactors =  FALSE )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  }
}

1.
{
  "old_function": {
    "name": "sandbag",
    "representation": "sandbag",
    "signature": "matrix",
    "parameters": "function ( x , is.G1 , is.S , is.G2M , gene.names = rownames ( x ) , fraction = 0.5 , subset.row = NULL )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   data.G1 -   t (   x [  subset.row ,  is.G1 , drop =  FALSE ] )   data.S -   t (   x [  subset.row ,  is.S , drop =  FALSE ] )   data.G2M -   t (   x [  subset.row ,  is.G2M , drop =  FALSE ] )   gene.names -   gene.names [  subset.row ]   G1.marker.pairs -   find.markers (  data.G1 ,  data.S ,  data.G2M , fraction =  fraction , gene.names =  gene.names )   S.marker.pairs -   find.markers (  data.S ,  data.G1 ,  data.G2M , fraction =  fraction , gene.names =  gene.names )   G2M.marker.pairs -   find.markers (  data.G2M ,  data.G1 ,  data.S , fraction =  fraction , gene.names =  gene.names )   return (   list ( G1 =  G1.marker.pairs , S =  S.marker.pairs , G2M =  G2M.marker.pairs ) ) } ",
    "filename": "sandbag.txt"
  },
  "new_function": {
    "name": "sandbag",
    "representation": "sandbag",
    "signature": "matrix",
    "parameters": "function ( x , phases , gene.names = rownames ( x ) , fraction = 0.5 , subset.row = NULL )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   gene.names -   gene.names [  subset.row ]   class.names -   names (  phases )  if (    is.null (  class.names ) ||   is.na (  class.names ) )   stop (  \"'phases' must have non-missing, non-NULL names\" )   gene.data -   lapply (  phases ,  function ( cl )   t (   x [  subset.row ,  cl , drop =  FALSE ] ) )   nclasses -   length (  gene.data )   marker.pairs -   vector (  \"list\" ,  nclasses )  for  ( i in   seq_len (  nclasses ) )  {    marker.pairs [[  i ] ] -   find.markers (   gene.data [[  i ] ] ,   gene.data [  -  i ] , fraction =  fraction , gene.names =  gene.names ) }    names (  marker.pairs ) -  class.names   return (  marker.pairs ) } ",
    "filename": "sandbag.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_5 scran_release_3_6

{
    "package": "scran",
    "release_versions": "scran_release_3_5 scran_release_3_6",
    "desc_release_old": "1.4.5",
    "desc_release_new": "1.6.9",
    "old_release_number": 2,
    "new_release_number": 3,
    "function_removals": 1,
    "function_additions": 0,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 0,
    "total_count": 1
}

##########
Functions Removed
##########

isSpike<-


##########
Functions Added
##########



##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_6 scran_release_3_7

{
    "package": "scran",
    "release_versions": "scran_release_3_6 scran_release_3_7",
    "desc_release_old": "1.6.9",
    "desc_release_new": "1.8.4",
    "old_release_number": 3,
    "new_release_number": 4,
    "function_removals": 0,
    "function_additions": 5,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 8,
    "total_count": 8
}

##########
Functions Removed
##########



##########
Functions Added
##########

buildKNNGraph
clusterModularity
makeTechTrend
multiBlockVar
parallelPCA


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , transposed = FALSE , pc.approx = FALSE , rand.seed = 1000 , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   ncells -   ncol (  x )  if (  !   is.null (  subset.row ) )  {   x -   x [   .subset_to_index (  subset.row ,  x , byrow =  TRUE ) , , drop =  FALSE ] } # Reducing dimensions, if 'd' is less than the number of genes.  if (  !  transposed )  {   x -   t (  x ) }  if (   !   is.na (  d ) undefined   d undefined   ncol (  x ) )  {  if (  pc.approx )  {  if (  !   is.na (  rand.seed ) )  {   set.seed (  rand.seed ) }   pc -   irlba :: prcomp_irlba (  x , n =  d , scale. =  FALSE , center =  TRUE ) } else  {   pc -   prcomp (  x , rank. =  d , scale. =  FALSE , center =  TRUE ) }   x -   pc $ x } # Getting the kNNs.   nn.out -   .find_knn (  x , k =  k , BPPARAM =  BPPARAM ) # Building the SNN graph.   g.out -   .Call (  cxx_build_snn ,   nn.out $ nn.index )   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , transposed = FALSE , pc.approx = FALSE , rand.seed = 1000 , irlba.args = list ( ) , knn.args = list ( ) , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , knn.args =  knn.args , BPPARAM =  BPPARAM ) # Building the SNN graph.   g.out -   .Call (  cxx_build_snn ,   nn.out $ nn.index )   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

1.
{
  "old_function": {
    "name": "computeSumFactors",
    "representation": "computeSumFactors",
    "signature": "ANY",
    "parameters": "function ( x , sizes = seq ( 20 , 100 , 5 ) , clusters = NULL , ref.clust = NULL , positive = FALSE , errors = FALSE , min.mean = 1 , subset.row = NULL )",
    "body": "{   ncells -   ncol (  x )  if (  !   is.null (  clusters ) )  {  if (   ncells !=   length (  clusters ) )  {   stop (  \"'x' ncols is not equal to 'clusters' length\" ) }   is.okay -  !   is.na (  clusters )   indices -   split (   which (  is.okay ) ,   clusters [  is.okay ] ) } else  {   indices -   list (   seq_len (  ncells ) ) } # Checking sizes.   sizes -   sort (   as.integer (  sizes ) )  if (   anyDuplicated (  sizes ) )  {   stop (  \"'sizes' are not unique\" ) } # Checking the subsetting (with interplay with min.mean if required).   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )  if (  !   is.null (  min.mean ) )  {   high.ave -   which (    scater :: calcAverage (  x ) =  min.mean )   subset.row -   intersect (  high.ave ,  subset.row ) } # Setting some other values.   nclusters -   length (  indices )   clust.nf -   clust.profile -   clust.libsizes -   clust.meanlib -   clust.se -   vector (  \"list\" ,  nclusters )   warned.neg -  FALSE # Computing normalization factors within each cluster first.  for  ( clust in   seq_len (  nclusters ) )  {   curdex -   indices [[  clust ] ]   cur.cells -   length (  curdex )   cur.sizes -  sizes  if (   any (   cur.sizes undefined  cur.cells ) )  {   cur.sizes -   cur.sizes [   cur.sizes =  cur.cells ]  if (   length (  cur.sizes ) )  {   warning (  \"not enough cells in at least one cluster for some 'sizes'\" ) } else  {   stop (  \"not enough cells in at least one cluster for any 'sizes'\" ) } }   cur.out -   .Call (  cxx_subset_and_divide ,  x ,   subset.row -  1L ,   curdex -  1L )   cur.libs -   cur.out [[  1 ] ]   exprs -   cur.out [[  2 ] ]   use.ave.cell -   cur.out [[  3 ] ]   ave.cell -   cur.out [[  4 ] ] # Using our summation approach.   sphere -   .generateSphere (  cur.libs )   new.sys -   .create_linear_system (  exprs ,  use.ave.cell ,  sphere ,  cur.sizes )   design -   new.sys $ design   output -   new.sys $ output # Weighted least-squares (inverse model for positivity).  if (  positive )  {   design -   as.matrix (  design )   fitted -   limSolve :: lsei ( A =  design , B =  output , G =   diag (  cur.cells ) , H =   numeric (  cur.cells ) , type =  2 )   final.nf -   fitted $ X } else  {   QR -   qr (  design )   final.nf -   qr.coef (  QR ,  output )  if (   any (   final.nf undefined  0 ) )  {  if (  !  warned.neg )  {   warning (  \"encountered negative size factor estimates\" ) }   warned.neg -  TRUE }  if (  errors )  {   warning (  \"errors=TRUE is no longer supported\" ) } } # Adding per-cluster information.    clust.nf [[  clust ] ] -  final.nf    clust.profile [[  clust ] ] -  ave.cell    clust.libsizes [[  clust ] ] -  cur.libs    clust.meanlib [[  clust ] ] -   mean (  cur.libs ) } # Adjusting size factors between clusters (using the cluster with the # median per-cell library size as the reference, if not specified).  if (   is.null (  ref.clust ) )  {   clust.meanlib -   unlist (  clust.meanlib )   ref.col -   which (    rank (  clust.meanlib , ties.method =  \"first\" ) ==    as.integer (    length (  clust.meanlib ) /  2 ) +  1L ) } else  {   ref.col -   which (    names (  indices ) ==  ref.clust )  if (    length (  ref.col ) ==  0L )  {   stop (  \"'ref.clust' value not in 'clusters'\" ) } }   clust.nf.scaled -   vector (  \"list\" ,  nclusters )  for  ( clust in   seq_len (  nclusters ) )  {    clust.nf.scaled [[  clust ] ] -    clust.nf [[  clust ] ] *   median (    clust.profile [[  clust ] ] /   clust.profile [[  ref.col ] ] , na.rm =  TRUE ) }   clust.nf.scaled -   unlist (  clust.nf.scaled ) # Returning centered size factors, rather than normalization factors.   clust.sf -   clust.nf.scaled *   unlist (  clust.libsizes )   final.sf -   rep (  NA_integer_ ,  ncells )   indices -   unlist (  indices )    final.sf [  indices ] -  clust.sf   is.pos -    final.sf undefined  0 undefined  !   is.na (  final.sf )   final.sf -   final.sf /   mean (   final.sf [  is.pos ] )   return (  final.sf ) } ",
    "replacementFunction": ".computeSumFactors",
    "filename": "computeSumFactors.txt"
  },
  "new_function": {
    "name": "computeSumFactors",
    "representation": "computeSumFactors",
    "signature": "ANY",
    "parameters": "function ( x , sizes = seq ( 20 , 100 , 5 ) , clusters = NULL , ref.clust = NULL , max.cluster.size = 3000 , positive = FALSE , errors = FALSE , min.mean = 1 , subset.row = NULL )",
    "body": "{   ncells -   ncol (  x )  if (   is.null (  clusters ) )  {   clusters -   integer (  ncells ) }   clusters -   .limit_cluster_size (  clusters ,  max.cluster.size )  if (   ncells !=   length (  clusters ) )  {   stop (  \"'ncol(x)' is not equal to 'length(clusters)'\" ) }   indices -   split (   seq_along (  clusters ) ,  clusters )  if (    length (  indices ) ==  0L )  { # To ensure that the empty cluster error triggers.   indices -   list (   integer (  0 ) ) } # Checking sizes and subsetting.   sizes -   sort (   as.integer (  sizes ) )  if (   anyDuplicated (  sizes ) )  {   stop (  \"'sizes' are not unique\" ) }   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   min.mean -   pmax (  min.mean ,  1e-8 ) # must be at least non-zero mean. # Setting some other values.   nclusters -   length (  indices )   clust.nf -   clust.profile -   clust.libsizes -   vector (  \"list\" ,  nclusters )   clust.meanlib -   numeric (  nclusters )   warned.neg -  FALSE # Computing normalization factors within each cluster first.  for  ( clust in   seq_len (  nclusters ) )  {   curdex -   indices [[  clust ] ]   cur.cells -   length (  curdex )   cur.sizes -  sizes  if (   any (   cur.sizes undefined  cur.cells ) )  {   cur.sizes -   cur.sizes [   cur.sizes =  cur.cells ]  if (   length (  cur.sizes ) )  {   warning (  \"not enough cells in at least one cluster for some 'sizes'\" ) } else  {   stop (  \"not enough cells in at least one cluster for any 'sizes'\" ) } }   cur.out -   .Call (  cxx_subset_and_divide ,  x ,   subset.row -  1L ,   curdex -  1L )   cur.libs -   cur.out [[  1 ] ]   exprs -   cur.out [[  2 ] ]   ave.cell -   cur.out [[  3 ] ] # Filtering by mean (easier to do it here in R, rather than C++).   mean.lib -   mean (  cur.libs )   high.ave -    min.mean /  mean.lib =  ave.cell # mimics calcAverage  if (  !   all (  high.ave ) )  {   exprs -   exprs [  high.ave , , drop =  FALSE ]   use.ave.cell -   ave.cell [  high.ave ] } else  {   use.ave.cell -  ave.cell } # Using our summation approach.   sphere -   .generateSphere (  cur.libs )   new.sys -   .create_linear_system (  exprs ,  use.ave.cell ,  sphere ,  cur.sizes )   design -   new.sys $ design   output -   new.sys $ output # Weighted least-squares (inverse model for positivity).  if (  positive )  {   design -   as.matrix (  design )   fitted -   limSolve :: lsei ( A =  design , B =  output , G =   diag (  cur.cells ) , H =   numeric (  cur.cells ) , type =  2 )   final.nf -   fitted $ X } else  {   QR -   qr (  design )   final.nf -   qr.coef (  QR ,  output )  if (   any (   final.nf undefined  0 ) )  {  if (  !  warned.neg )  {   warning (  \"encountered negative size factor estimates\" ) }   warned.neg -  TRUE }  if (  errors )  {   warning (  \"errors=TRUE is no longer supported\" ) } } # Adding per-cluster information.    clust.nf [[  clust ] ] -  final.nf    clust.profile [[  clust ] ] -  ave.cell    clust.libsizes [[  clust ] ] -  cur.libs    clust.meanlib [  clust ] -  mean.lib } # Adjusting size factors between clusters (using the cluster with the # median per-cell library size as the reference, if not specified).   rescaling.factors -   .rescale_clusters (  clust.profile ,  clust.meanlib , ref.clust =  ref.clust , min.mean =  min.mean , clust.names =   names (  indices ) )   clust.nf.scaled -   vector (  \"list\" ,  nclusters )  for  ( clust in   seq_len (  nclusters ) )  {    clust.nf.scaled [[  clust ] ] -    clust.nf [[  clust ] ] *   rescaling.factors [[  clust ] ] }   clust.nf.scaled -   unlist (  clust.nf.scaled ) # Returning centered size factors, rather than normalization factors.   clust.sf -   clust.nf.scaled *   unlist (  clust.libsizes )   final.sf -   rep (  NA_integer_ ,  ncells )   indices -   unlist (  indices )    final.sf [  indices ] -  clust.sf   is.pos -    final.sf undefined  0 undefined  !   is.na (  final.sf )   final.sf -   final.sf /   mean (   final.sf [  is.pos ] )   return (  final.sf ) } ",
    "replacementFunction": ".computeSumFactors",
    "filename": "computeSumFactors.txt"
  }
}

2.
{
  "old_function": {
    "name": "correlateNull",
    "representation": "correlateNull",
    "parameters": "function ( ncells , iters = 1e6 , design = NULL , residuals = FALSE )",
    "body": "{  if (  !   is.null (  design ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'design'\" ) }   groupings -   .is_one_way (  design )  if (    is.null (  groupings ) ||  residuals )  { # Using residualsd residual effects if the design matrix is not a one-way layout (or if forced by residuals=TRUE).   QR -   .ranksafe_qr (  design )   out -   .Call (  cxx_get_null_rho_design ,   QR $ qr ,   QR $ qraux ,   as.integer (  iters ) ) } else  { # Otherwise, estimating the correlation as a weighted mean of the correlations in each group. # This avoids the need for the normality assumption in the residual effect simulation.   out -  0  for  ( gr in  groupings )  {   out.g -   .Call (  cxx_get_null_rho ,   length (  gr ) ,   as.integer (  iters ) )   out -   out +   out.g *   length (  gr ) }   out -   out /   nrow (  design ) }   attrib -   list ( design =  design , residuals =  residuals ) } else  {   out -   .Call (  cxx_get_null_rho ,   as.integer (  ncells ) ,   as.integer (  iters ) )   attrib -  NULL } # Storing attributes, to make sure it matches up.   out -   sort (  out )    attributes (  out ) -  attrib   return (  out ) } ",
    "filename": "correlateNull.txt"
  },
  "new_function": {
    "name": "correlateNull",
    "representation": "correlateNull",
    "parameters": "function ( ncells , iters = 1e6 , block = NULL , design = NULL , residuals = FALSE )",
    "body": "{  if (  !   is.null (  block ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'block'\" ) }   groupings -   table (  block ) # Estimating the correlation as a weighted mean of the correlations in each group. # This avoids the need for the normality assumption in the residual effect simulation.   out -  0  for  ( ngr in  groupings )  {   out.g -   .Call (  cxx_get_null_rho ,  ngr ,   as.integer (  iters ) )   out -   out +   out.g *  ngr }   out -   out /   length (  block )   attrib -   list ( block =  block ) } else  if (  !   is.null (  design ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'design'\" ) }  if (  residuals )  {   .Deprecated ( msg =  \"'residuals=TRUE' is deprecated, choose between 'design' and 'block'\" ) } else  {   groupings -   .is_one_way (  design )  if (  !   is.null (  groupings ) )  {   .Deprecated ( msg =  \"'residuals=FALSE' for one-way layouts is deprecated, use 'block'\" ) } } # Using residualsd residual effects if the design matrix is not a one-way layout (or if forced by residuals=TRUE).   QR -   .ranksafe_qr (  design )   out -   .Call (  cxx_get_null_rho_design ,   QR $ qr ,   QR $ qraux ,   as.integer (  iters ) )   attrib -   list ( design =  design ) } else  {   out -   .Call (  cxx_get_null_rho ,   as.integer (  ncells ) ,   as.integer (  iters ) )   attrib -  NULL } # Storing attributes, to make sure it matches up.   out -   sort (  out )    attributes (  out ) -  attrib   return (  out ) } ",
    "filename": "correlateNull.txt"
  }
}

3.
{
  "old_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , tol = 1e-8 , iters = 1e6 , design = NULL , residuals = FALSE , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , block.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   compute.residuals -  FALSE  if (  !   is.null (  design ) )  {   blocks -   .is_one_way (  design )  if (    is.null (  blocks ) ||  residuals )  {   compute.residuals -  TRUE   blocks -   list (   seq_len (   ncol (  x ) ) ) }  if (   is.null (  null.dist ) )  {   null.dist -   correlateNull ( design =  design , residuals =  residuals , iters =  iters ) } } else  {   blocks -   list (   seq_len (   ncol (  x ) ) )  if (   is.null (  null.dist ) )  {   null.dist -   correlateNull (   ncol (  x ) , iters =  iters ) } } # Checking that the null distribution is sensible.  if (  !   identical (  design ,   attr (  null.dist ,  \"design\" ) ) )  {   warning (  \"'design' is not the same as that used to generate 'null.dist'\" ) }  if (  !   is.null (  design ) )  {  if (  !   identical (  residuals ,   attr (  null.dist ,  \"residuals\" ) ) )  {   warning (  \"'residuals' is not the same as that used to generate 'null.dist'\" ) } }   null.dist -   as.double (  null.dist )  if (   is.unsorted (  null.dist ) )  {   null.dist -   sort (  null.dist ) } # Checking the subsetting and tolerance   block.size -   as.integer (  block.size )   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings , block.size =  block.size )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names ) # Computing residuals; also replacing the subset vector, as it'll already be subsetted.  if (  compute.residuals )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   sgene2 -   vector (  \"list\" ,   length (  wout ) )  for  ( i in   seq_along (  wout ) )  {    sgene1 [[  i ] ] -    gene1 [   wout [[  i ] ] ] -  1L    sgene2 [[  i ] ] -    gene2 [   wout [[  i ] ] ] -  1L } # Iterating through all blocking levels (for one-way layouts; otherwise, this is a loop of length 1). # Computing correlations between gene pairs, and adding a weighted value to the final average.   all.rho -   numeric (   length (  gene1 ) )  for  ( subset.col in  blocks )  {   ranked.exprs -   .Call (  cxx_rank_subset ,  use.x ,  use.subset.row ,   subset.col -  1L ,   as.double (  tol ) )   out -   bpmapply ( FUN =  .get_correlation , gene1 =  sgene1 , gene2 =  sgene2 , MoreArgs =   list ( ranked.exprs =  ranked.exprs , block.size =  block.size ) , BPPARAM =  BPPARAM )   current.rho -   unlist (  out )   all.rho -   all.rho +    current.rho *   length (  subset.col ) /   ncol (  x ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   limited -    left ==  0L |   right ==  0L   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning output on a per-gene basis, testing if each gene is correlated to any other gene.  if (  per.gene )  {   by.gene -   .Call (  cxx_combine_corP ,   length (  subset.row ) ,   gene1 -  1L ,   gene2 -  1L ,  all.rho ,  all.pval ,  limited ,    order (  all.pval ) -  1L )  if (   is.character (  by.gene ) )   stop (  by.gene )   out -   data.frame ( gene =  final.names , rho =   by.gene [[  2 ] ] , p.value =   by.gene [[  1 ] ] , FDR =   p.adjust (   by.gene [[  1 ] ] , method =  \"BH\" ) , limited =   by.gene [[  3 ] ] , stringsAsFactors =  FALSE )    rownames (  out ) -  NULL   .is_sig_limited (  out )   return (  out ) } # Otherwise, returning the pairs themselves.   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   data.frame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  limited , stringsAsFactors =  FALSE )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  },
  "new_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , tol = 1e-8 , iters = 1e6 , block = NULL , design = NULL , residuals = FALSE , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , cache.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   null.out -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , null.dist =  null.dist , residuals =  residuals )   null.dist -   null.out $ null   by.block -   null.out $ blocks # Checking which pairwise correlations should be computed.   cache.size -   as.integer (  cache.size )   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder # Computing residuals (setting values that were originally zero to a lower bound). # Also replacing the subset vector, as it'll already be subsetted.  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   sgene2 -   vector (  \"list\" ,   length (  wout ) )  for  ( i in   seq_along (  wout ) )  {    sgene1 [[  i ] ] -    gene1 [   wout [[  i ] ] ] -  1L    sgene2 [[  i ] ] -    gene2 [   wout [[  i ] ] ] -  1L } # Iterating through all blocking levels (for one-way layouts; otherwise, this is a loop of length 1). # Computing correlations between gene pairs, and adding a weighted value to the final average.   all.rho -   numeric (   length (  gene1 ) )  for  ( subset.col in  by.block )  {   ranked.exprs -   .Call (  cxx_rank_subset ,  use.x ,  use.subset.row ,   subset.col -  1L ,   as.double (  tol ) )   out -   bpmapply ( FUN =  .get_correlation , gene1 =  sgene1 , gene2 =  sgene2 , MoreArgs =   list ( ranked.exprs =  ranked.exprs , cache.size =  cache.size ) , BPPARAM =  BPPARAM , SIMPLIFY =  FALSE )   current.rho -   unlist (  out )   all.rho -   all.rho +    current.rho *   length (  subset.col ) /   ncol (  x ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   limited -    left ==  0L |   right ==  0L   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning output on a per-gene basis, testing if each gene is correlated to any other gene.   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )  if (  per.gene )  {   by.gene -   .Call (  cxx_combine_corP ,   length (  subset.row ) ,   gene1 -  1L ,   gene2 -  1L ,  all.rho ,  all.pval ,  limited ,    order (  all.pval ) -  1L )   out -   data.frame ( gene =  final.names , rho =   by.gene [[  2 ] ] , p.value =   by.gene [[  1 ] ] , FDR =   p.adjust (   by.gene [[  1 ] ] , method =  \"BH\" ) , limited =   by.gene [[  3 ] ] , stringsAsFactors =  FALSE )    rownames (  out ) -  NULL   .is_sig_limited (  out )   return (  out ) } # Otherwise, returning the pairs themselves.   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  limited )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  }
}

4.
{
  "old_function": {
    "name": "denoisePCA",
    "representation": "denoisePCA",
    "signature": "ANY",
    "parameters": "function ( x , technical , design = NULL , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , preserve.dim = FALSE , approximate = FALSE , rand.seed = 1000 )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   checked -   .make_var_defaults (  x , fit =  NULL , design =  design )   QR -   .ranksafe_qr (   checked $ design )   stats -   .Call (  cxx_fit_linear_model ,   QR $ qr ,   QR $ qraux ,  x ,   subset.row -  1L ,  FALSE )   all.means -   stats [[  1 ] ]   all.var -   stats [[  2 ] ] # Filtering out genes with negative biological components.   tech.var -   technical (  all.means )   keep -   all.var undefined  tech.var   use.rows -   subset.row [  keep ]   all.means -   all.means [  keep ]   all.var -   all.var [  keep ]   tech.var -   tech.var [  keep ]   technical -   sum (  tech.var )  if (  !   is.null (  design ) )  { # Computing residuals; don't set a lower bound. # Note that this function implicitly subsets by subset.row.   rx -   .calc_residuals_wt_zeroes (  x , QR =  QR , subset.row =  use.rows , lower.bound =  NA ) # Rescaling residuals so that the variance is unbiased. # This is necessary because variance of residuals is underestimated.   rvar -   apply (  rx ,  1 ,  var ) # Replacing 'x' with the scaled residuals (these should already have a mean of zero, # see http://math.stackexchange.com/questions/494181/ for a good explanation).   y -   rx *   sqrt (   all.var /  rvar )   centering -   numeric (   nrow (  y ) ) } else  {   y -   x [  use.rows , , drop =  FALSE ]   centering -  all.means # all.means is already subsetted, remember. } # Checking various other arguments.   value -   match.arg (  value )   min.rank -   max (  1L ,  min.rank )   ncells -   ncol (  x )   max.rank -   min (  ncells ,  max.rank ) # Switching to IRLBA if an approximation is requested.  if (  approximate )  {  if (  !   is.na (  rand.seed ) )  {   set.seed (  rand.seed ) }   max.rank -   min (    dim (  y ) -  1L ,  max.rank )   nu -   ifelse (   value !=  \"n\" ,  max.rank ,  0L )   out -   irlba :: irlba (   t (  y ) , nu =  nu , nv =  max.rank , center =  centering , maxit =   max (  100 ,   max.rank *  10 ) ) # allowing more iterations if max.rank is high.   var.exp -     out $ d ^  2 /  (   ncells -  1 ) # Assuming all non-computed components were technical, and discarding them for further consideration.   leftovers -    sum (  all.var ) -   sum (  var.exp )   technical -   max (  0 ,   technical -  leftovers )   to.keep -   .get_npcs_to_keep (  var.exp ,  technical )   to.keep -   min (   max (  to.keep ,  min.rank ) ,  max.rank ) # Figuring out what value to return; the number of PCs, the PCs themselves, or a denoised low-rank matrix.  if (   value ==  \"n\" )  {   out.val -  to.keep } else  if (   value ==  \"pca\" )  {   ix -   seq_len (  to.keep )   out.val -   sweep (    out $ u [ ,  ix , drop =  FALSE ] ,  2 ,    out $ d [  ix ] , FUN =  \"*\" ) } else  if (   value ==  \"lowrank\" )  {   ix -   seq_len (  to.keep )   denoised -     out $ u [ ,  ix , drop =  FALSE ] %*%  (     out $ d [  ix ] *   t (    out $ v [ ,  ix , drop =  FALSE ] ) )   denoised -    t (  denoised ) +  all.means   out.val -   .restore_dimensions (  x ,  denoised ,  use.rows ,  subset.row , preserve.dim =  preserve.dim ) } } else  { # Centering the matrix and coercing it to a dense representation.   y -   t (   y -  centering )   y -   as.matrix (  y ) # Performing SVD to get the variance of each PC, and choosing the number of PCs to keep.   svd.out -   svd (  y , nu =  0 , nv =  0 )   var.exp -     svd.out $ d ^  2 /  (   ncells -  1 )   to.keep -   .get_npcs_to_keep (  var.exp ,  technical )   to.keep -   min (   max (  to.keep ,  min.rank ) ,  max.rank ) # Figuring out what value to return; the number of PCs, the PCs themselves, or a denoised low-rank matrix.  if (   value ==  \"n\" )  {   out.val -  to.keep } else  if (   value ==  \"pca\" )  {   pc.out -   prcomp (  y , rank. =  to.keep , scale. =  FALSE , center =  FALSE )   out.val -   pc.out $ x } else  if (   value ==  \"lowrank\" )  {   more.svd -   La.svd (  y , nu =  to.keep , nv =  to.keep )   denoised -    more.svd $ u %*%  (     more.svd $ d [   seq_len (  to.keep ) ] *   more.svd $ vt )   denoised -    t (  denoised ) +  all.means   out.val -   .restore_dimensions (  x ,  denoised ,  use.rows ,  subset.row , preserve.dim =  preserve.dim ) } } # Adding the percentage of variance explained.    attr (  out.val ,  \"percentVar\" ) -   var.exp /   sum (  all.var )   return (  out.val ) } ",
    "replacementFunction": ".denoisePCA",
    "filename": "denoisePCA.txt"
  },
  "new_function": {
    "name": "denoisePCA",
    "representation": "denoisePCA",
    "signature": "ANY",
    "parameters": "function ( x , technical , design = NULL , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , approximate = FALSE , rand.seed = 1000 , irlba.args = list ( ) )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )  if (  !   is.null (  design ) )  {   .Deprecated ( msg =  \"'design=' is deprecated.\\nSee '?denoisePCA' for alternatives.\" )   x -   limma :: removeBatchEffect (  x , design =  NULL , covariates =  design ) }   x2 -   DelayedArray (  x )   scale -  NULL # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {   x.var -   rowVars (  x2 )   scale -   sqrt (    technical $ total /  x.var ) # Making sure everyone has the reported total variance.   all.var -    technical $ total [  subset.row ]   tech.var -    technical $ tech [  subset.row ] } else  {   all.var -   rowVars (  x2 , rows =  subset.row )  if (   is.function (  technical ) )  {   all.means -   rowMeans2 (  x2 , rows =  subset.row )   tech.var -   technical (  all.means ) } else  {   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   use.rows -   subset.row [  keep ]   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   total.tech -   sum (  tech.var ) # Subsetting and scaling the matrix.   y -   x [  use.rows , , drop =  FALSE ]  if (  !   is.null (  scale ) )  {   y -   y *   scale [  use.rows ] } # Setting up the PCA function and its arguments.   value -   match.arg (  value )   args -   list ( y =   t (  y ) , max.rank =  max.rank , value =  value )  if (  approximate )  {   svdfun -  .irlba_svd   args -   c (  args ,  irlba.args ) } else  {   svdfun -  .full_svd } # Runing the PCA and choosing the number of PCs.   original -   do.call (  svdfun ,  args )   var.exp -     original $ d ^  2 /  (    ncol (  y ) -  1 )   npcs -   .get_npcs_to_keep (  var.exp ,  total.tech , total =   sum (  all.var ) )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) ) # Processing remaining aspects.   out.val -   .convert_to_output (  original ,  npcs ,  value ,  x ,  scale ,  use.rows )    attr (  out.val ,  \"percentVar\" ) -   var.exp /   sum (  all.var )   return (  out.val ) } ",
    "replacementFunction": ".denoisePCA",
    "filename": "denoisePCA.txt"
  }
}

5.
{
  "old_function": {
    "name": "findMarkers",
    "representation": "findMarkers",
    "signature": "ANY",
    "parameters": "function ( x , clusters , design = NULL , pval.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , min.mean = 0.1 , subset.row = NULL )",
    "body": "{ # Creating a design matrix.   clusters -   as.factor (  clusters )   full.design -   model.matrix (  ~   0 +  clusters )    colnames (  full.design ) -   clust.vals -   levels (  clusters )  if (  !   is.null (  design ) )  { # Removing terms to avoid linear dependencies on the intercept.   out -   qr.solve (  design ,   cbind (   rep (  1 ,   nrow (  design ) ) ) )   to.drop -    abs (  out ) undefined  1e-8  if (   any (  to.drop ) )  {   design -   design [ ,  -    which (  to.drop ) [  1 ] , drop =  FALSE ] }   full.design -   cbind (  full.design ,  design ) # Other linear dependencies will trigger warnings. }   pval.type -   match.arg (  pval.type )   direction -   match.arg (  direction ) # Fit a linear model with a one-way layout (need to account for the fact that the matrix is pivoted).   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   QR -   .ranksafe_qr (  full.design )   stats -   .Call (  cxx_fit_linear_model ,   QR $ qr ,   QR $ qraux ,  x ,   subset.row -  1L ,  TRUE )   coefficients -    stats [[  1 ] ] [   order (   QR $ pivot ) , , drop =  FALSE ]   means -   stats [[  2 ] ]   sigma2 -   stats [[  3 ] ] # Performing the EB shrinkage in two chunks if min.mean is requested. # This ensures that discreteness at low means is quarantined from the high abundances.   higher -   means =  min.mean  if (     is.null (  min.mean ) ||   all (  higher ) ||  !   any (  higher ) )  {   eb.out -   .perform_eb_shrinkage (  sigma2 , covariate =  means , design =  full.design ) } else  {   high.out -   .perform_eb_shrinkage (   sigma2 [  higher ] , covariate =   means [  higher ] , design =  full.design )   low.out -   .perform_eb_shrinkage (   sigma2 [  !  higher ] , covariate =   means [  !  higher ] , design =  full.design )   eb.out -   mapply (  low.out ,  high.out , FUN =  function ( low , high )  {   val -   numeric (   length (  means ) )    val [  higher ] -  high    val [  !  higher ] -  low  val } , SIMPLIFY =  FALSE ) } # Doing a dummy fit, to avoid having to manually calculate standard errors.   lfit -   lmFit (   rbind (   seq_len (   nrow (  full.design ) ) ) ,  full.design )   output -   vector (  \"list\" ,   length (  clust.vals ) )    names (  output ) -  clust.vals  for  ( h in   seq_along (  clust.vals ) )  {   host -   clust.vals [  h ]   not.h -    seq_along (  clust.vals ) [  -  h ]   targets -   clust.vals [  not.h ] # Computing standard errors (via limma).   con -   matrix (  0 ,   ncol (  full.design ) ,   length (  clust.vals ) )    diag (  con ) -  -  1    con [  h , ] -  1   con -   con [ ,  not.h , drop =  FALSE ]   lfit2 -   contrasts.fit (  lfit ,  con ) # Computing log-fold changes, t-statistics and p-values on a per-contrast basis. # This _could_ be vectorised, but it's less confusing to do it like this, # and there's not much speed gain to be had from vectorizing over contrasts.   ngenes -   length (  subset.row )   ncon -   length (  not.h )   all.lfc -   all.p -   matrix (  0 ,  ngenes ,  ncon )   ref.coef -   coefficients [  h , ]  for  ( con in   seq_len (  ncon ) )  {   cur.lfc -   ref.coef -   coefficients [   not.h [  con ] , ]    all.lfc [ ,  con ] -  cur.lfc   cur.t -   cur.lfc /  (     lfit2 $ stdev.unscaled [  con ] *   sqrt (   eb.out $ var.post ) )   cur.p -   2 *   pt (  -   abs (  cur.t ) , df =   eb.out $ df.total )  if (   direction ==  \"up\" )  {   cur.p -   ifelse (   cur.lfc undefined  0 ,   cur.p /  2 ,   1 -   cur.p /  2 ) } else  if (   direction ==  \"down\" )  {   cur.p -   ifelse (   cur.lfc undefined  0 ,   cur.p /  2 ,   1 -   cur.p /  2 ) }    all.p [ ,  con ] -  cur.p }    colnames (  all.lfc ) -   paste0 (  \"logFC.\" ,  targets )  if (   pval.type ==  \"any\" )  { # Computing Simes' p-value in a fully vectorised manner.   gene.id -   rep (   seq_len (  ngenes ) ,  ncon )   o -   order (  gene.id ,  all.p )   penalty -   rep (   ncon /   seq_len (  ncon ) ,  ngenes )   com.p -   matrix (    all.p [  o ] *  penalty ,  ngenes ,  ncon , byrow =  TRUE )   smallest -    (    max.col (  -  com.p ) -  1 ) *  ngenes +   seq_len (  ngenes )   pval -   com.p [  smallest ] } else  { # Computing the IUT p-value.   largest -    (    max.col (  all.p ) -  1 ) *  ngenes +   seq_len (  ngenes )   pval -   all.p [  largest ] } # Collating minimum ranks.   min.rank -   rep (  ngenes ,  ngenes )   min.p -   rep (  1 ,  ngenes )  for  ( con in   seq_len (  ncon ) )  {   cur.p -   all.p [ ,  con ]   cur.rank -   rank (  cur.p , ties.method =  \"first\" )   min.rank -   pmin (  min.rank ,  cur.rank )   min.p -   pmin (  min.p ,  cur.p ) } # Producing the output object.   marker.set -   data.frame ( Top =  min.rank , Gene =    rownames (  x ) [  subset.row ] , FDR =   p.adjust (  pval , method =  \"BH\" ) ,  all.lfc , stringsAsFactors =  FALSE , check.names =  FALSE )   marker.set -   marker.set [   order (   marker.set $ Top ,  min.p ) , ]    rownames (  marker.set ) -  NULL    output [[  host ] ] -  marker.set }   return (  output ) } ",
    "replacementFunction": ".findMarkers",
    "filename": "findMarkers.txt"
  },
  "new_function": {
    "name": "findMarkers",
    "representation": "findMarkers",
    "signature": "ANY",
    "parameters": "function ( x , clusters , block = NULL , design = NULL , pval.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , lfc = 0 , log.p = FALSE , full.stats = FALSE , subset.row = NULL )",
    "body": "{   ncells -   ncol (  x )   clusters -   as.factor (  clusters )  if (    length (  clusters ) !=  ncells )  {   stop (  \"length of 'clusters' does not equal 'ncol(x)'\" ) }   pval.type -   match.arg (  pval.type )   direction -   match.arg (  direction )   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE ) # Estimating the parameters.  if (   !   is.null (  block ) ||   is.null (  design ) )  {   fit -   .test_block_internal (  x ,  subset.row ,  clusters , block =  block , direction =  direction , lfc =  lfc , full.stats =  full.stats ) } else  {   fit -   .fit_lm_internal (  x ,  subset.row ,  clusters , design =  design , direction =  direction , lfc =  lfc , full.stats =  full.stats ) } # Figuring out how to rank within each DataFrame.   all.log.pval -   fit $ p.value   all.stats -   fit $ stats   output -   vector (  \"list\" ,   length (  all.stats ) )    names (  output ) -   names (  all.stats )  for  ( host in   names (  output ) )  {   stat.df -   do.call (  DataFrame ,   c (   all.stats [[  host ] ] ,   list ( check.names =  FALSE ) ) )  if (  !  full.stats )  {    colnames (  stat.df ) -   sprintf (  \"logFC.%s\" ,   colnames (  stat.df ) ) } else  {    colnames (  stat.df ) -   sprintf (  \"stats.%s\" ,   colnames (  stat.df ) ) } # Computing the combined p-value somehow.   log.p.val -   do.call (  cbind ,   all.log.pval [[  host ] ] )   pval -   .combine_pvalues (  log.p.val , pval.type =  pval.type , log.p.in =  TRUE , log.p.out =  log.p )  if (   pval.type ==  \"any\" )  { # Ranking by position within each gene list.   rank.out -   .rank_top_genes (  log.p.val )   min.rank -   rank.out $ rank   min.p -   rank.out $ value   gene.order -   order (  min.rank ,  min.p )   preamble -   DataFrame ( Top =  min.rank ) } else  {   gene.order -   order (  pval )  if (  log.p )  {   preamble -   DataFrame ( log.IUT.p =  pval ) } else  {   preamble -   DataFrame ( IUT.p =  pval ) } } # Performing a log-transformed version of the FDR correction, if desired.  if (  log.p )  {    preamble $ log.FDR -   .logBH (  pval ) } else  {    preamble $ FDR -   p.adjust (  pval , method =  \"BH\" ) } # Running through all stats lists and computing the FDR.  if (  full.stats )  {  if (  log.p )  {  for  ( target in   names (  stat.df ) )  {     stat.df [[  target ] ] $ log.FDR -   .logBH (    stat.df [[  target ] ] $ log.p.value ) } } else  {  for  ( target in   names (  stat.df ) )  {   cur.p -   exp (    stat.df [[  target ] ] $ log.p.value )     stat.df [[  target ] ] $ p.value -  cur.p     stat.df [[  target ] ] $ log.p.value -  NULL     stat.df [[  target ] ] $ FDR -   p.adjust (  cur.p , method =  \"BH\" ) } } } # Producing the output object.   gene.names -    rownames (  x ) [  subset.row ]  if (   is.null (  gene.names ) )  {   gene.names -  subset.row }   marker.set -   DataFrame (  preamble ,  stat.df , check.names =  FALSE , row.names =  gene.names )   marker.set -   marker.set [  gene.order , , drop =  FALSE ]    output [[  host ] ] -  marker.set }   return (  output ) } ",
    "replacementFunction": ".findMarkers",
    "filename": "findMarkers.txt"
  }
}

6.
{
  "old_function": {
    "name": "overlapExprs",
    "representation": "overlapExprs",
    "signature": "ANY",
    "parameters": "function ( x , groups , design = NULL , residuals = FALSE , tol = 1e-8 , BPPARAM = SerialParam ( ) , subset.row = NULL , lower.bound = NULL )",
    "body": "{   compute.residuals -  FALSE  if (  !   is.null (  design ) )  {   groupings -   .is_one_way (  design )  if (    is.null (  groupings ) ||  residuals )  {   compute.residuals -  TRUE   groupings -   list (   seq_len (   ncol (  x ) ) ) } } else  {   groupings -   list (   seq_len (   ncol (  x ) ) ) } # Checking dimensions.   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   ncells -   ncol (  x )  if (    length (  groups ) !=  ncells )  {   stop (  \"length of 'groups' not equal to number of cells\" ) }  if (   !   is.null (  design ) undefined    nrow (  design ) !=  ncells )  {   stop (  \"'nrow(design)' not equal to number of cells\" ) } # Computing residuals; also replacing the subset vector, as it'll already be subsetted.  if (  compute.residuals )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Setting up the output matrices.   groups -   as.factor (  groups )   unique.groups -   levels (  groups )   ngroups -   nlevels (  groups )   output -   used.cells -   vector (  \"list\" ,  ngroups )   ngenes -   length (  subset.row )  for  ( g in   seq_len (  ngroups ) )  {   temp -   matrix (  0 ,  ngenes ,   ngroups -  1L )    colnames (  temp ) -   unique.groups [  -  g ]    rownames (  temp ) -    rownames (  x ) [  subset.row ]    output [[  g ] ] -  temp   temp.n -   integer (   ngroups -  1L )    names (  temp.n ) -   colnames (  temp )    used.cells [[  g ] ] -  temp.n }    names (  output ) -    names (  used.cells ) -  unique.groups # Split up the jobs by genes for multicore execution   wout -   .worker_assign (   length (  use.subset.row ) ,  BPPARAM )  for  ( i in   seq_along (  wout ) )  {    wout [[  i ] ] -   use.subset.row [   wout [[  i ] ] ] }   tol -   as.double (  tol ) # Running through each blocking level, splitting cells into groups and computing the proportions.  for  ( subset.col in  groupings )  {   cur.groups -   groups [  subset.col ]   by.group -   split (  subset.col ,  cur.groups , drop =  FALSE )  if (    length (  by.group ) ==  1L )  {  next }   bout -   bplapply (  wout , FUN =  .find_overlap_exprs , x =  use.x , by.group =  by.group , tol =  tol , BPPARAM =  BPPARAM ) # Adding the proportions to what we already have.   props -   do.call (  rbind ,   lapply (  bout ,  \"[[\" , i =  1 ) )   numsc -    bout [[  1 ] ] [[  2 ] ]   output -   mapply (  \"+\" ,  output ,  props , SIMPLIFY =  FALSE )   used.cells -   mapply (  \"+\" ,  used.cells ,  numsc , SIMPLIFY =  FALSE ) } # Normalizing the output matrices.  for  ( g in   names (  output ) )  {    output [[  g ] ] -   t (    t (   output [[  g ] ] ) /   used.cells [[  g ] ] ) }   return (  output ) } ",
    "replacementFunction": ".overlapExprs",
    "filename": "overlapExprs.txt"
  },
  "new_function": {
    "name": "overlapExprs",
    "representation": "overlapExprs",
    "signature": "ANY",
    "parameters": "function ( x , groups , block = NULL , design = NULL , rank.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , tol = 1e-8 , BPPARAM = SerialParam ( ) , subset.row = NULL , lower.bound = NULL , residuals = FALSE )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   tol -   as.double (  tol )   ncells -   ncol (  x )  if (    length (  groups ) !=  ncells )  {   stop (  \"length of 'groups' not equal to number of cells\" ) }  if (   !   is.null (  design ) undefined    nrow (  design ) !=  ncells )  {   stop (  \"'nrow(design)' not equal to number of cells\" ) } # Computing residuals; also replacing the subset vector, as it'll already be subsetted.   by.block -   .check_blocking_inputs (  x , block =  block , design =  design , residuals =  residuals )  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Setting up the output matrices.   groups -   as.factor (  groups )   overlap -   used.cells -   rep (   list (  0 ) ,   nlevels (  groups ) )    names (  overlap ) -    names (  used.cells ) -   levels (  groups ) # Split up the jobs by genes for multicore execution   wout -   .worker_assign (   length (  use.subset.row ) ,  BPPARAM )  for  ( i in   seq_along (  wout ) )  {    wout [[  i ] ] -   use.subset.row [   wout [[  i ] ] ] } # Running through each blocking level, splitting cells into groups.  for  ( subset.col in  by.block )  {   cur.groups -   groups [  subset.col ]   by.group -   split (  subset.col ,  cur.groups , drop =  FALSE )   stopifnot (   identical (   names (  by.group ) ,   levels (  groups ) ) ) # Sanity check. # Computing the proportions within this block.   bout -   bplapply (  wout , FUN =  .find_overlap_exprs , x =  use.x , by.group =  by.group , tol =  tol , BPPARAM =  BPPARAM ) # Adding the proportions to what we already have.   props -   do.call (  rbind ,   lapply (  bout ,  \"[[\" , i =  1 ) )   numsc -    bout [[  1 ] ] [[  2 ] ]   overlap -   mapply (  \"+\" ,  overlap ,  props , SIMPLIFY =  FALSE )   used.cells -   mapply (  \"+\" ,  used.cells ,  numsc , SIMPLIFY =  FALSE ) } # Normalizing the overlap matrices.  for  ( g in   names (  overlap ) )  {    overlap [[  g ] ] -   t (    t (   overlap [[  g ] ] ) /   used.cells [[  g ] ] ) }   rank.type -   match.arg (  rank.type )   direction -   match.arg (  direction )   gene.names -    rownames (  x ) [  subset.row ]  if (   is.null (  gene.names ) )  {   gene.names -  subset.row # using indices instead. }   .prepare_overlap_output (  overlap , rank.type =  rank.type , direction =  direction , gene.names =  gene.names ) } ",
    "replacementFunction": ".overlapExprs",
    "filename": "overlapExprs.txt"
  }
}

7.
{
  "old_function": {
    "name": "trendVar",
    "representation": "trendVar",
    "signature": "ANY",
    "parameters": "function ( x , method = c ( \"loess\" , \"spline\" , \"semiloess\" ) , span = 0.3 , family = \"symmetric\" , degree = 1 , df = 4 , parametric = FALSE , start = NULL , min.mean = 0.1 , design = NULL , subset.row = NULL )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   checked -   .make_var_defaults (  x , fit =  NULL , design =  design )   design -   checked $ design   QR -   .ranksafe_qr (  design )   lout -   .Call (  cxx_fit_linear_model ,   QR $ qr ,   QR $ qraux ,  x ,   subset.row -  1L ,  FALSE )   means -   lout [[  1 ] ]   vars -   lout [[  2 ] ]    names (  means ) -    names (  vars ) -    rownames (  x ) [  subset.row ] # Filtering out zero-variance and low-abundance genes.   is.okay -    vars undefined  1e-8 undefined   means =  min.mean   kept.vars -   vars [  is.okay ]   kept.means -   means [  is.okay ]   method -   match.arg (  method )  if (   method ==  \"semiloess\" )  {   warning (  \"'semiloess' is deprecated, use parametric=TRUE and method='loess' instead\" )   method -  \"loess\"   parametric -  TRUE } # Fitting a parametric curve to try to flatten the shape. # This is of the form y = a*x/(x^n + b), but each coefficent is actually set # to exp(*) to avoid needing to set lower bounds.  if (  parametric )  {  if (    length (  kept.vars ) =  3L )  {   stop (  \"need at least 4 values for non-linear curve fitting\" ) }  if (   is.null (  start ) )   start -   .get_nls_starts (  kept.vars ,  kept.means )   init.fit -   nls (   kept.vars ~   (    exp (  A ) *  kept.means ) /  (    kept.means ^  (   1 +   exp (  N ) ) +   exp (  B ) ) , start =   list ( A =   log (   start $ a ) , B =   log (   start $ b ) , N =   log (   pmax (  1e-8 ,    start $ n -  1 ) ) ) , control =   nls.control ( warnOnly =  TRUE , maxiter =  500 ) )   to.fit -   log (   kept.vars /   fitted (  init.fit ) )   SUBSUBFUN -  function ( x )  {   predict (  init.fit ,   data.frame ( kept.means =  x ) ) } } else  {  if (    length (  kept.vars ) undefined  2L )  {   stop (  \"need at least 2 values for non-parametric curve fitting\" ) }   to.fit -   log (  kept.vars )   left.edge -   min (  kept.means )   SUBSUBFUN -  function ( x )  {   pmin (  1 ,   x /  left.edge ) } # To get a gradient from 0 to 1 below the supported range. } # Fitting loess or splines to the remainder.  if (   method ==  \"loess\" )  {   after.fit -   loess (   to.fit ~  kept.means , degree =  degree , family =  family , span =  span )   PREDICTOR -  function ( x )  {   predict (  after.fit ,   data.frame ( kept.means =  x ) ) } } else  {   after.fit -   aroma.light :: robustSmoothSpline (  kept.means ,  to.fit , df =  df )   PREDICTOR -  function ( x )  {     predict (  after.fit ,   data.frame ( kept.means =  x ) ) $ y [ ,  1 ] } } # Only trusting the parametric froms for extrapolation; restricting non-parametric forms within the supported range.   left.edge -   min (  kept.means )   right.edge -   max (  kept.means )   SUBFUN -  function ( x )  {   both.bounded -   pmax (   pmin (  x ,  right.edge ) ,  left.edge )    exp (   PREDICTOR (  both.bounded ) ) *   SUBSUBFUN (  x ) } # Estimating the df2, as well as scale shift from estimating mean of logs (assuming shape of trend is correct).   leftovers -   kept.vars /   SUBFUN (  kept.means )   f.fit -   fitFDistRobustly (  leftovers , df1 =    nrow (  design ) -   ncol (  design ) )   f.df2 -   f.fit $ df2 # We don't just want the scaling factor, we want the scaled mean of the F-distribution (see explanation below).  if (   is.infinite (  f.df2 ) )  {   f.scale -   f.fit $ scale } else  if (   f.df2 undefined  2 )  {   f.scale -     f.fit $ scale *  f.df2 /  (   f.df2 -  2 ) } else  {   warning (  \"undefined expectation for small df2\" )   f.scale -   f.fit $ scale }   FUN -  function ( x )  {   output -    SUBFUN (  x ) *  f.scale    names (  output ) -   names (  x )   return (  output ) }   return (   list ( mean =  means , var =  vars , trend =  FUN , design =  design , df2 =  f.df2 , start =  start ) ) } ",
    "replacementFunction": ".trend_var",
    "filename": "trendVar.txt"
  },
  "new_function": {
    "name": "trendVar",
    "representation": "trendVar",
    "signature": "ANY",
    "parameters": "function ( x , method = c ( \"loess\" , \"spline\" ) , parametric = FALSE , loess.args = list ( ) , spline.args = list ( ) , nls.args = list ( ) , span = NULL , family = NULL , degree = NULL , df = NULL , start = NULL , block = NULL , design = NULL , weighted = TRUE , min.mean = 0.1 , subset.row = NULL )",
    "body": "{   stats.out -   .get_var_stats (  x , block =  block , design =  design , subset.row =  subset.row ) # Filtering out zero-variance and low-abundance genes.   is.okay -    !   is.na (   stats.out $ vars ) undefined    stats.out $ vars undefined  1e-8 undefined    stats.out $ means =  min.mean   kept.vars -    stats.out $ vars [  is.okay ]   kept.means -    stats.out $ means [  is.okay ] # Figuring out the d.f. to keep for weighting.  if (   is.matrix (  is.okay ) )  {   kept.resid -    stats.out $ resid.df [    which (  is.okay , arr.ind =  TRUE ) [ ,  2 ] ] } else  {   kept.resid -   rep (   stats.out $ resid.df ,   sum (  is.okay ) ) } # Fitting a parametric curve to try to flatten the shape. # This is of the form y = a*x/(x^n + b), but each coefficent is actually set # to exp(*) to avoid needing to set lower bounds.  if (  parametric )  {  if (    length (  kept.vars ) =  3L )  {   stop (  \"need at least 4 values for non-linear curve fitting\" ) }   nls.args -   .setup_nls_args (  nls.args , start.args =   list ( vars =  kept.vars , means =  kept.means ) )    nls.args $ formula -   kept.vars ~   (    exp (  A ) *  kept.means ) /  (    kept.means ^  (   1 +   exp (  N ) ) +   exp (  B ) )  if (  weighted )  {    nls.args $ weights -  kept.resid }   init.fit -   do.call (  nls ,  nls.args )   to.fit -   log (   kept.vars /   fitted (  init.fit ) )   SUBSUBFUN -  function ( x )  {   predict (  init.fit ,   data.frame ( kept.means =  x ) ) } } else  {  if (    length (  kept.vars ) undefined  2L )  {   stop (  \"need at least 2 values for non-parametric curve fitting\" ) }   to.fit -   log (  kept.vars )   left.edge -   min (  kept.means )   SUBSUBFUN -  function ( x )  {   pmin (  1 ,   x /  left.edge ) } # To get a gradient from 0 to 1 below the supported range. } # Fitting loess or splines to the remainder. Note that we need kept.means as a variable in the formula for predict() to work!   method -   match.arg (  method )  if (   method ==  \"loess\" )  {   loess.args -   .setup_loess_args (  loess.args , degree =  degree , family =  family , span =  span )    loess.args $ formula -   to.fit ~  kept.means  if (  weighted )  {    loess.args $ weights -  kept.resid }   after.fit -   do.call (  loess ,  loess.args )   PREDICTOR -  function ( x )  {   predict (  after.fit ,   data.frame ( kept.means =  x ) ) } } else  {   spline.args -   .setup_spline_args (  spline.args , df =  df )    spline.args $ x -  kept.means    spline.args $ y -  to.fit  if (  weighted )  {    spline.args $ w -   kept.resid /   max (  kept.resid ) # For some reason, robustSmoothSpline only accepts weights in [0, 1]. }   after.fit -   do.call (  aroma.light :: robustSmoothSpline ,  spline.args )   PREDICTOR -  function ( x )  {    predict (  after.fit ,  x ) $ y } } # Only trusting the parametric SUBSUBFUN for extrapolation; restricting non-parametric forms within the supported range.   left.edge -   min (  kept.means )   right.edge -   max (  kept.means )   SUBFUN -  function ( x )  {   both.bounded -   pmax (   pmin (  x ,  right.edge ) ,  left.edge )    exp (   PREDICTOR (  both.bounded ) ) *   SUBSUBFUN (  x ) } # Estimating the df2, as well as scale shift from estimating mean of logs (assuming shape of trend is correct).   leftovers -   kept.vars /   SUBFUN (  kept.means )   f.fit -   fitFDistRobustly (  leftovers , df1 =  kept.resid )   f.df2 -   f.fit $ df2 # We don't just want the scaling factor, we want the scaled mean of the F-distribution (see explanation below).  if (   is.infinite (  f.df2 ) )  {   f.scale -   f.fit $ scale } else  if (   f.df2 undefined  2 )  {   f.scale -     f.fit $ scale *  f.df2 /  (   f.df2 -  2 ) } else  {   warning (  \"undefined expectation for small df2\" )   f.scale -   f.fit $ scale }   FUN -  function ( x )  {   output -    SUBFUN (  x ) *  f.scale    names (  output ) -   names (  x )   return (  output ) }    stats.out $ trend -  FUN    stats.out $ df2 -  f.df2   return (  stats.out ) } ",
    "replacementFunction": ".trend_var",
    "filename": "trendVar.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_7 scran_release_3_8

{
    "package": "scran",
    "release_versions": "scran_release_3_7 scran_release_3_8",
    "desc_release_old": "1.8.4",
    "desc_release_new": "1.10.2",
    "old_release_number": 4,
    "new_release_number": 5,
    "function_removals": 2,
    "function_additions": 14,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 11,
    "total_count": 13
}

##########
Functions Removed
##########

exploreData
selectorPlot


##########
Functions Added
##########

cleanSizeFactors
combineMarkers
combinePValues
cosineNorm
doubletCells
doubletCluster
fastMNN
multiBatchNorm
multiBatchPCA
multiBlockNorm
pairwiseTTests
pairwiseWilcox
scaledColRanks
simpleSumFactors


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , transposed = FALSE , pc.approx = FALSE , rand.seed = 1000 , irlba.args = list ( ) , knn.args = list ( ) , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , knn.args =  knn.args , BPPARAM =  BPPARAM ) # Building the SNN graph.   g.out -   .Call (  cxx_build_snn ,   nn.out $ nn.index )   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , type = c ( \"rank\" , \"number\" ) , transposed = FALSE , pc.approx = FALSE , rand.seed = NA , irlba.args = list ( ) , subset.row = NULL , BNPARAM = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM ) # Building the SNN graph.   type -   match.arg (  type )  if (   type ==  \"rank\" )  {   g.out -   .Call (  cxx_build_snn_rank ,   nn.out $ index ) } else  {   g.out -   .Call (  cxx_build_snn_number ,   nn.out $ index ) }   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

1.
{
  "old_function": {
    "name": "buildKNNGraph",
    "representation": "buildKNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , directed = FALSE , transposed = FALSE , pc.approx = FALSE , rand.seed = 1000 , irlba.args = list ( ) , knn.args = list ( ) , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , knn.args =  knn.args , BPPARAM =  BPPARAM ) # Building the KNN graph.   start -   as.vector (   row (   nn.out $ nn.index ) )   end -   as.vector (   nn.out $ nn.index )   interleaved -   as.vector (   rbind (  start ,  end ) )  if (  directed )  {   g -   make_graph (  interleaved , directed =  TRUE ) } else  {   g -   make_graph (  interleaved , directed =  FALSE )   g -   simplify (  g , edge.attr.comb =  \"first\" ) }   return (  g ) } ",
    "replacementFunction": ".buildKNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildKNNGraph",
    "representation": "buildKNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , directed = FALSE , transposed = FALSE , pc.approx = FALSE , rand.seed = NA , irlba.args = list ( ) , subset.row = NULL , BNPARAM = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM ) # Building the KNN graph.   start -   as.vector (   row (   nn.out $ index ) )   end -   as.vector (   nn.out $ index )   interleaved -   as.vector (   rbind (  start ,  end ) )  if (  directed )  {   g -   make_graph (  interleaved , directed =  TRUE ) } else  {   g -   make_graph (  interleaved , directed =  FALSE )   g -   simplify (  g , edge.attr.comb =  \"first\" ) }   return (  g ) } ",
    "replacementFunction": ".buildKNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

2.
{
  "old_function": {
    "name": "computeSumFactors",
    "representation": "computeSumFactors",
    "signature": "ANY",
    "parameters": "function ( x , sizes = seq ( 20 , 100 , 5 ) , clusters = NULL , ref.clust = NULL , max.cluster.size = 3000 , positive = FALSE , errors = FALSE , min.mean = 1 , subset.row = NULL )",
    "body": "{   ncells -   ncol (  x )  if (   is.null (  clusters ) )  {   clusters -   integer (  ncells ) }   clusters -   .limit_cluster_size (  clusters ,  max.cluster.size )  if (   ncells !=   length (  clusters ) )  {   stop (  \"'ncol(x)' is not equal to 'length(clusters)'\" ) }   indices -   split (   seq_along (  clusters ) ,  clusters )  if (    length (  indices ) ==  0L )  { # To ensure that the empty cluster error triggers.   indices -   list (   integer (  0 ) ) } # Checking sizes and subsetting.   sizes -   sort (   as.integer (  sizes ) )  if (   anyDuplicated (  sizes ) )  {   stop (  \"'sizes' are not unique\" ) }   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   min.mean -   pmax (  min.mean ,  1e-8 ) # must be at least non-zero mean. # Setting some other values.   nclusters -   length (  indices )   clust.nf -   clust.profile -   clust.libsizes -   vector (  \"list\" ,  nclusters )   clust.meanlib -   numeric (  nclusters )   warned.neg -  FALSE # Computing normalization factors within each cluster first.  for  ( clust in   seq_len (  nclusters ) )  {   curdex -   indices [[  clust ] ]   cur.cells -   length (  curdex )   cur.sizes -  sizes  if (   any (   cur.sizes undefined  cur.cells ) )  {   cur.sizes -   cur.sizes [   cur.sizes =  cur.cells ]  if (   length (  cur.sizes ) )  {   warning (  \"not enough cells in at least one cluster for some 'sizes'\" ) } else  {   stop (  \"not enough cells in at least one cluster for any 'sizes'\" ) } }   cur.out -   .Call (  cxx_subset_and_divide ,  x ,   subset.row -  1L ,   curdex -  1L )   cur.libs -   cur.out [[  1 ] ]   exprs -   cur.out [[  2 ] ]   ave.cell -   cur.out [[  3 ] ] # Filtering by mean (easier to do it here in R, rather than C++).   mean.lib -   mean (  cur.libs )   high.ave -    min.mean /  mean.lib =  ave.cell # mimics calcAverage  if (  !   all (  high.ave ) )  {   exprs -   exprs [  high.ave , , drop =  FALSE ]   use.ave.cell -   ave.cell [  high.ave ] } else  {   use.ave.cell -  ave.cell } # Using our summation approach.   sphere -   .generateSphere (  cur.libs )   new.sys -   .create_linear_system (  exprs ,  use.ave.cell ,  sphere ,  cur.sizes )   design -   new.sys $ design   output -   new.sys $ output # Weighted least-squares (inverse model for positivity).  if (  positive )  {   design -   as.matrix (  design )   fitted -   limSolve :: lsei ( A =  design , B =  output , G =   diag (  cur.cells ) , H =   numeric (  cur.cells ) , type =  2 )   final.nf -   fitted $ X } else  {   QR -   qr (  design )   final.nf -   qr.coef (  QR ,  output )  if (   any (   final.nf undefined  0 ) )  {  if (  !  warned.neg )  {   warning (  \"encountered negative size factor estimates\" ) }   warned.neg -  TRUE }  if (  errors )  {   warning (  \"errors=TRUE is no longer supported\" ) } } # Adding per-cluster information.    clust.nf [[  clust ] ] -  final.nf    clust.profile [[  clust ] ] -  ave.cell    clust.libsizes [[  clust ] ] -  cur.libs    clust.meanlib [  clust ] -  mean.lib } # Adjusting size factors between clusters (using the cluster with the # median per-cell library size as the reference, if not specified).   rescaling.factors -   .rescale_clusters (  clust.profile ,  clust.meanlib , ref.clust =  ref.clust , min.mean =  min.mean , clust.names =   names (  indices ) )   clust.nf.scaled -   vector (  \"list\" ,  nclusters )  for  ( clust in   seq_len (  nclusters ) )  {    clust.nf.scaled [[  clust ] ] -    clust.nf [[  clust ] ] *   rescaling.factors [[  clust ] ] }   clust.nf.scaled -   unlist (  clust.nf.scaled ) # Returning centered size factors, rather than normalization factors.   clust.sf -   clust.nf.scaled *   unlist (  clust.libsizes )   final.sf -   rep (  NA_integer_ ,  ncells )   indices -   unlist (  indices )    final.sf [  indices ] -  clust.sf   is.pos -    final.sf undefined  0 undefined  !   is.na (  final.sf )   final.sf -   final.sf /   mean (   final.sf [  is.pos ] )   return (  final.sf ) } ",
    "replacementFunction": ".computeSumFactors",
    "filename": "computeSumFactors.txt"
  },
  "new_function": {
    "name": "computeSumFactors",
    "representation": "computeSumFactors",
    "signature": "ANY",
    "parameters": "function ( x , sizes = seq ( 21 , 101 , 5 ) , clusters = NULL , ref.clust = NULL , max.cluster.size = 3000 , positive = TRUE , scaling = NULL , min.mean = 1 , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   ncells -   ncol (  x )  if (   is.null (  clusters ) )  {   clusters -   integer (  ncells ) }   clusters -   .limit_cluster_size (  clusters ,  max.cluster.size )  if (   ncells !=   length (  clusters ) )  {   stop (  \"'ncol(x)' is not equal to 'length(clusters)'\" ) }   indices -   split (   seq_along (  clusters ) ,  clusters )  if (    length (  indices ) ==  0L )  { # To ensure that the empty cluster error triggers.   indices -   list (   integer (  0 ) ) } # Checking sizes and subsetting.   sizes -   sort (   as.integer (  sizes ) )  if (   anyDuplicated (  sizes ) )  {   stop (  \"'sizes' are not unique\" ) }   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )  if (   is.null (  min.mean ) )  {   stop (  \"set 'min.mean=0' to turn off abundance filtering\" ) }   min.mean -   pmax (  min.mean ,  1e-8 ) # must be at least non-zero mean. # Setting some other values.   nclusters -   length (  indices )   clust.nf -   clust.profile -   clust.libsizes -   vector (  \"list\" ,  nclusters )   clust.meanlib -   numeric (  nclusters ) # Computing normalization factors within each cluster.   all.norm -   bplapply (  indices , FUN =  .per_cluster_normalize , x =  x , sizes =  sizes , subset.row =  subset.row , min.mean =  min.mean , positive =  positive , scaling =  scaling , BPPARAM =  BPPARAM )   clust.nf -   lapply (  all.norm ,  \"[[\" , i =  \"final.nf\" )   clust.profile -   lapply (  all.norm ,  \"[[\" , i =  \"ave.cell\" ) # Adjusting size factors between clusters.  if (   is.null (  ref.clust ) )  {   lib.sizes -   vapply (  clust.profile , FUN =  sum , FUN.VALUE =  0 )   ref.clust -   which (    rank (  lib.sizes , ties.method =  \"first\" ) ==    as.integer (    length (  lib.sizes ) /  2 ) +  1L ) }   rescaling.factors -   .rescale_clusters (  clust.profile , ref.col =  ref.clust , min.mean =  min.mean , char.err.msg =  \"'ref.clust' not in 'clusters'\" , rescale.err.msg =  \"inter-cluster rescaling factors are not strictly positive\" )   clust.nf.scaled -   vector (  \"list\" ,  nclusters )  for  ( clust in   seq_len (  nclusters ) )  {    clust.nf.scaled [[  clust ] ] -    clust.nf [[  clust ] ] *   rescaling.factors [[  clust ] ] }   clust.nf.scaled -   unlist (  clust.nf.scaled ) # Returning centered size factors, rather than normalization factors.   final.sf -   rep (  NA_integer_ ,  ncells )   indices -   unlist (  indices )    final.sf [  indices ] -  clust.nf.scaled   is.pos -    final.sf undefined  0 undefined  !   is.na (  final.sf )   final.sf -   final.sf /   mean (   final.sf [  is.pos ] )   return (  final.sf ) } ",
    "replacementFunction": ".computeSumFactors",
    "filename": "computeSumFactors.txt"
  }
}

3.
{
  "old_function": {
    "name": "correlateNull",
    "representation": "correlateNull",
    "parameters": "function ( ncells , iters = 1e6 , block = NULL , design = NULL , residuals = FALSE )",
    "body": "{  if (  !   is.null (  block ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'block'\" ) }   groupings -   table (  block ) # Estimating the correlation as a weighted mean of the correlations in each group. # This avoids the need for the normality assumption in the residual effect simulation.   out -  0  for  ( ngr in  groupings )  {   out.g -   .Call (  cxx_get_null_rho ,  ngr ,   as.integer (  iters ) )   out -   out +   out.g *  ngr }   out -   out /   length (  block )   attrib -   list ( block =  block ) } else  if (  !   is.null (  design ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'design'\" ) }  if (  residuals )  {   .Deprecated ( msg =  \"'residuals=TRUE' is deprecated, choose between 'design' and 'block'\" ) } else  {   groupings -   .is_one_way (  design )  if (  !   is.null (  groupings ) )  {   .Deprecated ( msg =  \"'residuals=FALSE' for one-way layouts is deprecated, use 'block'\" ) } } # Using residualsd residual effects if the design matrix is not a one-way layout (or if forced by residuals=TRUE).   QR -   .ranksafe_qr (  design )   out -   .Call (  cxx_get_null_rho_design ,   QR $ qr ,   QR $ qraux ,   as.integer (  iters ) )   attrib -   list ( design =  design ) } else  {   out -   .Call (  cxx_get_null_rho ,   as.integer (  ncells ) ,   as.integer (  iters ) )   attrib -  NULL } # Storing attributes, to make sure it matches up.   out -   sort (  out )    attributes (  out ) -  attrib   return (  out ) } ",
    "filename": "correlateNull.txt"
  },
  "new_function": {
    "name": "correlateNull",
    "representation": "correlateNull",
    "parameters": "function ( ncells , iters = 1e6 , block = NULL , design = NULL )",
    "body": "{  if (  !   is.null (  block ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'block'\" ) }   groupings -   table (  block ) # Estimating the correlation as a weighted mean of the correlations in each group. # This avoids the need for the normality assumption in the residual effect simulation.   out -  0  for  ( ngr in  groupings )  {   out.g -   .Call (  cxx_get_null_rho ,  ngr ,   as.integer (  iters ) )   out -   out +   out.g *  ngr }   out -   out /   length (  block )   attrib -   list ( block =  block ) } else  if (  !   is.null (  design ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'design'\" ) } # Using residual effects to compute the correlations.   QR -   .ranksafe_qr (  design )   out -   .Call (  cxx_get_null_rho_design ,   QR $ qr ,   QR $ qraux ,   as.integer (  iters ) )   attrib -   list ( design =  design ) } else  {   out -   .Call (  cxx_get_null_rho ,   as.integer (  ncells ) ,   as.integer (  iters ) )   attrib -  NULL } # Storing attributes, to make sure it matches up.   out -   sort (  out )    attributes (  out ) -  attrib   return (  out ) } ",
    "filename": "correlateNull.txt"
  }
}

4.
{
  "old_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , tol = 1e-8 , iters = 1e6 , block = NULL , design = NULL , residuals = FALSE , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , cache.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   null.out -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , null.dist =  null.dist , residuals =  residuals )   null.dist -   null.out $ null   by.block -   null.out $ blocks # Checking which pairwise correlations should be computed.   cache.size -   as.integer (  cache.size )   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder # Computing residuals (setting values that were originally zero to a lower bound). # Also replacing the subset vector, as it'll already be subsetted.  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   sgene2 -   vector (  \"list\" ,   length (  wout ) )  for  ( i in   seq_along (  wout ) )  {    sgene1 [[  i ] ] -    gene1 [   wout [[  i ] ] ] -  1L    sgene2 [[  i ] ] -    gene2 [   wout [[  i ] ] ] -  1L } # Iterating through all blocking levels (for one-way layouts; otherwise, this is a loop of length 1). # Computing correlations between gene pairs, and adding a weighted value to the final average.   all.rho -   numeric (   length (  gene1 ) )  for  ( subset.col in  by.block )  {   ranked.exprs -   .Call (  cxx_rank_subset ,  use.x ,  use.subset.row ,   subset.col -  1L ,   as.double (  tol ) )   out -   bpmapply ( FUN =  .get_correlation , gene1 =  sgene1 , gene2 =  sgene2 , MoreArgs =   list ( ranked.exprs =  ranked.exprs , cache.size =  cache.size ) , BPPARAM =  BPPARAM , SIMPLIFY =  FALSE )   current.rho -   unlist (  out )   all.rho -   all.rho +    current.rho *   length (  subset.col ) /   ncol (  x ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   limited -    left ==  0L |   right ==  0L   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning output on a per-gene basis, testing if each gene is correlated to any other gene.   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )  if (  per.gene )  {   by.gene -   .Call (  cxx_combine_corP ,   length (  subset.row ) ,   gene1 -  1L ,   gene2 -  1L ,  all.rho ,  all.pval ,  limited ,    order (  all.pval ) -  1L )   out -   data.frame ( gene =  final.names , rho =   by.gene [[  2 ] ] , p.value =   by.gene [[  1 ] ] , FDR =   p.adjust (   by.gene [[  1 ] ] , method =  \"BH\" ) , limited =   by.gene [[  3 ] ] , stringsAsFactors =  FALSE )    rownames (  out ) -  NULL   .is_sig_limited (  out )   return (  out ) } # Otherwise, returning the pairs themselves.   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  limited )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  },
  "new_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , tol = 1e-8 , iters = 1e6 , block = NULL , design = NULL , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , cache.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   null.out -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , null.dist =  null.dist )   null.dist -   null.out $ null   by.block -   null.out $ blocks # Checking which pairwise correlations should be computed.   cache.size -   as.integer (  cache.size )   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder # Computing residuals (setting values that were originally zero to a lower bound). # Also replacing the subset vector, as it'll already be subsetted.  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   .split_vector_by_workers (   gene1 -  1L ,  wout )   sgene2 -   .split_vector_by_workers (   gene2 -  1L ,  wout ) # Iterating through all blocking levels (for one-way layouts; otherwise, this is a loop of length 1). # Computing correlations between gene pairs, and adding a weighted value to the final average.   all.rho -   numeric (   length (  gene1 ) )  for  ( subset.col in  by.block )  {   ranked.exprs -   .Call (  cxx_rank_subset ,  use.x ,  use.subset.row ,   subset.col -  1L ,   as.double (  tol ) )   out -   bpmapply ( FUN =  .get_correlation , gene1 =  sgene1 , gene2 =  sgene2 , MoreArgs =   list ( ranked.exprs =  ranked.exprs , cache.size =  cache.size ) , BPPARAM =  BPPARAM , SIMPLIFY =  FALSE )   current.rho -   unlist (  out )   all.rho -   all.rho +    current.rho *   length (  subset.col ) /   ncol (  x ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   limited -    left ==  0L |   right ==  0L   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning output on a per-gene basis, testing if each gene is correlated to any other gene.   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )  if (  per.gene )  {   by.gene -   .Call (  cxx_combine_corP ,   length (  subset.row ) ,   gene1 -  1L ,   gene2 -  1L ,  all.rho ,  all.pval ,  limited ,    order (  all.pval ) -  1L )   out -   data.frame ( gene =  final.names , rho =   by.gene [[  2 ] ] , p.value =   by.gene [[  1 ] ] , FDR =   p.adjust (   by.gene [[  1 ] ] , method =  \"BH\" ) , limited =   by.gene [[  3 ] ] , stringsAsFactors =  FALSE )    rownames (  out ) -  NULL   .is_sig_limited (  out )   return (  out ) } # Otherwise, returning the pairs themselves.   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  limited )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  }
}

5.
{
  "old_function": {
    "name": "denoisePCA",
    "representation": "denoisePCA",
    "signature": "ANY",
    "parameters": "function ( x , technical , design = NULL , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , approximate = FALSE , rand.seed = 1000 , irlba.args = list ( ) )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )  if (  !   is.null (  design ) )  {   .Deprecated ( msg =  \"'design=' is deprecated.\\nSee '?denoisePCA' for alternatives.\" )   x -   limma :: removeBatchEffect (  x , design =  NULL , covariates =  design ) }   x2 -   DelayedArray (  x )   scale -  NULL # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {   x.var -   rowVars (  x2 )   scale -   sqrt (    technical $ total /  x.var ) # Making sure everyone has the reported total variance.   all.var -    technical $ total [  subset.row ]   tech.var -    technical $ tech [  subset.row ] } else  {   all.var -   rowVars (  x2 , rows =  subset.row )  if (   is.function (  technical ) )  {   all.means -   rowMeans2 (  x2 , rows =  subset.row )   tech.var -   technical (  all.means ) } else  {   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   use.rows -   subset.row [  keep ]   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   total.tech -   sum (  tech.var ) # Subsetting and scaling the matrix.   y -   x [  use.rows , , drop =  FALSE ]  if (  !   is.null (  scale ) )  {   y -   y *   scale [  use.rows ] } # Setting up the PCA function and its arguments.   value -   match.arg (  value )   args -   list ( y =   t (  y ) , max.rank =  max.rank , value =  value )  if (  approximate )  {   svdfun -  .irlba_svd   args -   c (  args ,  irlba.args ) } else  {   svdfun -  .full_svd } # Runing the PCA and choosing the number of PCs.   original -   do.call (  svdfun ,  args )   var.exp -     original $ d ^  2 /  (    ncol (  y ) -  1 )   npcs -   .get_npcs_to_keep (  var.exp ,  total.tech , total =   sum (  all.var ) )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) ) # Processing remaining aspects.   out.val -   .convert_to_output (  original ,  npcs ,  value ,  x ,  scale ,  use.rows )    attr (  out.val ,  \"percentVar\" ) -   var.exp /   sum (  all.var )   return (  out.val ) } ",
    "replacementFunction": ".denoisePCA",
    "filename": "denoisePCA.txt"
  },
  "new_function": {
    "name": "denoisePCA",
    "representation": "denoisePCA",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , approximate = FALSE , rand.seed = 1000 , irlba.args = list ( ) )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x2 -   DelayedArray (  x )   all.var -   rowVars (  x2 , rows =  subset.row ) # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {   scale -   all.var /    technical $ total [  subset.row ] # Making sure everyone has the reported total variance.    scale [   is.na (  scale ) ] -  0   tech.var -     technical $ tech [  subset.row ] *  scale } else  {  if (   is.function (  technical ) )  {   all.means -   rowMeans2 (  x2 , rows =  subset.row )   tech.var -   technical (  all.means ) } else  {   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   use.rows -   subset.row [  keep ]   y -   x [  use.rows , , drop =  FALSE ]   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   total.tech -   sum (  tech.var ) # Setting up the SVD results.   value -   match.arg (  value )   svd.out -   .centered_SVD (   t (  y ) ,  max.rank , approximate =  approximate , extra.args =  irlba.args , keep.left =  (   value !=  \"n\" ) , keep.right =  (   value ==  \"lowrank\" ) ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    ncol (  y ) -  1 )   npcs -   .get_npcs_to_keep (  var.exp ,  total.tech , total =   sum (  all.var ) )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) ) # Processing remaining aspects.   out.val -   switch (  value , n =  npcs , pca =   .svd_to_pca (  svd.out ,  npcs ) , lowrank =   .svd_to_lowrank (  svd.out ,  npcs ,  x ,  use.rows ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /   sum (  all.var )   return (  out.val ) } ",
    "replacementFunction": ".denoisePCA",
    "filename": "denoisePCA.txt"
  }
}

6.
{
  "old_function": {
    "name": "findMarkers",
    "representation": "findMarkers",
    "signature": "ANY",
    "parameters": "function ( x , clusters , block = NULL , design = NULL , pval.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , lfc = 0 , log.p = FALSE , full.stats = FALSE , subset.row = NULL )",
    "body": "{   ncells -   ncol (  x )   clusters -   as.factor (  clusters )  if (    length (  clusters ) !=  ncells )  {   stop (  \"length of 'clusters' does not equal 'ncol(x)'\" ) }   pval.type -   match.arg (  pval.type )   direction -   match.arg (  direction )   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE ) # Estimating the parameters.  if (   !   is.null (  block ) ||   is.null (  design ) )  {   fit -   .test_block_internal (  x ,  subset.row ,  clusters , block =  block , direction =  direction , lfc =  lfc , full.stats =  full.stats ) } else  {   fit -   .fit_lm_internal (  x ,  subset.row ,  clusters , design =  design , direction =  direction , lfc =  lfc , full.stats =  full.stats ) } # Figuring out how to rank within each DataFrame.   all.log.pval -   fit $ p.value   all.stats -   fit $ stats   output -   vector (  \"list\" ,   length (  all.stats ) )    names (  output ) -   names (  all.stats )  for  ( host in   names (  output ) )  {   stat.df -   do.call (  DataFrame ,   c (   all.stats [[  host ] ] ,   list ( check.names =  FALSE ) ) )  if (  !  full.stats )  {    colnames (  stat.df ) -   sprintf (  \"logFC.%s\" ,   colnames (  stat.df ) ) } else  {    colnames (  stat.df ) -   sprintf (  \"stats.%s\" ,   colnames (  stat.df ) ) } # Computing the combined p-value somehow.   log.p.val -   do.call (  cbind ,   all.log.pval [[  host ] ] )   pval -   .combine_pvalues (  log.p.val , pval.type =  pval.type , log.p.in =  TRUE , log.p.out =  log.p )  if (   pval.type ==  \"any\" )  { # Ranking by position within each gene list.   rank.out -   .rank_top_genes (  log.p.val )   min.rank -   rank.out $ rank   min.p -   rank.out $ value   gene.order -   order (  min.rank ,  min.p )   preamble -   DataFrame ( Top =  min.rank ) } else  {   gene.order -   order (  pval )  if (  log.p )  {   preamble -   DataFrame ( log.IUT.p =  pval ) } else  {   preamble -   DataFrame ( IUT.p =  pval ) } } # Performing a log-transformed version of the FDR correction, if desired.  if (  log.p )  {    preamble $ log.FDR -   .logBH (  pval ) } else  {    preamble $ FDR -   p.adjust (  pval , method =  \"BH\" ) } # Running through all stats lists and computing the FDR.  if (  full.stats )  {  if (  log.p )  {  for  ( target in   names (  stat.df ) )  {     stat.df [[  target ] ] $ log.FDR -   .logBH (    stat.df [[  target ] ] $ log.p.value ) } } else  {  for  ( target in   names (  stat.df ) )  {   cur.p -   exp (    stat.df [[  target ] ] $ log.p.value )     stat.df [[  target ] ] $ p.value -  cur.p     stat.df [[  target ] ] $ log.p.value -  NULL     stat.df [[  target ] ] $ FDR -   p.adjust (  cur.p , method =  \"BH\" ) } } } # Producing the output object.   gene.names -    rownames (  x ) [  subset.row ]  if (   is.null (  gene.names ) )  {   gene.names -  subset.row }   marker.set -   DataFrame (  preamble ,  stat.df , check.names =  FALSE , row.names =  gene.names )   marker.set -   marker.set [  gene.order , , drop =  FALSE ]    output [[  host ] ] -  marker.set }   return (  output ) } ",
    "replacementFunction": ".findMarkers",
    "filename": "findMarkers.txt"
  },
  "new_function": {
    "name": "findMarkers",
    "representation": "findMarkers",
    "signature": "ANY",
    "parameters": "function ( x , clusters , gene.names = rownames ( x ) , block = NULL , design = NULL , pval.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , lfc = 0 , log.p = FALSE , full.stats = FALSE , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   fit -   pairwiseTTests (  x ,  clusters , block =  block , design =  design , direction =  direction , lfc =  lfc , gene.names =  gene.names , log.p =  TRUE , subset.row =  subset.row , BPPARAM =  BPPARAM )   combineMarkers (   fit $ statistics ,   fit $ pairs , pval.type =  pval.type , log.p.in =  TRUE , log.p.out =  log.p , full.stats =  full.stats , pval.field =  \"log.p.value\" ) } ",
    "replacementFunction": ".findMarkers",
    "filename": "findMarkers.txt"
  }
}

7.
{
  "old_function": {
    "name": "makeTechTrend",
    "representation": "makeTechTrend",
    "parameters": "function ( means , size.factors = 1 , tol = 1e-6 , dispersion = 0 , pseudo.count = 1 , x = NULL )",
    "body": "{  if (  !   is.null (  x ) )  {   size.factors -   sizeFactors (  x )  if (   is.null (  size.factors ) )  {   size.factors -   colSums (   counts (  x ) )   size.factors -   size.factors /   mean (  size.factors ) }   pseudo.count -   .get_log_offset (  x )  if (   is.null (  pseudo.count ) )  {   stop (  \"'log.exprs.offset' not specified in 'metadata(x)'\" ) }   all.ave -   rowMeans (   logcounts (  x ) )   upper.value -   max (  all.ave )   means -    2 ^   seq ( from =  0 , to =  upper.value , length.out =  100 ) -  pseudo.count }  if (    abs (    mean (  size.factors ) -  1 ) undefined  1e-6 )  {   stop (  \"size factors should be centred at unity\" ) } # Calling the C++ code to do the heavy lifting.   collected -   .Call (  cxx_calc_log_count_stats ,  means ,  size.factors ,  tol ,  dispersion ,  pseudo.count )   collected.means -   collected [[  1 ] ]   collected.vars -   collected [[  2 ] ] # Creating a spline output function.   splinefun (  collected.means ,  collected.vars ) } ",
    "filename": "makeTechTrend.txt"
  },
  "new_function": {
    "name": "makeTechTrend",
    "representation": "makeTechTrend",
    "parameters": "function ( means , size.factors = 1 , tol = 1e-6 , dispersion = 0 , pseudo.count = 1 , approx.npts = Inf , x = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{  if (  !   is.null (  x ) )  {   size.factors -   sizeFactors (  x )  if (   is.null (  size.factors ) )  {   size.factors -   colSums (   counts (  x ) )   size.factors -   size.factors /   mean (  size.factors ) }   pseudo.count -   .get_log_offset (  x )  if (   is.null (  pseudo.count ) )  {   stop (  \"'log.exprs.offset' not specified in 'metadata(x)'\" ) }   all.ave -   rowMeans (   logcounts (  x ) )   upper.value -   max (  all.ave )   means -    2 ^   seq ( from =  0 , to =  upper.value , length.out =  100 ) -  pseudo.count }  if (    abs (    mean (  size.factors ) -  1 ) undefined  1e-6 )  {   stop (  \"size factors should be centred at unity\" ) }   to.core -   .worker_assign (   length (  means ) ,  BPPARAM )   by.core -   .split_vector_by_workers (  means ,  to.core )  if (   is.finite (  approx.npts ) )  {  if (   approx.npts undefined  2 )  {   stop (  \"'approx.npts' should be at least 2\" ) } # Approximating by fitting a spline with respect to the size factors. # This avoids having to compute these statistics for each size factor.   lsf -   log (  size.factors )  if (    length (  lsf ) =  approx.npts )  {   lpts -  lsf } else  {   lpts -   seq (   min (  lsf ) ,   max (  lsf ) , length.out =  approx.npts ) }   pts -   exp (  lpts )   out.expected -   bplapply (  by.core , FUN =  .tech_mean_computer , size.factors =  pts , tol =  tol , dispersion =  dispersion , pseudo.count =  pseudo.count , BPPARAM =  BPPARAM )   collected.means -   .interpolate_and_average (  lpts ,   unlist (  out.expected , recursive =  FALSE ) ,  lsf )   by.core.constant -   .split_vector_by_workers (  collected.means ,  to.core )   out.sqdiff -   bpmapply ( FUN =  .tech_var_computer , means =  by.core , logmeans =  by.core.constant , MoreArgs =   list ( size.factors =  pts , tol =  tol , dispersion =  dispersion , pseudo.count =  pseudo.count ) , BPPARAM =  BPPARAM , SIMPLIFY =  FALSE , USE.NAMES =  FALSE )   collected.vars -   .interpolate_and_average (  lpts ,   unlist (  out.sqdiff , recursive =  FALSE ) ,  lsf ) } else  { # Calling the C++ code to do the heavy lifting.   raw.out -   bplapply (  by.core , FUN =  .tech_trend_computer , size.factors =  size.factors , tol =  tol , dispersion =  dispersion , pseudo.count =  pseudo.count , BPPARAM =  BPPARAM )   collected.means -   unlist (   lapply (  raw.out ,  \"[[\" , i =  1 ) )   collected.vars -   unlist (   lapply (  raw.out ,  \"[[\" , i =  2 ) ) } # Creating a spline output function.   splinefun (  collected.means ,  collected.vars ) } ",
    "filename": "makeTechTrend.txt"
  }
}

8.
{
  "old_function": {
    "name": "overlapExprs",
    "representation": "overlapExprs",
    "signature": "ANY",
    "parameters": "function ( x , groups , block = NULL , design = NULL , rank.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , tol = 1e-8 , BPPARAM = SerialParam ( ) , subset.row = NULL , lower.bound = NULL , residuals = FALSE )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   tol -   as.double (  tol )   ncells -   ncol (  x )  if (    length (  groups ) !=  ncells )  {   stop (  \"length of 'groups' not equal to number of cells\" ) }  if (   !   is.null (  design ) undefined    nrow (  design ) !=  ncells )  {   stop (  \"'nrow(design)' not equal to number of cells\" ) } # Computing residuals; also replacing the subset vector, as it'll already be subsetted.   by.block -   .check_blocking_inputs (  x , block =  block , design =  design , residuals =  residuals )  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Setting up the output matrices.   groups -   as.factor (  groups )   overlap -   used.cells -   rep (   list (  0 ) ,   nlevels (  groups ) )    names (  overlap ) -    names (  used.cells ) -   levels (  groups ) # Split up the jobs by genes for multicore execution   wout -   .worker_assign (   length (  use.subset.row ) ,  BPPARAM )  for  ( i in   seq_along (  wout ) )  {    wout [[  i ] ] -   use.subset.row [   wout [[  i ] ] ] } # Running through each blocking level, splitting cells into groups.  for  ( subset.col in  by.block )  {   cur.groups -   groups [  subset.col ]   by.group -   split (  subset.col ,  cur.groups , drop =  FALSE )   stopifnot (   identical (   names (  by.group ) ,   levels (  groups ) ) ) # Sanity check. # Computing the proportions within this block.   bout -   bplapply (  wout , FUN =  .find_overlap_exprs , x =  use.x , by.group =  by.group , tol =  tol , BPPARAM =  BPPARAM ) # Adding the proportions to what we already have.   props -   do.call (  rbind ,   lapply (  bout ,  \"[[\" , i =  1 ) )   numsc -    bout [[  1 ] ] [[  2 ] ]   overlap -   mapply (  \"+\" ,  overlap ,  props , SIMPLIFY =  FALSE )   used.cells -   mapply (  \"+\" ,  used.cells ,  numsc , SIMPLIFY =  FALSE ) } # Normalizing the overlap matrices.  for  ( g in   names (  overlap ) )  {    overlap [[  g ] ] -   t (    t (   overlap [[  g ] ] ) /   used.cells [[  g ] ] ) }   rank.type -   match.arg (  rank.type )   direction -   match.arg (  direction )   gene.names -    rownames (  x ) [  subset.row ]  if (   is.null (  gene.names ) )  {   gene.names -  subset.row # using indices instead. }   .prepare_overlap_output (  overlap , rank.type =  rank.type , direction =  direction , gene.names =  gene.names ) } ",
    "replacementFunction": ".overlapExprs",
    "filename": "overlapExprs.txt"
  },
  "new_function": {
    "name": "overlapExprs",
    "representation": "overlapExprs",
    "signature": "ANY",
    "parameters": "function ( x , groups , gene.names = rownames ( x ) , block = NULL , pval.type = c ( \"any\" , \"all\" ) , direction = c ( \"any\" , \"up\" , \"down\" ) , tol = 1e-8 , log.p = FALSE , full.stats = FALSE , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   fit -   pairwiseWilcox (  x ,  groups , block =  block , direction =  direction , gene.names =  gene.names , log.p =  TRUE , subset.row =  subset.row , BPPARAM =  BPPARAM )   combineMarkers (   fit $ statistics ,   fit $ pairs , pval.type =  pval.type , log.p.in =  TRUE , log.p.out =  log.p , full.stats =  full.stats , pval.field =  \"log.p.value\" , effect.field =  \"overlap\" ) } ",
    "replacementFunction": ".overlapExprs",
    "filename": "overlapExprs.txt"
  }
}

9.
{
  "old_function": {
    "name": "parallelPCA",
    "representation": "parallelPCA",
    "signature": "ANY",
    "parameters": "function ( x , subset.row = NULL , scale = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , niters = 50 , threshold = 0.1 , approximate = FALSE , irlba.args = list ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   x0 -  x   scale0 -  scale # Subsetting and scaling the matrix.  if (  !   is.null (  subset.row ) )  {   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x -   x [  subset.row , , drop =  FALSE ]   scale -   scale [  subset.row ] }  if (  !   is.null (  scale ) )  {   x -   x *  scale } # Setting up the PCA function and its arguments.   value -   match.arg (  value )   args -   list ( y =   t (  x ) , max.rank =  max.rank , value =  value )  if (  approximate )  {   svdfun -  .irlba_svd   args -   c (  args ,  irlba.args ) } else  {   svdfun -  .full_svd } # Running it once, and then multiple times after permutation.   original -   do.call (  svdfun ,  args )   original.d2 -    original $ d ^  2   permuted -   bplapply (   seq_len (  niters ) , FUN =  .parallel_PA , svdfun =  svdfun , args =  args , BPPARAM =  BPPARAM )   permutations -   do.call (  cbind ,  permuted ) # Figuring out where the original drops to \"within range\" of permuted.   prop -   rowMeans (   permutations =  original.d2 )   above -   prop undefined  threshold  if (  !   any (  above ) )  {   npcs -   length (  above ) } else  {   npcs -    min (   which (  above ) ) -  1L }   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  original.d2 ) ) # Collating the return value.   out.val -   .convert_to_output (  original ,  npcs ,  value ,  x0 ,  scale0 ,  subset.row )   var.exp -   original.d2 /  (    ncol (  x ) -  1 )   all.var -   sum (   rowVars (   DelayedArray (  x ) ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /  all.var    attr (  out.val ,  \"permuted.percentVar\" ) -     t (  permutations ) /  (    ncol (  x ) -  1L ) /  all.var   return (  out.val ) } ",
    "replacementFunction": ".parallelPCA",
    "filename": "parallelPCA.txt"
  },
  "new_function": {
    "name": "parallelPCA",
    "representation": "parallelPCA",
    "signature": "ANY",
    "parameters": "function ( x , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , niters = 50 , threshold = 0.1 , approximate = FALSE , irlba.args = list ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{ # Subsetting the matrix.   x0 -  x  if (  !   is.null (  subset.row ) )  {   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x -   x [  subset.row , , drop =  FALSE ] }   y -   t (  x ) # Running the PCA function once.   value -   match.arg (  value )   svd.out -   .centered_SVD (  y ,  max.rank , approximate =  approximate , extra.args =  irlba.args , keep.left =  (   value !=  \"n\" ) , keep.right =  (   value ==  \"lowrank\" ) ) # Running it once, and then multiple times after permutation.   original.d2 -    svd.out $ d ^  2   permuted -   bplapply (   rep (  max.rank ,  niters ) , FUN =  .parallel_PA , y =  y , approximate =  approximate , extra.args =  irlba.args , BPPARAM =  BPPARAM )   permutations -   do.call (  cbind ,  permuted ) # Figuring out where the original drops to \"within range\" of permuted.   prop -   rowMeans (   permutations =  original.d2 )   above -   prop undefined  threshold  if (  !   any (  above ) )  {   npcs -   length (  above ) } else  {   npcs -    min (   which (  above ) ) -  1L }   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  original.d2 ) ) # Collating the return value.   out.val -   switch (  value , n =  npcs , pca =   .svd_to_pca (  svd.out ,  npcs ) , lowrank =   .svd_to_lowrank (  svd.out ,  npcs ,  x0 ,  subset.row ) )   var.exp -   original.d2 /  (    ncol (  x ) -  1 )   all.var -   sum (   rowVars (   DelayedArray (  x ) ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /  all.var    attr (  out.val ,  \"permuted.percentVar\" ) -     t (  permutations ) /  (    ncol (  x ) -  1L ) /  all.var   return (  out.val ) } ",
    "replacementFunction": ".parallelPCA",
    "filename": "parallelPCA.txt"
  }
}

10.
{
  "old_function": {
    "name": "trendVar",
    "representation": "trendVar",
    "signature": "ANY",
    "parameters": "function ( x , method = c ( \"loess\" , \"spline\" ) , parametric = FALSE , loess.args = list ( ) , spline.args = list ( ) , nls.args = list ( ) , span = NULL , family = NULL , degree = NULL , df = NULL , start = NULL , block = NULL , design = NULL , weighted = TRUE , min.mean = 0.1 , subset.row = NULL )",
    "body": "{   stats.out -   .get_var_stats (  x , block =  block , design =  design , subset.row =  subset.row ) # Filtering out zero-variance and low-abundance genes.   is.okay -    !   is.na (   stats.out $ vars ) undefined    stats.out $ vars undefined  1e-8 undefined    stats.out $ means =  min.mean   kept.vars -    stats.out $ vars [  is.okay ]   kept.means -    stats.out $ means [  is.okay ] # Figuring out the d.f. to keep for weighting.  if (   is.matrix (  is.okay ) )  {   kept.resid -    stats.out $ resid.df [    which (  is.okay , arr.ind =  TRUE ) [ ,  2 ] ] } else  {   kept.resid -   rep (   stats.out $ resid.df ,   sum (  is.okay ) ) } # Fitting a parametric curve to try to flatten the shape. # This is of the form y = a*x/(x^n + b), but each coefficent is actually set # to exp(*) to avoid needing to set lower bounds.  if (  parametric )  {  if (    length (  kept.vars ) =  3L )  {   stop (  \"need at least 4 values for non-linear curve fitting\" ) }   nls.args -   .setup_nls_args (  nls.args , start.args =   list ( vars =  kept.vars , means =  kept.means ) )    nls.args $ formula -   kept.vars ~   (    exp (  A ) *  kept.means ) /  (    kept.means ^  (   1 +   exp (  N ) ) +   exp (  B ) )  if (  weighted )  {    nls.args $ weights -  kept.resid }   init.fit -   do.call (  nls ,  nls.args )   to.fit -   log (   kept.vars /   fitted (  init.fit ) )   SUBSUBFUN -  function ( x )  {   predict (  init.fit ,   data.frame ( kept.means =  x ) ) } } else  {  if (    length (  kept.vars ) undefined  2L )  {   stop (  \"need at least 2 values for non-parametric curve fitting\" ) }   to.fit -   log (  kept.vars )   left.edge -   min (  kept.means )   SUBSUBFUN -  function ( x )  {   pmin (  1 ,   x /  left.edge ) } # To get a gradient from 0 to 1 below the supported range. } # Fitting loess or splines to the remainder. Note that we need kept.means as a variable in the formula for predict() to work!   method -   match.arg (  method )  if (   method ==  \"loess\" )  {   loess.args -   .setup_loess_args (  loess.args , degree =  degree , family =  family , span =  span )    loess.args $ formula -   to.fit ~  kept.means  if (  weighted )  {    loess.args $ weights -  kept.resid }   after.fit -   do.call (  loess ,  loess.args )   PREDICTOR -  function ( x )  {   predict (  after.fit ,   data.frame ( kept.means =  x ) ) } } else  {   spline.args -   .setup_spline_args (  spline.args , df =  df )    spline.args $ x -  kept.means    spline.args $ y -  to.fit  if (  weighted )  {    spline.args $ w -   kept.resid /   max (  kept.resid ) # For some reason, robustSmoothSpline only accepts weights in [0, 1]. }   after.fit -   do.call (  aroma.light :: robustSmoothSpline ,  spline.args )   PREDICTOR -  function ( x )  {    predict (  after.fit ,  x ) $ y } } # Only trusting the parametric SUBSUBFUN for extrapolation; restricting non-parametric forms within the supported range.   left.edge -   min (  kept.means )   right.edge -   max (  kept.means )   SUBFUN -  function ( x )  {   both.bounded -   pmax (   pmin (  x ,  right.edge ) ,  left.edge )    exp (   PREDICTOR (  both.bounded ) ) *   SUBSUBFUN (  x ) } # Estimating the df2, as well as scale shift from estimating mean of logs (assuming shape of trend is correct).   leftovers -   kept.vars /   SUBFUN (  kept.means )   f.fit -   fitFDistRobustly (  leftovers , df1 =  kept.resid )   f.df2 -   f.fit $ df2 # We don't just want the scaling factor, we want the scaled mean of the F-distribution (see explanation below).  if (   is.infinite (  f.df2 ) )  {   f.scale -   f.fit $ scale } else  if (   f.df2 undefined  2 )  {   f.scale -     f.fit $ scale *  f.df2 /  (   f.df2 -  2 ) } else  {   warning (  \"undefined expectation for small df2\" )   f.scale -   f.fit $ scale }   FUN -  function ( x )  {   output -    SUBFUN (  x ) *  f.scale    names (  output ) -   names (  x )   return (  output ) }    stats.out $ trend -  FUN    stats.out $ df2 -  f.df2   return (  stats.out ) } ",
    "replacementFunction": ".trend_var",
    "filename": "trendVar.txt"
  },
  "new_function": {
    "name": "trendVar",
    "representation": "trendVar",
    "signature": "ANY",
    "parameters": "function ( x , method = c ( \"loess\" , \"spline\" ) , parametric = FALSE , loess.args = list ( ) , spline.args = list ( ) , nls.args = list ( ) , block = NULL , design = NULL , weighted = TRUE , min.mean = 0.1 , subset.row = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   stats.out -   .get_var_stats (  x , block =  block , design =  design , subset.row =  subset.row , BPPARAM =  BPPARAM ) # Filtering out zero-variance and low-abundance genes.   is.okay -    !   is.na (   stats.out $ vars ) undefined    stats.out $ vars undefined  1e-8 undefined    stats.out $ means =  min.mean   kept.vars -    stats.out $ vars [  is.okay ]   kept.means -    stats.out $ means [  is.okay ] # Figuring out the d.f. to keep for weighting.  if (   is.matrix (  is.okay ) )  {   kept.resid -    stats.out $ resid.df [    which (  is.okay , arr.ind =  TRUE ) [ ,  2 ] ] } else  {   kept.resid -   rep (   stats.out $ resid.df ,   sum (  is.okay ) ) } # Fitting a parametric curve to try to flatten the shape. # This is of the form y = a*x/(x^n + b), but each coefficent is actually set # to exp(*) to avoid needing to set lower bounds.  if (  parametric )  {  if (    length (  kept.vars ) =  3L )  {   stop (  \"need at least 4 points for non-linear curve fitting\" ) }   nls.args -   .setup_nls_args (  nls.args , start.args =   list ( vars =  kept.vars , means =  kept.means ) )    nls.args $ formula -   kept.vars ~   (    exp (  A ) *  kept.means ) /  (    kept.means ^  (   1 +   exp (  N ) ) +   exp (  B ) )  if (  weighted )  {    nls.args $ weights -  kept.resid }   init.fit -   do.call (  nls ,  nls.args )   to.fit -   log (   kept.vars /   fitted (  init.fit ) )   SUBSUBFUN -  function ( x )  {   predict (  init.fit ,   data.frame ( kept.means =  x ) ) } } else  {  if (    length (  kept.vars ) undefined  2L )  {   stop (  \"need at least 2 points for non-parametric curve fitting\" ) }   to.fit -   log (  kept.vars )   left.edge -   min (  kept.means )   SUBSUBFUN -  function ( x )  {   pmin (  1 ,   x /  left.edge ) } # To get a gradient from 0 to 1 below the supported range. } # Fitting loess or splines to the remainder. Note that we need kept.means as a variable in the formula for predict() to work!   method -   match.arg (  method )  if (   method ==  \"loess\" )  {   loess.args -   .setup_loess_args (  loess.args )    loess.args $ formula -   to.fit ~  kept.means  if (  weighted )  {    loess.args $ weights -  kept.resid }   after.fit -   do.call (  loess ,  loess.args )   PREDICTOR -  function ( x )  {   predict (  after.fit ,   data.frame ( kept.means =  x ) ) } } else  {   spline.args -   .setup_spline_args (  spline.args )    spline.args $ x -  kept.means    spline.args $ y -  to.fit  if (  weighted )  {    spline.args $ w -   kept.resid /   max (  kept.resid ) # For some reason, robustSmoothSpline only accepts weights in [0, 1]. }   after.fit -   do.call (  aroma.light :: robustSmoothSpline ,  spline.args )   PREDICTOR -  function ( x )  {    predict (  after.fit ,  x ) $ y } } # Only trusting the parametric SUBSUBFUN for extrapolation; restricting non-parametric forms within the supported range.   left.edge -   min (  kept.means )   right.edge -   max (  kept.means )   SUBFUN -  function ( x )  {   both.bounded -   pmax (   pmin (  x ,  right.edge ) ,  left.edge )    exp (   PREDICTOR (  both.bounded ) ) *   SUBSUBFUN (  x ) } # Estimating the df2, as well as scale shift from estimating mean of logs (assuming shape of trend is correct).   leftovers -   kept.vars /   SUBFUN (  kept.means )   f.fit -   fitFDistRobustly (  leftovers , df1 =  kept.resid )   f.df2 -   f.fit $ df2 # We don't just want the scaling factor, we want the scaled mean of the F-distribution (see explanation below).  if (   is.infinite (  f.df2 ) )  {   f.scale -   f.fit $ scale } else  if (   f.df2 undefined  2 )  {   f.scale -     f.fit $ scale *  f.df2 /  (   f.df2 -  2 ) } else  {   warning (  \"undefined expectation for small df2\" )   f.scale -   f.fit $ scale }   FUN -  function ( x )  {   output -    SUBFUN (  x ) *  f.scale    names (  output ) -   names (  x )   return (  output ) }    stats.out $ trend -  FUN    stats.out $ df2 -  f.df2   return (  stats.out ) } ",
    "replacementFunction": ".trend_var",
    "filename": "trendVar.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_8 scran_release_3_9

{
    "package": "scran",
    "release_versions": "scran_release_3_8 scran_release_3_9",
    "desc_release_old": "1.10.2",
    "desc_release_new": "1.12.1",
    "old_release_number": 5,
    "new_release_number": 6,
    "function_removals": 1,
    "function_additions": 2,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 6,
    "total_count": 7
}

##########
Functions Removed
##########

simpleSumFactors


##########
Functions Added
##########

correlateGenes
denoisePCANumber


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , type = c ( \"rank\" , \"number\" ) , transposed = FALSE , pc.approx = FALSE , rand.seed = NA , irlba.args = list ( ) , subset.row = NULL , BNPARAM = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM ) # Building the SNN graph.   type -   match.arg (  type )  if (   type ==  \"rank\" )  {   g.out -   .Call (  cxx_build_snn_rank ,   nn.out $ index ) } else  {   g.out -   .Call (  cxx_build_snn_number ,   nn.out $ index ) }   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , type = c ( \"rank\" , \"number\" ) , transposed = FALSE , pc.approx = NULL , irlba.args = list ( ) , subset.row = NULL , BNPARAM = KmknnParam ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Building the SNN graph.   type -   match.arg (  type )  if (   type ==  \"rank\" )  {   g.out -   .Call (  cxx_build_snn_rank ,   nn.out $ index ) } else  {   g.out -   .Call (  cxx_build_snn_number ,   nn.out $ index ) }   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

1.
{
  "old_function": {
    "name": "buildKNNGraph",
    "representation": "buildKNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , directed = FALSE , transposed = FALSE , pc.approx = FALSE , rand.seed = NA , irlba.args = list ( ) , subset.row = NULL , BNPARAM = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , rand.seed =  rand.seed , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM ) # Building the KNN graph.   start -   as.vector (   row (   nn.out $ index ) )   end -   as.vector (   nn.out $ index )   interleaved -   as.vector (   rbind (  start ,  end ) )  if (  directed )  {   g -   make_graph (  interleaved , directed =  TRUE ) } else  {   g -   make_graph (  interleaved , directed =  FALSE )   g -   simplify (  g , edge.attr.comb =  \"first\" ) }   return (  g ) } ",
    "replacementFunction": ".buildKNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildKNNGraph",
    "representation": "buildKNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , directed = FALSE , transposed = FALSE , pc.approx = NULL , irlba.args = list ( ) , subset.row = NULL , BNPARAM = KmknnParam ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Building the KNN graph.   start -   as.vector (   row (   nn.out $ index ) )   end -   as.vector (   nn.out $ index )   interleaved -   as.vector (   rbind (  start ,  end ) )  if (  directed )  {   g -   make_graph (  interleaved , directed =  TRUE ) } else  {   g -   make_graph (  interleaved , directed =  FALSE )   g -   simplify (  g , edge.attr.comb =  \"first\" ) }   return (  g ) } ",
    "replacementFunction": ".buildKNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

2.
{
  "old_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , tol = 1e-8 , iters = 1e6 , block = NULL , design = NULL , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , cache.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   null.out -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , null.dist =  null.dist )   null.dist -   null.out $ null   by.block -   null.out $ blocks # Checking which pairwise correlations should be computed.   cache.size -   as.integer (  cache.size )   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder # Computing residuals (setting values that were originally zero to a lower bound). # Also replacing the subset vector, as it'll already be subsetted.  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -    seq_len (   nrow (  use.x ) ) -  1L } else  {   use.x -  x   use.subset.row -   subset.row -  1L } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   .split_vector_by_workers (   gene1 -  1L ,  wout )   sgene2 -   .split_vector_by_workers (   gene2 -  1L ,  wout ) # Iterating through all blocking levels (for one-way layouts; otherwise, this is a loop of length 1). # Computing correlations between gene pairs, and adding a weighted value to the final average.   all.rho -   numeric (   length (  gene1 ) )  for  ( subset.col in  by.block )  {   ranked.exprs -   .Call (  cxx_rank_subset ,  use.x ,  use.subset.row ,   subset.col -  1L ,   as.double (  tol ) )   out -   bpmapply ( FUN =  .get_correlation , gene1 =  sgene1 , gene2 =  sgene2 , MoreArgs =   list ( ranked.exprs =  ranked.exprs , cache.size =  cache.size ) , BPPARAM =  BPPARAM , SIMPLIFY =  FALSE )   current.rho -   unlist (  out )   all.rho -   all.rho +    current.rho *   length (  subset.col ) /   ncol (  x ) } # Estimating the p-values (need to shift values to break ties conservatively by increasing the p-value).   left -   findInterval (   all.rho +  1e-8 ,  null.dist )   right -    length (  null.dist ) -   findInterval (   all.rho -  1e-8 ,  null.dist )   limited -    left ==  0L |   right ==  0L   all.pval -    (    pmin (  left ,  right ) +  1 ) *  2 /  (    length (  null.dist ) +  1 )   all.pval -   pmin (  all.pval ,  1 ) # Returning output on a per-gene basis, testing if each gene is correlated to any other gene.   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )  if (  per.gene )  {   by.gene -   .Call (  cxx_combine_corP ,   length (  subset.row ) ,   gene1 -  1L ,   gene2 -  1L ,  all.rho ,  all.pval ,  limited ,    order (  all.pval ) -  1L )   out -   data.frame ( gene =  final.names , rho =   by.gene [[  2 ] ] , p.value =   by.gene [[  1 ] ] , FDR =   p.adjust (   by.gene [[  1 ] ] , method =  \"BH\" ) , limited =   by.gene [[  3 ] ] , stringsAsFactors =  FALSE )    rownames (  out ) -  NULL   .is_sig_limited (  out )   return (  out ) } # Otherwise, returning the pairs themselves.   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  limited )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  },
  "new_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , ties.method = c ( \"expected\" , \"average\" ) , tol = 1e-8 , iters = 1e6 , block = NULL , design = NULL , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , cache.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   null.out -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , null.dist =  null.dist , BPPARAM =  BPPARAM )   null.dist -   null.out $ null   by.block -   null.out $ blocks # Checking which pairwise correlations should be computed.   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder # Computing residuals (setting values that were originally zero to a lower bound). # Also replacing the subset vector, as it'll already be subsetted.  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -   seq_len (   nrow (  use.x ) ) } else  {   use.x -  x   use.subset.row -  subset.row } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   .split_vector_by_workers (   gene1 -  1L ,  wout )   sgene2 -   .split_vector_by_workers (   gene2 -  1L ,  wout )   all.rho -   .calc_blocked_rho (  sgene1 ,  sgene2 , x =  use.x , subset.row =  use.subset.row , by.block =  by.block , tol =  tol , ties.method =   match.arg (  ties.method ) , BPPARAM =  BPPARAM )   stats -   .rho_to_pval (  all.rho ,  null.dist )   all.pval -   stats $ p   all.lim -   stats $ limited # Formatting the output.   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  all.lim )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )  if (  per.gene )  {   .Deprecated ( msg =  \"'per.gene=' is deprecated.\\nUse 'correlateGenes' instead.\" )   out -   correlateGenes (  out ) }   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  }
}

3.
{
  "old_function": {
    "name": "denoisePCA",
    "representation": "denoisePCA",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , approximate = FALSE , rand.seed = 1000 , irlba.args = list ( ) )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x2 -   DelayedArray (  x )   all.var -   rowVars (  x2 , rows =  subset.row ) # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {   scale -   all.var /    technical $ total [  subset.row ] # Making sure everyone has the reported total variance.    scale [   is.na (  scale ) ] -  0   tech.var -     technical $ tech [  subset.row ] *  scale } else  {  if (   is.function (  technical ) )  {   all.means -   rowMeans2 (  x2 , rows =  subset.row )   tech.var -   technical (  all.means ) } else  {   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   use.rows -   subset.row [  keep ]   y -   x [  use.rows , , drop =  FALSE ]   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   total.tech -   sum (  tech.var ) # Setting up the SVD results.   value -   match.arg (  value )   svd.out -   .centered_SVD (   t (  y ) ,  max.rank , approximate =  approximate , extra.args =  irlba.args , keep.left =  (   value !=  \"n\" ) , keep.right =  (   value ==  \"lowrank\" ) ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    ncol (  y ) -  1 )   npcs -   .get_npcs_to_keep (  var.exp ,  total.tech , total =   sum (  all.var ) )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) ) # Processing remaining aspects.   out.val -   switch (  value , n =  npcs , pca =   .svd_to_pca (  svd.out ,  npcs ) , lowrank =   .svd_to_lowrank (  svd.out ,  npcs ,  x ,  use.rows ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /   sum (  all.var )   return (  out.val ) } ",
    "replacementFunction": ".denoisePCA",
    "filename": "denoisePCA.txt"
  },
  "new_function": {
    "name": "denoisePCA",
    "representation": "denoisePCA",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , approximate = NULL , irlba.args = list ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x2 -   DelayedArray (  x )   all.var -   rowVars (  x2 , rows =  subset.row ) # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {   scale -   all.var /    technical $ total [  subset.row ] # Making sure everyone has the reported total variance.    scale [   is.na (  scale ) ] -  0   tech.var -     technical $ tech [  subset.row ] *  scale } else  {  if (   is.function (  technical ) )  {   all.means -   rowMeans2 (  x2 , rows =  subset.row )   tech.var -   technical (  all.means ) } else  {   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   use.rows -   subset.row [  keep ]   y -   x [  use.rows , , drop =  FALSE ] # Setting up the SVD results.   value -   match.arg (  value )   svd.out -   .centered_SVD (   t (  y ) ,  max.rank , approximate =  approximate , extra.args =  irlba.args , keep.left =  (   value !=  \"n\" ) , keep.right =  (   value ==  \"lowrank\" ) , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    ncol (  y ) -  1 )   total.var -   sum (  all.var )   npcs -   denoisePCANumber (  var.exp ,   sum (  tech.var ) ,  total.var )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) ) # Processing remaining aspects.   out.val -   switch (  value , n =  npcs , pca =   .svd_to_pca (  svd.out ,  npcs ) , lowrank =   .svd_to_lowrank (  svd.out ,  npcs ,  x ,  use.rows ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /  total.var  out.val } ",
    "replacementFunction": ".denoisePCA",
    "filename": "denoisePCA.txt"
  }
}

4.
{
  "old_function": {
    "name": "doubletCells",
    "representation": "doubletCells",
    "signature": "ANY",
    "parameters": "function ( x , size.factors.norm = NULL , size.factors.content = NULL , k = 50 , subset.row = NULL , niters = max ( 10000 , ncol ( x ) ) , block = 10000 , d = 50 , approximate = FALSE , irlba.args = list ( ) , force.match = FALSE , force.k = 20 , force.ndist = 3 , BNPARAM = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{  if (  !   is.null (  subset.row ) )  {   x -   x [  subset.row , , drop =  FALSE ] }  if (   is.null (  size.factors.norm ) )  {   size.factors.norm -   librarySizeFactors (  x ) }  if (  !   is.null (  size.factors.content ) )  {   x -   normalizeCounts (  x ,  size.factors.content , return_log =  FALSE , centre_size_factors =  FALSE )   size.factors.norm -   size.factors.norm /  size.factors.content }   y -   normalizeCounts (  x ,  size.factors.norm , centre_size_factors =  FALSE ) # Running the SVD.   svd.out -   .centered_SVD (   t (  y ) , max.rank =  d , approximate =  approximate , extra.args =  irlba.args , keep.left =  TRUE , keep.right =  TRUE )   pcs -   .svd_to_pca (  svd.out , ncomp =  d , named =  FALSE )   sim.pcs -   .spawn_doublet_pcs (  x ,  size.factors.norm , V =   svd.out $ v , centers =   rowMeans (  y ) , niters =  niters , block =  block ) # Force doublets to nearest neighbours in the original data set.   pre.pcs -   buildNNIndex (  pcs , BNPARAM =  BNPARAM )  if (  force.match )  {   closest -   queryKNN ( query =  sim.pcs , k =  force.k , BNINDEX =  pre.pcs , BPPARAM =  BPPARAM )   sim.pcs -   .compute_tricube_average (  pcs ,   closest $ index ,   closest $ distance , ndist =  force.ndist ) } # Computing densities, using a distance computed from the kth nearest neighbor.   self.dist -    findKNN ( BNINDEX =  pre.pcs , k =  k , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   dist2nth -   pmax (  1e-8 ,   median (   self.dist [ ,   ncol (  self.dist ) ] ) )  if (   is (  pre.pcs ,  \"KmknnIndex\" ) )  {   args -   list ( precomputed =  pre.pcs ) # skipping re-clustering if it's of the right type. } else  {   args -   list ( X =  pcs ) }   self.dist -    do.call (  findNeighbors ,   c (  args ,   list ( threshold =  dist2nth , BPPARAM =  BPPARAM , get.index =  FALSE ) ) ) $ distance   sim.dist -    queryNeighbors (  sim.pcs , query =  pcs , threshold =  dist2nth , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   rel.dens -   bpmapply ( FUN =  function ( self , sim , limit )  {    sum (   (   1 -   (   sim /  limit ) ^  3 ) ^  3 ) /    sum (   (   1 -   (   self /  limit ) ^  3 ) ^  3 ) ^  2 } , self =  self.dist , sim =  sim.dist , limit =  dist2nth , BPPARAM =  BPPARAM )   rel.dens /  (   niters /   ncol (  x ) ) } ",
    "replacementFunction": ".doublet_cells",
    "filename": "doubletCells.txt"
  },
  "new_function": {
    "name": "doubletCells",
    "representation": "doubletCells",
    "signature": "ANY",
    "parameters": "function ( x , size.factors.norm = NULL , size.factors.content = NULL , k = 50 , subset.row = NULL , niters = max ( 10000 , ncol ( x ) ) , block = 10000 , d = 50 , approximate = NULL , irlba.args = list ( ) , force.match = FALSE , force.k = 20 , force.ndist = 3 , BNPARAM = KmknnParam ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{  if (  !   is.null (  subset.row ) )  {   x -   x [  subset.row , , drop =  FALSE ] }  if (   is.null (  size.factors.norm ) )  {   size.factors.norm -   librarySizeFactors (  x ) }  if (  !   is.null (  size.factors.content ) )  {   x -   normalizeCounts (  x ,  size.factors.content , return_log =  FALSE , centre_size_factors =  FALSE )   size.factors.norm -   size.factors.norm /  size.factors.content }   y -   normalizeCounts (  x ,  size.factors.norm , centre_size_factors =  FALSE ) # Running the SVD.   svd.out -   .centered_SVD (   t (  y ) , max.rank =  d , approximate =  approximate , extra.args =  irlba.args , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM )   pcs -   .svd_to_pca (  svd.out , ncomp =  d , named =  FALSE )   sim.pcs -   .spawn_doublet_pcs (  x ,  size.factors.norm , V =   svd.out $ v , centers =   rowMeans (  y ) , niters =  niters , block =  block ) # Force doublets to nearest neighbours in the original data set.   pre.pcs -   buildIndex (  pcs , BNPARAM =  BNPARAM )  if (  force.match )  {   closest -   queryKNN ( query =  sim.pcs , k =  force.k , BNINDEX =  pre.pcs , BPPARAM =  BPPARAM )   sim.pcs -   .compute_tricube_average (  pcs ,   closest $ index ,   closest $ distance , ndist =  force.ndist ) } # Computing densities, using a distance computed from the kth nearest neighbor.   self.dist -    findKNN ( BNINDEX =  pre.pcs , k =  k , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   dist2nth -   pmax (  1e-8 ,   median (   self.dist [ ,   ncol (  self.dist ) ] ) )   self.dist -    findNeighbors ( threshold =  dist2nth , BNINDEX =  pre.pcs , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   sim.dist -    queryNeighbors (  sim.pcs , query =  pcs , threshold =  dist2nth , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   rel.dens -   bpmapply ( FUN =  function ( self , sim , limit )  {    sum (   (   1 -   (   sim /  limit ) ^  3 ) ^  3 ) /    sum (   (   1 -   (   self /  limit ) ^  3 ) ^  3 ) ^  2 } , self =  self.dist , sim =  sim.dist , limit =  dist2nth , BPPARAM =  BPPARAM )   rel.dens /  (   niters /   ncol (  x ) ) } ",
    "replacementFunction": ".doublet_cells",
    "filename": "doubletCells.txt"
  }
}

5.
{
  "old_function": {
    "name": "parallelPCA",
    "representation": "parallelPCA",
    "signature": "ANY",
    "parameters": "function ( x , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , niters = 50 , threshold = 0.1 , approximate = FALSE , irlba.args = list ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{ # Subsetting the matrix.   x0 -  x  if (  !   is.null (  subset.row ) )  {   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x -   x [  subset.row , , drop =  FALSE ] }   y -   t (  x ) # Running the PCA function once.   value -   match.arg (  value )   svd.out -   .centered_SVD (  y ,  max.rank , approximate =  approximate , extra.args =  irlba.args , keep.left =  (   value !=  \"n\" ) , keep.right =  (   value ==  \"lowrank\" ) ) # Running it once, and then multiple times after permutation.   original.d2 -    svd.out $ d ^  2   permuted -   bplapply (   rep (  max.rank ,  niters ) , FUN =  .parallel_PA , y =  y , approximate =  approximate , extra.args =  irlba.args , BPPARAM =  BPPARAM )   permutations -   do.call (  cbind ,  permuted ) # Figuring out where the original drops to \"within range\" of permuted.   prop -   rowMeans (   permutations =  original.d2 )   above -   prop undefined  threshold  if (  !   any (  above ) )  {   npcs -   length (  above ) } else  {   npcs -    min (   which (  above ) ) -  1L }   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  original.d2 ) ) # Collating the return value.   out.val -   switch (  value , n =  npcs , pca =   .svd_to_pca (  svd.out ,  npcs ) , lowrank =   .svd_to_lowrank (  svd.out ,  npcs ,  x0 ,  subset.row ) )   var.exp -   original.d2 /  (    ncol (  x ) -  1 )   all.var -   sum (   rowVars (   DelayedArray (  x ) ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /  all.var    attr (  out.val ,  \"permuted.percentVar\" ) -     t (  permutations ) /  (    ncol (  x ) -  1L ) /  all.var   return (  out.val ) } ",
    "replacementFunction": ".parallelPCA",
    "filename": "parallelPCA.txt"
  },
  "new_function": {
    "name": "parallelPCA",
    "representation": "parallelPCA",
    "signature": "ANY",
    "parameters": "function ( x , subset.row = NULL , value = c ( \"pca\" , \"n\" , \"lowrank\" ) , min.rank = 5 , max.rank = 100 , niters = 50 , threshold = 0.1 , approximate = NULL , irlba.args = list ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{ # Subsetting the matrix.   x0 -  x  if (  !   is.null (  subset.row ) )  {   subset.row -   .subset_to_index (  subset.row ,  x , byrow =  TRUE )   x -   x [  subset.row , , drop =  FALSE ] }   y -   t (  x ) # Running the PCA function once.   value -   match.arg (  value )   svd.out -   .centered_SVD (  y ,  max.rank , approximate =  approximate , extra.args =  irlba.args , keep.left =  (   value !=  \"n\" ) , keep.right =  (   value ==  \"lowrank\" ) , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM )   original.d2 -    svd.out $ d ^  2 # Running it once, and then multiple times after permutation.   pcg.states -   .setup_pcg_state (  niters )   permuted -   bpmapply ( FUN =  .parallel_PA , max.rank =   rep (  max.rank ,  niters ) , seed =    pcg.states $ seeds [[  1 ] ] , stream =    pcg.states $ streams [[  1 ] ] , MoreArgs =   list ( y =  y , approximate =  approximate , extra.args =  irlba.args , BSPARAM =  BSPARAM ) , BPPARAM =  BPPARAM , SIMPLIFY =  FALSE , USE.NAMES =  FALSE )   permutations -   do.call (  cbind ,  permuted ) # Figuring out where the original drops to \"within range\" of permuted.   prop -   rowMeans (   permutations =  original.d2 )   above -   prop undefined  threshold  if (  !   any (  above ) )  {   npcs -   length (  above ) } else  {   npcs -    min (   which (  above ) ) -  1L }   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  original.d2 ) ) # Collating the return value.   out.val -   switch (  value , n =  npcs , pca =   .svd_to_pca (  svd.out ,  npcs ) , lowrank =   .svd_to_lowrank (  svd.out ,  npcs ,  x0 ,  subset.row ) )   var.exp -   original.d2 /  (    ncol (  x ) -  1 )   all.var -   sum (   rowVars (   DelayedArray (  x ) ) )    attr (  out.val ,  \"percentVar\" ) -   var.exp /  all.var    attr (  out.val ,  \"permuted.percentVar\" ) -     t (  permutations ) /  (    ncol (  x ) -  1L ) /  all.var   return (  out.val ) } ",
    "replacementFunction": ".parallelPCA",
    "filename": "parallelPCA.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_9 scran_release_3_11

{
    "package": "scran",
    "release_versions": "scran_release_3_9 scran_release_3_11",
    "desc_release_old": "1.12.1",
    "desc_release_new": "1.16.0",
    "old_release_number": 6,
    "new_release_number": 7,
    "function_removals": 5,
    "function_additions": 29,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 7,
    "total_count": 12
}

##########
Functions Removed
##########

cosineNorm
fastMNN
mnnCorrect
multiBatchNorm
multiBatchPCA


##########
Functions Added
##########

bootstrapCluster
calculateSumFactors
clusterKNNGraph
clusterPurity
clusterSNNGraph
coassignProb
combineCV2
decideTestsPerLabel
doubletRecovery
fitTrendCV2
fitTrendPoisson
fitTrendVar
getClusteredPCs
getDenoisedPCs
getMarkerEffects
getTopHVGs
getTopMarkers
modelGeneCV2
modelGeneCV2WithSpikes
modelGeneVar
modelGeneVarByPoisson
modelGeneVarWithSpikes
multiMarkerStats
neighborsToKNNGraph
neighborsToSNNGraph
pairwiseBinom
pseudoBulkDGE
quickSubCluster
summarizeTestsPerLabel


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , type = c ( \"rank\" , \"number\" ) , transposed = FALSE , pc.approx = NULL , irlba.args = list ( ) , subset.row = NULL , BNPARAM = KmknnParam ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Building the SNN graph.   type -   match.arg (  type )  if (   type ==  \"rank\" )  {   g.out -   .Call (  cxx_build_snn_rank ,   nn.out $ index ) } else  {   g.out -   .Call (  cxx_build_snn_number ,   nn.out $ index ) }   edges -   g.out [[  1 ] ]   weights -   g.out [[  2 ] ]   g -   make_graph (  edges , directed =  FALSE )     E (  g ) $ weight -  weights   g -   simplify (  g , edge.attr.comb =  \"first\" ) # symmetric, so doesn't really matter.   return (  g ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildSNNGraph",
    "representation": "buildSNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , type = c ( \"rank\" , \"number\" , \"jaccard\" ) , transposed = FALSE , subset.row = NULL , BNPARAM = KmknnParam ( ) , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , k =  k , BNPARAM =  BNPARAM , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM )   neighborsToSNNGraph (   nn.out $ index , type =   match.arg (  type ) ) } ",
    "replacementFunction": ".buildSNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

1.
{
  "old_function": {
    "name": "buildKNNGraph",
    "representation": "buildKNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , directed = FALSE , transposed = FALSE , pc.approx = NULL , irlba.args = list ( ) , subset.row = NULL , BNPARAM = KmknnParam ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , pc.approx =  pc.approx , irlba.args =  irlba.args , k =  k , BNPARAM =  BNPARAM , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Building the KNN graph.   start -   as.vector (   row (   nn.out $ index ) )   end -   as.vector (   nn.out $ index )   interleaved -   as.vector (   rbind (  start ,  end ) )  if (  directed )  {   g -   make_graph (  interleaved , directed =  TRUE ) } else  {   g -   make_graph (  interleaved , directed =  FALSE )   g -   simplify (  g , edge.attr.comb =  \"first\" ) }   return (  g ) } ",
    "replacementFunction": ".buildKNNGraph",
    "filename": "buildSNNGraph.txt"
  },
  "new_function": {
    "name": "buildKNNGraph",
    "representation": "buildKNNGraph",
    "signature": "ANY",
    "parameters": "function ( x , k = 10 , d = 50 , directed = FALSE , transposed = FALSE , subset.row = NULL , BNPARAM = KmknnParam ( ) , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   nn.out -   .setup_knn_data ( x =  x , subset.row =  subset.row , d =  d , transposed =  transposed , k =  k , BNPARAM =  BNPARAM , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM )   neighborsToKNNGraph (   nn.out $ index , directed =  directed ) } ",
    "replacementFunction": ".buildKNNGraph",
    "filename": "buildSNNGraph.txt"
  }
}

2.
{
  "old_function": {
    "name": "clusterModularity",
    "representation": "clusterModularity",
    "parameters": "function ( graph , clusters , get.values = FALSE )",
    "body": "{   by.clust -   split (   seq_along (  clusters ) ,  clusters )   uclust -   names (  by.clust )   nclust -   length (  uclust )   mod.mat -   matrix (  0 ,  nclust ,  nclust )    rownames (  mod.mat ) -    colnames (  mod.mat ) -  uclust # Calculating the observed weight within/between clusters.   grmat -   graph [ ]  for  ( x in   seq_along (  by.clust ) )  {  for  ( y in   seq_len (  x ) )  {   current -   by.clust [[  x ] ]   other -   by.clust [[  y ] ]    mod.mat [  y ,  x ] -    mod.mat [  x ,  y ] -   sum (   grmat [  current ,  other ] ) } } # Calcuating the expected weight if they were randomly distributed.   total.weight -   sum (  mod.mat )   clust.prop -    colSums (  mod.mat ) /  total.weight   expected.mat -    tcrossprod (  clust.prop ) *  total.weight  if (  get.values )  {   return (   list ( observed =  mod.mat , expected =  expected.mat ) ) } else  {   return (    1 /  total.weight *  (   mod.mat -  expected.mat ) ) } } ",
    "filename": "clusterModularity.txt"
  },
  "new_function": {
    "name": "clusterModularity",
    "representation": "clusterModularity",
    "parameters": "function ( graph , clusters , get.weights = FALSE , as.ratio = FALSE )",
    "body": "{   by.clust -   split (   seq_along (  clusters ) ,  clusters )   uclust -   names (  by.clust )   nclust -   length (  uclust )   mod.mat -   matrix (  0 ,  nclust ,  nclust )    dimnames (  mod.mat ) -   list (  uclust ,  uclust )   clust.total -   numeric (  nclust )    names (  clust.total ) -  uclust   fullmat -   graph [ ] # Calculating the observed weight within/between clusters.  for  ( x in   seq_along (  by.clust ) )  {   current -   by.clust [[  x ] ]  for  ( y in   seq_len (  x ) )  {   other -   by.clust [[  y ] ]   grmat -   fullmat [  current ,  other , drop =  FALSE ]   grsum -   sum (  grmat )  if (   x ==  y )  {   old.diag -   sum (   diag (  grmat ) )    diag (  grmat ) -  0   self.sum -   sum (  grmat )  if (  !   is.directed (  graph ) )  { # Only count undirected edges within a cluster once.   self.sum -   self.sum /  2 } else  { # Need to count directed edges between different nodes twice.   grsum -   grsum +  self.sum } # Self-edges contribute twice to total node weight, # according to igraph::modularity.   grsum -   grsum +  old.diag    mod.mat [  x ,  y ] -   self.sum +  old.diag } else  {  if (   is.directed (  graph ) )  {   grsum -   grsum +   sum (   graph [  other ,  current ] ) }    mod.mat [  y ,  x ] -  grsum } # If x==y, this is equivalent to adding edge weights for each node twice. # THIS IS DELIBERATE; the total per-node weight is that for all edges # involving each node, so edges between nodes in the same cluster are # allowed to be double-counted here when aggregating node weights per cluster.    clust.total [  x ] -    clust.total [  x ] +  grsum  if (   x !=  y )  {    clust.total [  y ] -    clust.total [  y ] +  grsum } } } # Calcuating the expected weight if they were randomly distributed. # Note some effort is involved in 'folding' it into an upper triangular.   total.weight -   sum (  mod.mat )   clust.prop -   clust.total /   sum (  clust.total )   expected.mat -    tcrossprod (  clust.prop ) *  total.weight    expected.mat [   lower.tri (  expected.mat ) ] -  0   old.diag -   diag (  expected.mat )   expected.mat -   expected.mat *  2    diag (  expected.mat ) -  old.diag  if (  get.weights )  {   list ( observed =  mod.mat , expected =  expected.mat ) } else  if (  as.ratio )  {   mod.mat /  expected.mat } else  {    1 /  total.weight *  (   mod.mat -  expected.mat ) } } ",
    "filename": "clusterModularity.txt"
  }
}

3.
{
  "old_function": {
    "name": "combineMarkers",
    "representation": "combineMarkers",
    "parameters": "function ( de.lists , pairs , pval.field = \"p.value\" , effect.field = \"logFC\" , pval.type = c ( \"any\" , \"all\" ) , log.p.in = FALSE , log.p.out = log.p.in , output.field = NULL , full.stats = FALSE )",
    "body": "{  if (    length (  de.lists ) !=   nrow (  pairs ) )  {   stop (  \"'nrow(pairs)' must be equal to 'length(de.lists)'\" ) }  if (   is.null (  output.field ) )  {   output.field -  if (  full.stats )  \"stats\" else  effect.field }   pval.type -   match.arg (  pval.type )   method -   switch (  pval.type , any =  \"simes\" , all =  \"berger\" ) # Checking that all genes are the same across lists.   gene.names -  NULL  for  ( x in   seq_along (  de.lists ) )  {   current -   de.lists [[  x ] ]   curnames -   rownames (  current )  if (   is.null (  gene.names ) )  {   gene.names -  curnames } else  if (  !   identical (  gene.names ,  curnames ) )  {   stop (  \"row names should be the same for all elements of 'de.lists'\" ) } } # Processing by the first element of each pair.   first.fac -   factor (   pairs [ ,  1 ] ,   unique (   pairs [ ,  1 ] ) )   by.first -   split (   seq_along (  de.lists ) ,  first.fac , drop =  TRUE )   output -   vector (  \"list\" ,   length (  by.first ) )    names (  output ) -   names (  by.first )  for  ( host in   names (  by.first ) )  {   chosen -   by.first [[  host ] ]   targets -   pairs [  chosen ,  2 ]   cur.stats -   de.lists [  chosen ]   keep -  !   is.na (  targets )   targets -   targets [  keep ]   cur.stats -   cur.stats [  keep ]   all.p -   lapply (  cur.stats ,  \"[[\" , i =  pval.field )   pval -   do.call (  combinePValues ,   c (  all.p ,   list ( method =  method , log.p =  log.p.in ) ) )   preamble -   DataFrame ( row.names =  gene.names ) # Determining rank.  if (   pval.type ==  \"any\" )  {   rank.out -   .rank_top_genes (  all.p )   min.rank -   rank.out $ rank   min.p -   rank.out $ value   gene.order -   order (  min.rank ,  min.p )    preamble $ Top -  min.rank } else  {   gene.order -   order (  pval ) } # Correcting for multiple testing. We try to preserve the log-ness as long as we can, # to avoid underflow upon exp()'ing that could be avoided by correction.  if (  log.p.in )  {   corrected -   .logBH (  pval ) } else  {   corrected -   p.adjust (  pval , method =  \"BH\" ) }  if (   log.p.out !=  log.p.in )  {   transFUN -  if (  log.p.out )  log else  exp   pval -   transFUN (  pval )   corrected -   transFUN (  corrected ) }   prefix -  if (  log.p.out )  \"log.\" else  \"\"    preamble [[   paste0 (  prefix ,  \"p.value\" ) ] ] -  pval    preamble [[   paste0 (  prefix ,  \"FDR\" ) ] ] -  corrected # Saving effect sizes or all statistics.  if (  full.stats )  {   cur.stats -   lapply (  cur.stats , FUN =  function ( x )  {   I (   as (  x , Class =  \"DataFrame\" ) ) } )   stat.df -   do.call (  DataFrame ,   c (  cur.stats ,   list ( check.names =  FALSE ) ) ) } else  {   all.effects -   lapply (  cur.stats ,  \"[[\" , i =  effect.field )   stat.df -   DataFrame (  all.effects ) }    colnames (  stat.df ) -   sprintf (  \"%s.%s\" ,  output.field ,  targets ) # Producing the output object.   marker.set -   cbind (  preamble ,  stat.df )   marker.set -   marker.set [  gene.order , , drop =  FALSE ]    output [[  host ] ] -  marker.set }   return (   as (  output ,  \"List\" ) ) } ",
    "filename": "combineMarkers.txt"
  },
  "new_function": {
    "name": "combineMarkers",
    "representation": "combineMarkers",
    "parameters": "function ( de.lists , pairs , pval.field = \"p.value\" , effect.field = \"logFC\" , pval.type = c ( \"any\" , \"some\" , \"all\" ) , min.prop = NULL , log.p.in = FALSE , log.p.out = log.p.in , output.field = NULL , full.stats = FALSE , sorted = TRUE , flatten = TRUE , BPPARAM = SerialParam ( ) )",
    "body": "{  if (    length (  de.lists ) !=   nrow (  pairs ) )  {   stop (  \"'nrow(pairs)' must be equal to 'length(de.lists)'\" ) }  if (   is.null (  output.field ) )  {   output.field -  if (  full.stats )  \"stats\" else  effect.field }   report.effects -  !   is.null (  effect.field )   pval.type -   match.arg (  pval.type )   method -   switch (  pval.type , any =  \"simes\" , some =  \"holm-middle\" , all =  \"berger\" )  if (   is.null (  min.prop ) )  {   min.prop -  if (   pval.type ==  \"any\" )  0 else  0.5 } # Checking that all genes are the same across lists.   gene.names -  NULL  for  ( x in   seq_along (  de.lists ) )  {   current -   de.lists [[  x ] ]   curnames -   rownames (  current )  if (   is.null (  gene.names ) )  {   gene.names -  curnames } else  if (  !   identical (  gene.names ,  curnames ) )  {   stop (  \"row names should be the same for all elements of 'de.lists'\" ) } } # Processing by the first element of each pair.   first.fac -   factor (   pairs [ ,  1 ] ,   unique (   pairs [ ,  1 ] ) )   by.first -   split (   seq_along (  de.lists ) ,  first.fac , drop =  TRUE )   output -   bplapply (  by.first , FUN =  .combine_markers_internal , pairs =  pairs , de.lists =  de.lists , method =  method , gene.names =  gene.names , report.effects =  report.effects , pval.type =  pval.type , effect.field =  effect.field , min.prop =  min.prop , log.p.in =  log.p.in , log.p.out =  log.p.out , pval.field =  pval.field , output.field =  output.field , full.stats =  full.stats , sorted =  sorted , flatten =  flatten , BPPARAM =  BPPARAM )   SimpleList (  output ) } ",
    "filename": "combineMarkers.txt"
  }
}

4.
{
  "old_function": {
    "name": "correlateNull",
    "representation": "correlateNull",
    "parameters": "function ( ncells , iters = 1e6 , block = NULL , design = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{  if (  !   is.null (  block ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'block'\" ) }   groupings -   table (  block ) # Estimating the correlation as a weighted mean of the correlations in each group. # This avoids the need for the normality assumption in the residual effect simulation.   out -  0  for  ( ngr in  groupings )  {   out.g -   .within_block_null ( ncells =  ngr , iters =   as.integer (  iters ) , BPPARAM =  BPPARAM )   out -   out +   out.g *  ngr }   out -   out /   length (  block )   attrib -   list ( block =  block ) } else  if (  !   is.null (  design ) )  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'design'\" ) } # Using residual effects to compute the correlations.   QR -   .ranksafe_qr (  design )   iters.per.core -   .niters_by_nworkers (   as.integer (  iters ) ,  BPPARAM )   pcg.states -   .setup_pcg_state (  iters.per.core )   out -   bpmapply ( iters =  iters.per.core , seeds =   pcg.states $ seeds , streams =   pcg.states $ streams , MoreArgs =   list ( QR =  QR ) , FUN =  .with_design_null , SIMPLIFY =  FALSE , USE.NAMES =  FALSE )   out -   unlist (  out )   attrib -   list ( design =  design ) } else  {   out -   .within_block_null ( iters =   as.integer (  iters ) , ncells =   as.integer (  ncells ) , BPPARAM =  BPPARAM )   attrib -  NULL } # Storing attributes, to make sure it matches up.   out -   sort (  out )    attributes (  out ) -  attrib   return (  out ) } ",
    "filename": "correlateNull.txt"
  },
  "new_function": {
    "name": "correlateNull",
    "representation": "correlateNull",
    "parameters": "function ( ncells , iters = 1e6 , block = NULL , design = NULL , equiweight = TRUE , BPPARAM = SerialParam ( ) )",
    "body": "{  if (  !   bpisup (  BPPARAM ) )  {   bpstart (  BPPARAM )   on.exit (   bpstop (  BPPARAM ) ) }   iters -   as.integer (  iters )   iters.per.core -   .niters_by_nworkers (  iters ,  BPPARAM )   blockFUN -  function ( group )  {   pcg.state -   .setup_pcg_state (  iters.per.core )   out.g -   bpmapply ( Niters =  iters.per.core , Seeds =   pcg.state $ seeds , Streams =   pcg.state $ streams , MoreArgs =   list ( Ncells =   length (  group ) ) , FUN =  get_null_rho , SIMPLIFY =  FALSE , USE.NAMES =  FALSE , BPPARAM =  BPPARAM )   unlist (  out.g ) }   designFUN -  function ( QR )  {   pcg.state -   .setup_pcg_state (  iters.per.core )   out.g -   bpmapply ( Niters =  iters.per.core , Seeds =   pcg.state $ seeds , Streams =   pcg.state $ streams , MoreArgs =   list ( qr =   QR $ qr , qraux =   QR $ qraux ) , FUN =  get_null_rho_design , SIMPLIFY =  FALSE , USE.NAMES =  FALSE , BPPARAM =  BPPARAM )   unlist (  out.g ) } # Additional error messages.  if (   is.null (  design ) )  {  if (   !   is.null (  block ) undefined  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'block'\" ) } } else  {  if (  !   missing (  ncells ) )  {   stop (  \"cannot specify both 'ncells' and 'design'\" ) } else  if (  !   is.null (  block ) )  {   stop (  \"cannot specify both 'block' and 'design'\" ) } }   .correlator_base (  ncells ,  block ,  design ,  equiweight ,  blockFUN ,  designFUN ,  BPPARAM ,  iters ) } ",
    "filename": "correlateNull.txt"
  }
}

5.
{
  "old_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , ties.method = c ( \"expected\" , \"average\" ) , tol = 1e-8 , iters = 1e6 , block = NULL , design = NULL , lower.bound = NULL , use.names = TRUE , subset.row = NULL , pairings = NULL , per.gene = FALSE , cache.size = 100L , BPPARAM = SerialParam ( ) )",
    "body": "{   null.out -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , null.dist =  null.dist , BPPARAM =  BPPARAM )   null.dist -   null.out $ null   by.block -   null.out $ blocks # Checking which pairwise correlations should be computed.   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder # Computing residuals (setting values that were originally zero to a lower bound). # Also replacing the subset vector, as it'll already be subsetted.  if (   !   is.null (  design ) undefined   is.null (  block ) )  {   use.x -   .calc_residuals_wt_zeroes (  x ,  design , subset.row =  subset.row , lower.bound =  lower.bound )   use.subset.row -   seq_len (   nrow (  use.x ) ) } else  {   use.x -  x   use.subset.row -  subset.row } # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .worker_assign (   length (  gene1 ) ,  BPPARAM )   sgene1 -   .split_vector_by_workers (   gene1 -  1L ,  wout )   sgene2 -   .split_vector_by_workers (   gene2 -  1L ,  wout )   all.rho -   .calc_blocked_rho (  sgene1 ,  sgene2 , x =  use.x , subset.row =  use.subset.row , by.block =  by.block , tol =  tol , ties.method =   match.arg (  ties.method ) , BPPARAM =  BPPARAM )   stats -   .rho_to_pval (  all.rho ,  null.dist )   all.pval -   stats $ p   all.lim -   stats $ limited # Formatting the output.   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  all.lim )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )  if (  per.gene )  {   .Deprecated ( msg =  \"'per.gene=' is deprecated.\\nUse 'correlateGenes' instead.\" )   out -   correlateGenes (  out ) }   return (  out ) } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  },
  "new_function": {
    "name": "correlatePairs",
    "representation": "correlatePairs",
    "signature": "ANY",
    "parameters": "function ( x , null.dist = NULL , ties.method = c ( \"expected\" , \"average\" ) , iters = 1e6 , block = NULL , design = NULL , equiweight = TRUE , use.names = TRUE , subset.row = NULL , pairings = NULL , BPPARAM = SerialParam ( ) )",
    "body": "{ # Checking which pairwise correlations should be computed.   pair.out -   .construct_pair_indices ( subset.row =  subset.row , x =  x , pairings =  pairings )   subset.row -   pair.out $ subset.row   gene1 -   pair.out $ gene1   gene2 -   pair.out $ gene2   reorder -   pair.out $ reorder  if (   .bpNotSharedOrUp (  BPPARAM ) )  {   bpstart (  BPPARAM )   on.exit (   bpstop (  BPPARAM ) ) }   null.dist -   .check_null_dist (  x , block =  block , design =  design , iters =  iters , equiweight =  equiweight , null.dist =  null.dist , BPPARAM =  BPPARAM )   ties.method -   match.arg (  ties.method ) # Splitting up gene pairs into jobs for multicore execution, converting to 0-based indices.   wout -   .assignIndicesToWorkers (   length (  gene1 ) ,  BPPARAM )   sgene1 -   .splitVectorByWorkers (   gene1 -  1L ,  BPPARAM , assignments =  wout )   sgene2 -   .splitVectorByWorkers (   gene2 -  1L ,  BPPARAM , assignments =  wout )   blockFUN -  function ( subset.col )  {   .calculate_rho (  sgene1 ,  sgene2 , x =  x , subset.row =  subset.row , subset.col =  subset.col , ties.method =  ties.method , BPPARAM =  BPPARAM ) }   designFUN -  function ( QR )  {   x -   get_residuals (  x ,   QR $ qr ,   QR $ qraux ,   subset.row -  1L ,  NA_real_ )   .calculate_rho (  sgene1 ,  sgene2 , x =  x , subset.row =  NULL , subset.col =  NULL , ties.method =  ties.method , BPPARAM =  BPPARAM ) }  if (   !   is.null (  block ) undefined  !   is.null (  design ) )  {   stop (  \"cannot specify both 'block' and 'design'\" ) }   all.rho -   .correlator_base (   ncol (  x ) ,  block ,  design ,  equiweight ,  blockFUN ,  designFUN ,  BPPARAM ,   length (  gene1 ) ) # Computing p-values and formatting the output.   stats -   .rho_to_pval (  all.rho ,  null.dist )   all.pval -   stats $ p   all.lim -   stats $ limited   final.names -   .choose_gene_names ( subset.row =  subset.row , x =  x , use.names =  use.names )   gene1 -   final.names [  gene1 ]   gene2 -   final.names [  gene2 ]   out -   DataFrame ( gene1 =  gene1 , gene2 =  gene2 , rho =  all.rho , p.value =  all.pval , FDR =   p.adjust (  all.pval , method =  \"BH\" ) , limited =  all.lim )  if (  reorder )  {   out -   out [   order (   out $ p.value ,  -   abs (   out $ rho ) ) , ]    rownames (  out ) -  NULL }   .is_sig_limited (  out )  out } ",
    "replacementFunction": ".correlate_pairs",
    "filename": "correlatePairs.txt"
  }
}

6.
{
  "old_function": {
    "name": "doubletCells",
    "representation": "doubletCells",
    "signature": "ANY",
    "parameters": "function ( x , size.factors.norm = NULL , size.factors.content = NULL , k = 50 , subset.row = NULL , niters = max ( 10000 , ncol ( x ) ) , block = 10000 , d = 50 , approximate = NULL , irlba.args = list ( ) , force.match = FALSE , force.k = 20 , force.ndist = 3 , BNPARAM = KmknnParam ( ) , BSPARAM = ExactParam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{  if (  !   is.null (  subset.row ) )  {   x -   x [  subset.row , , drop =  FALSE ] }  if (   is.null (  size.factors.norm ) )  {   size.factors.norm -   librarySizeFactors (  x ) }  if (  !   is.null (  size.factors.content ) )  {   x -   normalizeCounts (  x ,  size.factors.content , return_log =  FALSE , centre_size_factors =  FALSE )   size.factors.norm -   size.factors.norm /  size.factors.content }   y -   normalizeCounts (  x ,  size.factors.norm , centre_size_factors =  FALSE ) # Running the SVD.   svd.out -   .centered_SVD (   t (  y ) , max.rank =  d , approximate =  approximate , extra.args =  irlba.args , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM )   pcs -   .svd_to_pca (  svd.out , ncomp =  d , named =  FALSE )   sim.pcs -   .spawn_doublet_pcs (  x ,  size.factors.norm , V =   svd.out $ v , centers =   rowMeans (  y ) , niters =  niters , block =  block ) # Force doublets to nearest neighbours in the original data set.   pre.pcs -   buildIndex (  pcs , BNPARAM =  BNPARAM )  if (  force.match )  {   closest -   queryKNN ( query =  sim.pcs , k =  force.k , BNINDEX =  pre.pcs , BPPARAM =  BPPARAM )   sim.pcs -   .compute_tricube_average (  pcs ,   closest $ index ,   closest $ distance , ndist =  force.ndist ) } # Computing densities, using a distance computed from the kth nearest neighbor.   self.dist -    findKNN ( BNINDEX =  pre.pcs , k =  k , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   dist2nth -   pmax (  1e-8 ,   median (   self.dist [ ,   ncol (  self.dist ) ] ) )   self.dist -    findNeighbors ( threshold =  dist2nth , BNINDEX =  pre.pcs , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   sim.dist -    queryNeighbors (  sim.pcs , query =  pcs , threshold =  dist2nth , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM , get.index =  FALSE ) $ distance   rel.dens -   bpmapply ( FUN =  function ( self , sim , limit )  {    sum (   (   1 -   (   sim /  limit ) ^  3 ) ^  3 ) /    sum (   (   1 -   (   self /  limit ) ^  3 ) ^  3 ) ^  2 } , self =  self.dist , sim =  sim.dist , limit =  dist2nth , BPPARAM =  BPPARAM )   rel.dens /  (   niters /   ncol (  x ) ) } ",
    "replacementFunction": ".doublet_cells",
    "filename": "doubletCells.txt"
  },
  "new_function": {
    "name": "doubletCells",
    "representation": "doubletCells",
    "signature": "ANY",
    "parameters": "function ( x , size.factors.norm = NULL , size.factors.content = NULL , k = 50 , subset.row = NULL , niters = max ( 10000 , ncol ( x ) ) , block = 10000 , d = 50 , force.match = FALSE , force.k = 20 , force.ndist = 3 , BNPARAM = KmknnParam ( ) , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{ # Setting up the parallelization.   old -   getAutoBPPARAM ( )   setAutoBPPARAM (  BPPARAM )   on.exit (   setAutoBPPARAM (  old ) )  if (   .bpNotSharedOrUp (  BPPARAM ) )  {   bpstart (  BPPARAM )   on.exit (   bpstop (  BPPARAM ) ) }  if (  !   is.null (  subset.row ) )  {   x -   x [  subset.row , , drop =  FALSE ] }  if (   is.null (  size.factors.norm ) )  {   size.factors.norm -   librarySizeFactors (  x , BPPARAM =  BPPARAM ) } # Manually controlling the size factor centering here to ensure the final counts are on the same scale.   size.factors.norm -   size.factors.norm /   mean (  size.factors.norm )  if (  !   is.null (  size.factors.content ) )  {   x -   normalizeCounts (  x ,  size.factors.content , log =  FALSE , center_size_factors =  FALSE )   size.factors.norm -   size.factors.norm /  size.factors.content }   y -   normalizeCounts (  x ,  size.factors.norm , center_size_factors =  FALSE ) # Running the SVD.   svd.out -   .centered_SVD (   t (  y ) , max.rank =  d , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM )   pcs -   .svd_to_pca (  svd.out , ncomp =  d , named =  FALSE )   sim.pcs -   .spawn_doublet_pcs (  x ,  size.factors.norm , V =   svd.out $ v , centers =   rowMeans (  y ) , niters =  niters , block =  block ) # Force doublets to nearest neighbours in the original data set.   pre.pcs -   buildIndex (  pcs , BNPARAM =  BNPARAM )  if (  force.match )  {   closest -   queryKNN ( query =  sim.pcs , k =  force.k , BNINDEX =  pre.pcs , BPPARAM =  BPPARAM )   sim.pcs -   .compute_tricube_average (  pcs ,   closest $ index ,   closest $ distance , ndist =  force.ndist ) } # Computing densities, using a distance computed from the kth nearest neighbor.   self.dist -    findKNN ( BNINDEX =  pre.pcs , k =  k , BPPARAM =  BPPARAM , last =  1 , get.index =  FALSE , warn.ties =  FALSE ) $ distance   dist2nth -   pmax (  1e-8 ,   median (  self.dist ) )   self.n -   findNeighbors ( threshold =  dist2nth , BNINDEX =  pre.pcs , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM , get.distance =  FALSE , get.index =  FALSE )   sim.n -   queryNeighbors (  sim.pcs , query =  pcs , threshold =  dist2nth , BNPARAM =  BNPARAM , BPPARAM =  BPPARAM , get.distance =  FALSE , get.index =  FALSE )   self.prop -   self.n /   ncol (  x )   sim.prop -   sim.n /  niters   sim.prop /   self.prop ^  2 } ",
    "replacementFunction": ".doublet_cells",
    "filename": "doubletCells.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_11 scran_release_3_12

{
    "package": "scran",
    "release_versions": "scran_release_3_11 scran_release_3_12",
    "desc_release_old": "1.16.0",
    "desc_release_new": "1.18.7",
    "old_release_number": 7,
    "new_release_number": 8,
    "function_removals": 0,
    "function_additions": 10,
    "parameter_removals": 0,
    "parameter_additions": 1,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 2,
    "total_count": 2
}

##########
Functions Removed
##########



##########
Functions Added
##########

.logBH
combineBlocks
connectClusterMST
createClusterMST
orderClusterMST
pseudoBulkSpecific
quickPseudotime
summaryMarkerStats
testLinearModel
testPseudotime


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########

0.
{
  "old_function": {
    "name": "pseudoBulkDGE",
    "representation": "pseudoBulkDGE",
    "signature": "ANY",
    "parameters": "function ( x , sample , label , design , coef = ncol ( design ) , contrast = NULL , condition = NULL , lfc = 0 )",
    "body": "{   sample -   as.character (  sample )   label -   as.character (  label )  if (  !   identical (   sort (   rownames (  design ) ) ,   sort (   unique (  sample ) ) ) )  {   stop (  \"'rownames(design)' and 'sample' should have the same levels\" ) }   de.results -   list ( )   failed -   character (  0 )  for  ( i in   sort (   unique (  label ) ) )  {   chosen -   i ==  label   curx -   x [ ,  chosen , drop =  FALSE ]   y -   DGEList (  curx , samples =   data.frame ( sample =   sample [  chosen ] , stringsAsFactors =  FALSE ) )   m -   match (   as.character (    y $ samples $ sample ) ,   rownames (  design ) )   curdesign -   design [  m , , drop =  FALSE ]   curcond -   condition [  m ]   gkeep -   filterByExpr (  y , design =  curdesign , group =  curcond )   y -   y [  gkeep , ]   y -   calcNormFactors (  y )   rank -    qr (  curdesign ) $ rank  if (    rank ==   nrow (  curdesign ) ||   rank undefined   ncol (  curdesign ) )  {   failed -   c (  failed ,  i )   res -   try (  {   fit -   glmFit (  y ,  curdesign , dispersion =  0.05 )  if (   lfc ==  0 )  {   glmLRT (  fit , coef =  coef , contrast =  contrast ) } else  {   glmTreat (  fit , lfc =  lfc , coef =  coef , contrast =  contrast ) } } , silent =  TRUE )  if (   is (  res ,  \"try-error\" ) )  {   refnames -   c (  \"logFC\" ,  if (   lfc !=  0 )  \"unshrunk.logFC\" ,  \"logCPM\" ,  \"PValue\" ,  \"LR\" ,  \"FDR\" )   empty -   lapply (  refnames ,  function ( i )   rep (  NA_real_ ,   nrow (  x ) ) )    names (  empty ) -  refnames   empty -   DataFrame (  empty , row.names =   rownames (  x ) )    de.results [[  i ] ] -  empty  next } else  {     res $ table $ PValue -   rep (  NA_real_ ,   nrow (   res $ table ) ) } } else  {   y -   estimateDisp (  y ,  curdesign )   fit -   glmQLFit (  y ,  curdesign , robust =  TRUE )  if (   lfc ==  0 )  {   res -   glmQLFTest (  fit , coef =  coef , contrast =  contrast ) } else  {   res -   glmTreat (  fit , lfc =  lfc , coef =  coef , contrast =  contrast ) } }   tab -   topTags (  res , n =  Inf , sort.by =  \"none\" )   expander -   match (   seq_len (   nrow (  x ) ) ,   which (  gkeep ) )   tab -   DataFrame (    tab $ table [  expander , , drop =  FALSE ] )    rownames (  tab ) -   rownames (  x )    de.results [[  i ] ] -  tab }   output -   List (  de.results )     metadata (  output ) $ failed -  failed  output } ",
    "replacementFunction": ".pseudo_bulk_dge",
    "filename": "pseudoBulkDGE.txt"
  },
  "new_function": {
    "name": "pseudoBulkDGE",
    "representation": "pseudoBulkDGE",
    "signature": "ANY",
    "parameters": "function ( x , col.data , label , design , coef , contrast = NULL , condition = NULL , lfc = 0 , include.intermediates = TRUE , row.data = NULL , sorted = FALSE , method = c ( \"edgeR\" , \"voom\" ) , qualities = TRUE , robust = TRUE , sample = NULL )",
    "body": "{  if (    is.function (  design ) ||   is (  design ,  \"formula\" ) )  {   .pseudo_bulk_dge ( x =  x , col.data =  col.data , label =  label , condition =  condition , design =  design , coef =  coef , contrast =  contrast , lfc =  lfc , row.data =  row.data , sorted =  sorted , include.intermediates =  include.intermediates , method =   match.arg (  method ) , qualities =  qualities , robust =  robust ) } else  {   .Deprecated ( msg =  \"matrix arguments for 'design=' are deprecated. \\nUse functions or formulas instead.\" )   .pseudo_bulk_old ( x =  x , label =  label , sample =  sample , design =  design , condition =  condition , coef =  coef , contrast =  contrast , lfc =  lfc ) } } ",
    "replacementFunction": ".pseudo_bulk_master",
    "filename": "pseudoBulkDGE.txt"
  }
}



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "pseudoBulkDGE",
    "representation": "pseudoBulkDGE",
    "signature": "ANY",
    "parameters": "function ( x , sample , label , design , coef = ncol ( design ) , contrast = NULL , condition = NULL , lfc = 0 )",
    "body": "{   sample -   as.character (  sample )   label -   as.character (  label )  if (  !   identical (   sort (   rownames (  design ) ) ,   sort (   unique (  sample ) ) ) )  {   stop (  \"'rownames(design)' and 'sample' should have the same levels\" ) }   de.results -   list ( )   failed -   character (  0 )  for  ( i in   sort (   unique (  label ) ) )  {   chosen -   i ==  label   curx -   x [ ,  chosen , drop =  FALSE ]   y -   DGEList (  curx , samples =   data.frame ( sample =   sample [  chosen ] , stringsAsFactors =  FALSE ) )   m -   match (   as.character (    y $ samples $ sample ) ,   rownames (  design ) )   curdesign -   design [  m , , drop =  FALSE ]   curcond -   condition [  m ]   gkeep -   filterByExpr (  y , design =  curdesign , group =  curcond )   y -   y [  gkeep , ]   y -   calcNormFactors (  y )   rank -    qr (  curdesign ) $ rank  if (    rank ==   nrow (  curdesign ) ||   rank undefined   ncol (  curdesign ) )  {   failed -   c (  failed ,  i )   res -   try (  {   fit -   glmFit (  y ,  curdesign , dispersion =  0.05 )  if (   lfc ==  0 )  {   glmLRT (  fit , coef =  coef , contrast =  contrast ) } else  {   glmTreat (  fit , lfc =  lfc , coef =  coef , contrast =  contrast ) } } , silent =  TRUE )  if (   is (  res ,  \"try-error\" ) )  {   refnames -   c (  \"logFC\" ,  if (   lfc !=  0 )  \"unshrunk.logFC\" ,  \"logCPM\" ,  \"PValue\" ,  \"LR\" ,  \"FDR\" )   empty -   lapply (  refnames ,  function ( i )   rep (  NA_real_ ,   nrow (  x ) ) )    names (  empty ) -  refnames   empty -   DataFrame (  empty , row.names =   rownames (  x ) )    de.results [[  i ] ] -  empty  next } else  {     res $ table $ PValue -   rep (  NA_real_ ,   nrow (   res $ table ) ) } } else  {   y -   estimateDisp (  y ,  curdesign )   fit -   glmQLFit (  y ,  curdesign , robust =  TRUE )  if (   lfc ==  0 )  {   res -   glmQLFTest (  fit , coef =  coef , contrast =  contrast ) } else  {   res -   glmTreat (  fit , lfc =  lfc , coef =  coef , contrast =  contrast ) } }   tab -   topTags (  res , n =  Inf , sort.by =  \"none\" )   expander -   match (   seq_len (   nrow (  x ) ) ,   which (  gkeep ) )   tab -   DataFrame (    tab $ table [  expander , , drop =  FALSE ] )    rownames (  tab ) -   rownames (  x )    de.results [[  i ] ] -  tab }   output -   List (  de.results )     metadata (  output ) $ failed -  failed  output } ",
    "replacementFunction": ".pseudo_bulk_dge",
    "filename": "pseudoBulkDGE.txt"
  },
  "new_function": {
    "name": "pseudoBulkDGE",
    "representation": "pseudoBulkDGE",
    "signature": "ANY",
    "parameters": "function ( x , col.data , label , design , coef , contrast = NULL , condition = NULL , lfc = 0 , include.intermediates = TRUE , row.data = NULL , sorted = FALSE , method = c ( \"edgeR\" , \"voom\" ) , qualities = TRUE , robust = TRUE , sample = NULL )",
    "body": "{  if (    is.function (  design ) ||   is (  design ,  \"formula\" ) )  {   .pseudo_bulk_dge ( x =  x , col.data =  col.data , label =  label , condition =  condition , design =  design , coef =  coef , contrast =  contrast , lfc =  lfc , row.data =  row.data , sorted =  sorted , include.intermediates =  include.intermediates , method =   match.arg (  method ) , qualities =  qualities , robust =  robust ) } else  {   .Deprecated ( msg =  \"matrix arguments for 'design=' are deprecated. \\nUse functions or formulas instead.\" )   .pseudo_bulk_old ( x =  x , label =  label , sample =  sample , design =  design , condition =  condition , coef =  coef , contrast =  contrast , lfc =  lfc ) } } ",
    "replacementFunction": ".pseudo_bulk_master",
    "filename": "pseudoBulkDGE.txt"
  }
}

1.
{
  "old_function": {
    "name": "quickSubCluster",
    "representation": "quickSubCluster",
    "signature": "SingleCellExperiment",
    "parameters": "function ( x , groups , normalize = TRUE , prepFUN = NULL , min.ncells = 50 , clusterFUN = NULL , format = \"%s.%s\" , assay.type = \"counts\" )",
    "body": "{  if (  normalize )  {   alt.assay -  \"logcounts\" } else  {   alt.assay -  assay.type }  if (   is.null (  prepFUN ) )  {   prepFUN -  function ( x )  { # Putting this check here to avoid skipping # user-specified modifications.  if (    ncol (  x ) undefined  2L )  {   return (  x ) } # For consistency with quickCluster().   dec -   modelGeneVar (  x , assay.type =  alt.assay )   top -   getTopHVGs (  dec , n =  500 , prop =  0.1 )   denoisePCA (  x ,  dec , subset.row =  top , assay.type =  alt.assay ) } }  if (   is.null (  clusterFUN ) )  {   clusterFUN -  function ( x )  {   g.trans -   buildSNNGraph (  x , use.dimred =  \"PCA\" )    cluster_walktrap (  g.trans ) $ membership } }   all.sce -   collated -   list ( )   by.group -   split (   seq_along (  groups ) ,  groups )  for  ( i in   names (  by.group ) )  {   y -   x [ ,   by.group [[  i ] ] ]  if (  normalize )  {   y -   logNormCounts (  y , exprs_values =  assay.type ) }   y -   prepFUN (  y )  if (    ncol (  y ) =  min.ncells )  {   clusters -   clusterFUN (  y )   clusters -   sprintf (  format ,  i ,  clusters ) } else  {   clusters -   rep (  i ,   ncol (  y ) ) }    y $ subcluster -  clusters    all.sce [[  i ] ] -  y    collated [[  i ] ] -  clusters }   all.clusters -   rep (  NA_character_ ,   ncol (  x ) )    all.clusters [   unlist (  by.group ) ] -   unlist (  collated )   output -   as (  all.sce ,  \"List\" )    metadata (  output ) -   list ( index =  by.group , subcluster =  all.clusters )  output } ",
    "replacementFunction": ".quick_sub_cluster",
    "filename": "quickSubCluster.txt"
  },
  "new_function": {
    "name": "quickSubCluster",
    "representation": "quickSubCluster",
    "signature": "SingleCellExperiment",
    "parameters": "function ( x , groups , normalize = TRUE , prepFUN = NULL , min.ncells = 50 , clusterFUN = NULL , BLUSPARAM = NNGraphParam ( ) , format = \"%s.%s\" , assay.type = \"counts\" , simplify = FALSE )",
    "body": "{  if (  normalize )  {   alt.assay -  \"logcounts\" } else  {   alt.assay -  assay.type }  if (   is.null (  prepFUN ) )  {   prepFUN -  function ( x )  { # Putting this check here to avoid skipping # user-specified modifications.  if (    ncol (  x ) undefined  2L )  {   return (  x ) } # For consistency with quickCluster().   dec -   modelGeneVar (  x , assay.type =  alt.assay )   top -   getTopHVGs (  dec , n =  500 , prop =  0.1 )   denoisePCA (  x ,  dec , subset.row =  top , assay.type =  alt.assay ) } }  if (   is.null (  clusterFUN ) )  {   clusterFUN -  function ( x )  {   clusterRows (   reducedDim (  x ,  \"PCA\" ) ,  BLUSPARAM ) } }   all.sce -   collated -   list ( )   by.group -   split (   seq_along (  groups ) ,  groups )  for  ( i in   names (  by.group ) )  {   y -   x [ ,   by.group [[  i ] ] ]  if (  normalize )  {   y -   logNormCounts (  y , exprs_values =  assay.type ) }   y -   prepFUN (  y )  if (    ncol (  y ) =  min.ncells )  {   clusters -   clusterFUN (  y )   clusters -   sprintf (  format ,  i ,  clusters ) } else  {   clusters -   rep (  i ,   ncol (  y ) ) }    y $ subcluster -  clusters    all.sce [[  i ] ] -  y    collated [[  i ] ] -  clusters }   all.clusters -   rep (  NA_character_ ,   ncol (  x ) )    all.clusters [   unlist (  by.group ) ] -   unlist (  collated )  if (  simplify )  {  all.clusters } else  {   output -   as (  all.sce ,  \"List\" )    metadata (  output ) -   list ( index =  by.group , subcluster =  all.clusters )  output } } ",
    "replacementFunction": ".quick_sub_cluster",
    "filename": "quickSubCluster.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_12 scran_release_3_13

{
    "package": "scran",
    "release_versions": "scran_release_3_12 scran_release_3_13",
    "desc_release_old": "1.18.7",
    "desc_release_new": "1.20.1",
    "old_release_number": 8,
    "new_release_number": 9,
    "function_removals": 4,
    "function_additions": 5,
    "parameter_removals": 0,
    "parameter_additions": 1,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 1,
    "total_count": 5
}

##########
Functions Removed
##########

cleanSizeFactors
computeSpikeFactors
neighborsToKNNGraph
neighborsToSNNGraph


##########
Functions Added
##########

clusterCells
computeMinRank
fixedPCA
rhoToPValue
scoreMarkers


##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########

0.
{
  "old_function": {
    "name": "getDenoisedPCs",
    "representation": "getDenoisedPCs",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row = NULL , min.rank = 5 , max.rank = 50 , fill.missing = FALSE , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   subset.row -   .subset2index (  subset.row ,  x , byrow =  TRUE )   stats -   .compute_mean_var (  x , BPPARAM =  BPPARAM , subset.row =  subset.row , design =  NULL , block.FUN =  compute_blocked_stats_none , block =  NULL )   all.var -   stats $ vars # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {  if (     nrow (  technical ) !=   nrow (  x ) ||  (    nrow (  x ) # as 0-subset matrix removes rownames! undefined  !   identical (   rownames (  technical ) ,   rownames (  x ) ) ) )  {   stop (  \"'technical' should have the same rows as 'x'\" ) } # Making sure everyone has the reported total variance.   total.var -    technical $ total [  subset.row ]   scale -   all.var /  total.var   tech.var -     technical $ tech [  subset.row ] *  scale    tech.var [    all.var ==  0 undefined   total.var ==  0 ] -  0    tech.var [    all.var !=  0 undefined   total.var ==  0 ] -  Inf } else  {  if (   is.function (  technical ) )  {   tech.var -   technical (   stats $ means ) } else  {  if (    length (  technical ) !=   nrow (  x ) )  {   stop (  \"'length(technical)' should be the same as 'nrow(x)'\" ) }   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   use.rows -   subset.row [  keep ]   y -   x [  use.rows , , drop =  FALSE ] # Setting up the SVD results.   svd.out -   .centered_SVD (   t (  y ) ,  max.rank , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    ncol (  y ) -  1 )   total.var -   sum (  all.var )   npcs -   denoisePCANumber (  var.exp ,   sum (  tech.var ) ,  total.var )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) )   list ( components =   .svd_to_pca (  svd.out ,  npcs ) , rotation =   .svd_to_rot (  svd.out ,  npcs ,  x ,  use.rows ,  fill.missing ) , percent.var =    var.exp /  total.var *  100 ) } ",
    "replacementFunction": ".get_denoised_pcs",
    "filename": "denoisePCA.txt"
  },
  "new_function": {
    "name": "getDenoisedPCs",
    "representation": "getDenoisedPCs",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row , min.rank = 5 , max.rank = 50 , fill.missing = FALSE , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   subset.row -   .process_subset_for_pca (  subset.row ,  x )   stats -   .compute_mean_var (  x , BPPARAM =  BPPARAM , subset.row =  subset.row , design =  NULL , block.FUN =  compute_blocked_stats_none , block =  NULL )   all.var -   stats $ vars # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {  if (     nrow (  technical ) !=   nrow (  x ) ||  (    nrow (  x ) # as 0-subset matrix removes rownames! undefined  !   identical (   rownames (  technical ) ,   rownames (  x ) ) ) )  {   stop (  \"'technical' should have the same rows as 'x'\" ) } # Making sure everyone has the reported total variance.   total.var -    technical $ total [  subset.row ]   scale -   all.var /  total.var   tech.var -     technical $ tech [  subset.row ] *  scale    tech.var [    all.var ==  0 undefined   total.var ==  0 ] -  0    tech.var [    all.var !=  0 undefined   total.var ==  0 ] -  Inf } else  {  if (   is.function (  technical ) )  {   tech.var -   technical (   stats $ means ) } else  {  if (    length (  technical ) !=   nrow (  x ) )  {   stop (  \"'length(technical)' should be the same as 'nrow(x)'\" ) }   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components before running the SVD.   keep -   all.var undefined  tech.var   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   use.rows -   subset.row [  keep ]   y -   t (   x [  use.rows , , drop =  FALSE ] )   y -   realizeFileBackedMatrix (  y )   svd.out -   .centered_SVD (  y ,  max.rank , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    nrow (  y ) -  1 )   total.var -   sum (  all.var )   npcs -   denoisePCANumber (  var.exp ,   sum (  tech.var ) ,  total.var )   npcs -   max (  npcs ,   min (  min.rank ,   length (  var.exp ) ) )   list ( components =   .svd_to_pca (  svd.out ,  npcs ) , rotation =   .svd_to_rot (  svd.out ,  npcs ,  x ,  use.rows ,  fill.missing ) , var.explained =  var.exp , percent.var =    var.exp /  total.var *  100 , used.rows =  use.rows ) } ",
    "replacementFunction": ".get_denoised_pcs",
    "filename": "denoisePCA.txt"
  }
}



##########
All Parameter Breaking Changes
##########

0.
{
  "old_function": {
    "name": "getDenoisedPCs",
    "representation": "getDenoisedPCs",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row = NULL , min.rank = 5 , max.rank = 50 , fill.missing = FALSE , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   subset.row -   .subset2index (  subset.row ,  x , byrow =  TRUE )   stats -   .compute_mean_var (  x , BPPARAM =  BPPARAM , subset.row =  subset.row , design =  NULL , block.FUN =  compute_blocked_stats_none , block =  NULL )   all.var -   stats $ vars # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {  if (     nrow (  technical ) !=   nrow (  x ) ||  (    nrow (  x ) # as 0-subset matrix removes rownames! undefined  !   identical (   rownames (  technical ) ,   rownames (  x ) ) ) )  {   stop (  \"'technical' should have the same rows as 'x'\" ) } # Making sure everyone has the reported total variance.   total.var -    technical $ total [  subset.row ]   scale -   all.var /  total.var   tech.var -     technical $ tech [  subset.row ] *  scale    tech.var [    all.var ==  0 undefined   total.var ==  0 ] -  0    tech.var [    all.var !=  0 undefined   total.var ==  0 ] -  Inf } else  {  if (   is.function (  technical ) )  {   tech.var -   technical (   stats $ means ) } else  {  if (    length (  technical ) !=   nrow (  x ) )  {   stop (  \"'length(technical)' should be the same as 'nrow(x)'\" ) }   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components.   keep -   all.var undefined  tech.var   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   use.rows -   subset.row [  keep ]   y -   x [  use.rows , , drop =  FALSE ] # Setting up the SVD results.   svd.out -   .centered_SVD (   t (  y ) ,  max.rank , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    ncol (  y ) -  1 )   total.var -   sum (  all.var )   npcs -   denoisePCANumber (  var.exp ,   sum (  tech.var ) ,  total.var )   npcs -   .keep_rank_in_range (  npcs ,  min.rank ,   length (  var.exp ) )   list ( components =   .svd_to_pca (  svd.out ,  npcs ) , rotation =   .svd_to_rot (  svd.out ,  npcs ,  x ,  use.rows ,  fill.missing ) , percent.var =    var.exp /  total.var *  100 ) } ",
    "replacementFunction": ".get_denoised_pcs",
    "filename": "denoisePCA.txt"
  },
  "new_function": {
    "name": "getDenoisedPCs",
    "representation": "getDenoisedPCs",
    "signature": "ANY",
    "parameters": "function ( x , technical , subset.row , min.rank = 5 , max.rank = 50 , fill.missing = FALSE , BSPARAM = bsparam ( ) , BPPARAM = SerialParam ( ) )",
    "body": "{   subset.row -   .process_subset_for_pca (  subset.row ,  x )   stats -   .compute_mean_var (  x , BPPARAM =  BPPARAM , subset.row =  subset.row , design =  NULL , block.FUN =  compute_blocked_stats_none , block =  NULL )   all.var -   stats $ vars # Processing different mechanisms through which we specify the technical component.  if (   is (  technical ,  \"DataFrame\" ) )  {  if (     nrow (  technical ) !=   nrow (  x ) ||  (    nrow (  x ) # as 0-subset matrix removes rownames! undefined  !   identical (   rownames (  technical ) ,   rownames (  x ) ) ) )  {   stop (  \"'technical' should have the same rows as 'x'\" ) } # Making sure everyone has the reported total variance.   total.var -    technical $ total [  subset.row ]   scale -   all.var /  total.var   tech.var -     technical $ tech [  subset.row ] *  scale    tech.var [    all.var ==  0 undefined   total.var ==  0 ] -  0    tech.var [    all.var !=  0 undefined   total.var ==  0 ] -  Inf } else  {  if (   is.function (  technical ) )  {   tech.var -   technical (   stats $ means ) } else  {  if (    length (  technical ) !=   nrow (  x ) )  {   stop (  \"'length(technical)' should be the same as 'nrow(x)'\" ) }   tech.var -   technical [  subset.row ] } } # Filtering out genes with negative biological components before running the SVD.   keep -   all.var undefined  tech.var   tech.var -   tech.var [  keep ]   all.var -   all.var [  keep ]   use.rows -   subset.row [  keep ]   y -   t (   x [  use.rows , , drop =  FALSE ] )   y -   realizeFileBackedMatrix (  y )   svd.out -   .centered_SVD (  y ,  max.rank , keep.left =  TRUE , keep.right =  TRUE , BSPARAM =  BSPARAM , BPPARAM =  BPPARAM ) # Choosing the number of PCs.   var.exp -     svd.out $ d ^  2 /  (    nrow (  y ) -  1 )   total.var -   sum (  all.var )   npcs -   denoisePCANumber (  var.exp ,   sum (  tech.var ) ,  total.var )   npcs -   max (  npcs ,   min (  min.rank ,   length (  var.exp ) ) )   list ( components =   .svd_to_pca (  svd.out ,  npcs ) , rotation =   .svd_to_rot (  svd.out ,  npcs ,  x ,  use.rows ,  fill.missing ) , var.explained =  var.exp , percent.var =    var.exp /  total.var *  100 , used.rows =  use.rows ) } ",
    "replacementFunction": ".get_denoised_pcs",
    "filename": "denoisePCA.txt"
  }
}


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_13 scran_release_3_14

{
    "package": "scran",
    "release_versions": "scran_release_3_13 scran_release_3_14",
    "desc_release_old": "1.20.1",
    "desc_release_new": "1.22.1",
    "old_release_number": 9,
    "new_release_number": 10,
    "function_removals": 0,
    "function_additions": 0,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 0,
    "total_count": 0
}

##########
Functions Removed
##########



##########
Functions Added
##########



##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########


###############################
###############################
###############################
###############################
Checking Versions:  scran_release_3_14 scran_master

{
    "package": "scran",
    "release_versions": "scran_release_3_14 scran_master",
    "desc_release_old": "1.22.1",
    "desc_release_new": "1.23.1",
    "old_release_number": 10,
    "new_release_number": 11,
    "function_removals": 0,
    "function_additions": 0,
    "parameter_removals": 0,
    "parameter_additions": 0,
    "parameter_renames": 0,
    "parameter_default_changes": 0,
    "parameter_overall_changes": 0,
    "total_count": 0
}

##########
Functions Removed
##########



##########
Functions Added
##########



##########
Removed Non Default Parameters
##########



##########
Added Non Default Parameters
##########



##########
All Parameter Breaking Changes
##########

